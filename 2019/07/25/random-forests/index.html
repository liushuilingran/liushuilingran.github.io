<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>随机森林 - MCFON</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="MCFON"><meta name="msapplication-TileImage" content="/img/favicon.png"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="MCFON"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta description="Bagging&amp;amp;emsp;&amp;amp;emsp;Bagging采用自助采样法(bootstrap sampling)采样数据。给定包含m个样本的数据集，我们先随机取出一个样本放入采样集中，再把该样本放回初始数据集，使得下次采样时，样本仍可能被选中，这样，经过m次随机采样操作，我们得到包含m个样本的采样集。"><meta property="og:type" content="blog"><meta property="og:title" content="随机森林"><meta property="og:url" content="https://huzhiliang.com/2019/07/25/random-forests/"><meta property="og:site_name" content="MCFON"><meta property="og:description" content="Bagging&amp;amp;emsp;&amp;amp;emsp;Bagging采用自助采样法(bootstrap sampling)采样数据。给定包含m个样本的数据集，我们先随机取出一个样本放入采样集中，再把该样本放回初始数据集，使得下次采样时，样本仍可能被选中，这样，经过m次随机采样操作，我们得到包含m个样本的采样集。"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://huzhiliang.com/gallery/11.jpg"><meta property="article:published_time" content="2019-07-25T08:00:26.000Z"><meta property="article:modified_time" content="2019-07-25T17:29:16.563Z"><meta property="article:author" content="ฅ´ω`ฅ"><meta property="article:tag" content="spark"><meta property="article:tag" content="组合树"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/gallery/11.jpg"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://huzhiliang.com/2019/07/25/random-forests/"},"headline":"MCFON","image":["https://huzhiliang.com/gallery/11.jpg"],"datePublished":"2019-07-25T08:00:26.000Z","dateModified":"2019-07-25T17:29:16.563Z","author":{"@type":"Person","name":"ฅ´ω`ฅ"},"description":"Bagging&amp;emsp;&amp;emsp;Bagging采用自助采样法(bootstrap sampling)采样数据。给定包含m个样本的数据集，我们先随机取出一个样本放入采样集中，再把该样本放回初始数据集，使得下次采样时，样本仍可能被选中，这样，经过m次随机采样操作，我们得到包含m个样本的采样集。"}</script><link rel="canonical" href="https://huzhiliang.com/2019/07/25/random-forests/"><link rel="icon" href="/img/favicon.png"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.3.0"></head><body class="is-2-column"><script type="text/javascript" src="/js/imaegoo/night.js"></script><canvas id="universe"></canvas><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img class="logo-img" src="/" alt="MCFON" height="28"><img class="logo-img-dark" src="/" alt="MCFON" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">首页</a><a class="navbar-item" href="/archives/">时间轴</a><a class="navbar-item" href="/categories/">分类</a><a class="navbar-item" href="/tags/">标签</a><a class="navbar-item" target="_blank" rel="noopener" href="https://imaegoo.azurewebsites.net">网盘</a><a class="navbar-item" href="/messages/">留言板</a><a class="navbar-item" href="/friends/">友情链接</a><a class="navbar-item" href="/about/">关于</a></div><div class="navbar-end"><a class="navbar-item night" id="night-nav" title="Night Mode" href="javascript:;"><i class="fas fa-moon" id="night-icon"></i></a><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2019-07-25T08:00:26.000Z" title="2019-07-25T08:00:26.000Z">2019-07-25</time>发表</span><span class="level-item"><time dateTime="2019-07-25T17:29:16.563Z" title="2019-07-25T17:29:16.563Z">2019-07-26</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></span><span class="level-item">40 分钟读完 (大约6071个字)</span><span class="level-item leancloud_visitors" id="/2019/07/25/random-forests/" data-flag-title="随机森林"><i class="far fa-eye"></i>&nbsp;&nbsp;<span id="twikoo_visitors"><i class="fa fa-spinner fa-spin"></i></span>次访问</span></div></div><h1 class="title is-3 is-size-4-mobile">随机森林</h1><div class="content"><h2 id="Bagging"><a href="#Bagging" class="headerlink" title="Bagging"></a>Bagging</h2><p>&emsp;&emsp;<code>Bagging</code>采用自助采样法(<code>bootstrap sampling</code>)采样数据。给定包含<code>m</code>个样本的数据集，我们先随机取出一个样本放入采样集中，再把该样本放回初始数据集，使得下次采样时，样本仍可能被选中，<br>这样，经过<code>m</code>次随机采样操作，我们得到包含<code>m</code>个样本的采样集。<br><a id="more"></a><br>&emsp;&emsp;按照此方式，我们可以采样出<code>T</code>个含<code>m</code>个训练样本的采样集，然后基于每个采样集训练出一个基本学习器，再将这些基本学习器进行结合。这就是<code>Bagging</code>的一般流程。在对预测输出进行结合时，<code>Bagging</code>通常使用简单投票法，<br>对回归问题使用简单平均法。若分类预测时，出现两个类收到同样票数的情形，则最简单的做法是随机选择一个，也可以进一步考察学习器投票的置信度来确定最终胜者。</p>
<p>&emsp;&emsp;<code>Bagging</code>的算法描述如下图所示。</p>
<div  align="center"><img src="../images/imgs8/1.1.png" width = "400" height = "220" alt="1.1" align="center" /></div>

<h2 id="随机森林"><a href="#随机森林" class="headerlink" title="随机森林"></a>随机森林</h2><p>&emsp;&emsp;随机森林是<code>Bagging</code>的一个扩展变体。随机森林在以决策树为基学习器构建<code>Bagging</code>集成的基础上，进一步在决策树的训练过程中引入了随机属性选择。具体来讲，传统决策树在选择划分属性时，<br>在当前节点的属性集合（假设有<code>d</code>个属性）中选择一个最优属性；而在随机森林中，对基决策树的每个节点，先从该节点的属性集合中随机选择一个包含<code>k</code>个属性的子集，然后再从这个子集中选择一个最优属性用于划分。<br>这里的参数<code>k</code>控制了随机性的引入程度。若令<code>k=d</code>，则基决策树的构建与传统决策树相同；若令<code>k=1</code>，则是随机选择一个属性用于划分。在<code>MLlib</code>中，有两种选择用于分类，即<code>k=log2(d)</code>、<code>k=sqrt(d)</code>；<br>一种选择用于回归，即<code>k=1/3d</code>。在源码分析中会详细介绍。</p>
<p>&emsp;&emsp;可以看出，随机森林对<code>Bagging</code>只做了小改动，但是与<code>Bagging</code>中基学习器的“多样性”仅仅通过样本扰动（通过对初始训练集采样）而来不同，随机森林中基学习器的多样性不仅来自样本扰动，还来自属性扰动。<br>这使得最终集成的泛化性能可通过个体学习器之间差异度的增加而进一步提升。</p>
<h2 id="随机森林在分布式环境下的优化策略"><a href="#随机森林在分布式环境下的优化策略" class="headerlink" title="随机森林在分布式环境下的优化策略"></a>随机森林在分布式环境下的优化策略</h2><p>&emsp;&emsp;随机森林算法在单机环境下很容易实现，但在分布式环境下特别是在<code>Spark</code>平台上，传统单机形式的迭代方式必须要进行相应改进才能适用于分布式环境<br>，这是因为在分布式环境下，数据也是分布式的，算法设计不得当会生成大量的<code>IO</code>操作，例如频繁的网络数据传输，从而影响算法效率。<br>因此，在<code>Spark</code>上进行随机森林算法的实现，需要进行一定的优化，<code>Spark</code>中的随机森林算法主要实现了三个优化策略：</p>
<ul>
<li>切分点抽样统计，如下图所示。在单机环境下的决策树对连续变量进行切分点选择时，一般是通过对特征点进行排序，然后取相邻两个数之间的点作为切分点，这在单机环境下是可行的，但如果在分布式环境下如此操作的话，<br>会带来大量的网络传输操作，特别是当数据量达到<code>PB</code>级时，算法效率将极为低下。为避免该问题，<code>Spark</code>中的随机森林在构建决策树时，会对各分区采用一定的子特征策略进行抽样，然后生成各个分区的统计数据，并最终得到切分点。<br>(从源代码里面看，是先对样本进行抽样，然后根据抽样样本值出现的次数进行排序，然后再进行切分)。</li>
</ul>
<div  align="center"><img src="../images/imgs8/1.2.png" width = "600" height = "350" alt="1.2" align="center" /></div>

<ul>
<li>特征装箱（<code>Binning</code>），如下图所示。决策树的构建过程就是对特征的取值不断进行划分的过程，对于离散的特征，如果有<code>M</code>个值，最多有<code>2^(M-1) - 1</code>个划分。如果值是有序的，那么就最多<code>M-1</code>个划分。<br>比如年龄特征，有老，中，少3个值，如果无序有<code>2^2-1=3</code>个划分，即<code>老|中，少；老，中|少；老，少|中</code>。；如果是有序的，即按老，中，少的序，那么只有<code>m-1</code>个，即2种划分，<code>老|中，少；老，中|少</code>。<br>对于连续的特征，其实就是进行范围划分，而划分的点就是<code>split</code>（切分点），划分出的区间就是<code>bin</code>。对于连续特征，理论上<code>split</code>是无数的，在分布环境下不可能取出所有的值，因此它采用的是切点抽样统计方法。</li>
</ul>
<div  align="center"><img src="../images/imgs8/1.3.png" width = "600" height = "400" alt="1.3" align="center" /></div>

<ul>
<li>逐层训练（<code>level-wise training</code>），如下图所示。单机版本的决策树生成过程是通过递归调用（本质上是深度优先）的方式构造树，在构造树的同时，需要移动数据，将同一个子节点的数据移动到一起。<br>此方法在分布式数据结构上无法有效的执行，而且也无法执行，因为数据太大，无法放在一起，所以在分布式环境下采用的策略是逐层构建树节点（本质上是广度优先），这样遍历所有数据的次数等于所有树中的最大层数。<br>每次遍历时，只需要计算每个节点所有切分点统计参数，遍历完后，根据节点的特征划分，决定是否切分，以及如何切分。</li>
</ul>
<div  align="center"><img src="../images/imgs8/1.4.png" width = "600" height = "350" alt="1.4" align="center" /></div>

<h2 id="使用实例"><a href="#使用实例" class="headerlink" title="使用实例"></a>使用实例</h2><p>&emsp;&emsp;下面的例子用于分类。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.mllib.tree.<span class="type">RandomForest</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.mllib.tree.model.<span class="type">RandomForestModel</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.mllib.util.<span class="type">MLUtils</span></span><br><span class="line"><span class="comment">// Load and parse the data file.</span></span><br><span class="line"><span class="keyword">val</span> data = <span class="type">MLUtils</span>.loadLibSVMFile(sc, <span class="string">&quot;data/mllib/sample_libsvm_data.txt&quot;</span>)</span><br><span class="line"><span class="comment">// Split the data into training and test sets (30% held out for testing)</span></span><br><span class="line"><span class="keyword">val</span> splits = data.randomSplit(<span class="type">Array</span>(<span class="number">0.7</span>, <span class="number">0.3</span>))</span><br><span class="line"><span class="keyword">val</span> (trainingData, testData) = (splits(<span class="number">0</span>), splits(<span class="number">1</span>))</span><br><span class="line"><span class="comment">// Train a RandomForest model.</span></span><br><span class="line"><span class="comment">// 空的类别特征信息表示所有的特征都是连续的.</span></span><br><span class="line"><span class="keyword">val</span> numClasses = <span class="number">2</span></span><br><span class="line"><span class="keyword">val</span> categoricalFeaturesInfo = <span class="type">Map</span>[<span class="type">Int</span>, <span class="type">Int</span>]()</span><br><span class="line"><span class="keyword">val</span> numTrees = <span class="number">3</span> <span class="comment">// Use more in practice.</span></span><br><span class="line"><span class="keyword">val</span> featureSubsetStrategy = <span class="string">&quot;auto&quot;</span> <span class="comment">// Let the algorithm choose.</span></span><br><span class="line"><span class="keyword">val</span> impurity = <span class="string">&quot;gini&quot;</span></span><br><span class="line"><span class="keyword">val</span> maxDepth = <span class="number">4</span></span><br><span class="line"><span class="keyword">val</span> maxBins = <span class="number">32</span></span><br><span class="line"><span class="keyword">val</span> model = <span class="type">RandomForest</span>.trainClassifier(trainingData, numClasses, categoricalFeaturesInfo,</span><br><span class="line">  numTrees, featureSubsetStrategy, impurity, maxDepth, maxBins)</span><br><span class="line"><span class="comment">// Evaluate model on test instances and compute test error</span></span><br><span class="line"><span class="keyword">val</span> labelAndPreds = testData.map &#123; point =&gt;</span><br><span class="line">  <span class="keyword">val</span> prediction = model.predict(point.features)</span><br><span class="line">  (point.label, prediction)</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">val</span> testErr = labelAndPreds.filter(r =&gt; r._1 != r._2).count.toDouble / testData.count()</span><br><span class="line">println(<span class="string">&quot;Test Error = &quot;</span> + testErr)</span><br><span class="line">println(<span class="string">&quot;Learned classification forest model:\n&quot;</span> + model.toDebugString)</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;下面的例子用于回归。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.mllib.tree.<span class="type">RandomForest</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.mllib.tree.model.<span class="type">RandomForestModel</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.mllib.util.<span class="type">MLUtils</span></span><br><span class="line"><span class="comment">// Load and parse the data file.</span></span><br><span class="line"><span class="keyword">val</span> data = <span class="type">MLUtils</span>.loadLibSVMFile(sc, <span class="string">&quot;data/mllib/sample_libsvm_data.txt&quot;</span>)</span><br><span class="line"><span class="comment">// Split the data into training and test sets (30% held out for testing)</span></span><br><span class="line"><span class="keyword">val</span> splits = data.randomSplit(<span class="type">Array</span>(<span class="number">0.7</span>, <span class="number">0.3</span>))</span><br><span class="line"><span class="keyword">val</span> (trainingData, testData) = (splits(<span class="number">0</span>), splits(<span class="number">1</span>))</span><br><span class="line"><span class="comment">// Train a RandomForest model.</span></span><br><span class="line"><span class="comment">// 空的类别特征信息表示所有的特征都是连续的</span></span><br><span class="line"><span class="keyword">val</span> numClasses = <span class="number">2</span></span><br><span class="line"><span class="keyword">val</span> categoricalFeaturesInfo = <span class="type">Map</span>[<span class="type">Int</span>, <span class="type">Int</span>]()</span><br><span class="line"><span class="keyword">val</span> numTrees = <span class="number">3</span> <span class="comment">// Use more in practice.</span></span><br><span class="line"><span class="keyword">val</span> featureSubsetStrategy = <span class="string">&quot;auto&quot;</span> <span class="comment">// Let the algorithm choose.</span></span><br><span class="line"><span class="keyword">val</span> impurity = <span class="string">&quot;variance&quot;</span></span><br><span class="line"><span class="keyword">val</span> maxDepth = <span class="number">4</span></span><br><span class="line"><span class="keyword">val</span> maxBins = <span class="number">32</span></span><br><span class="line"><span class="keyword">val</span> model = <span class="type">RandomForest</span>.trainRegressor(trainingData, categoricalFeaturesInfo,</span><br><span class="line">  numTrees, featureSubsetStrategy, impurity, maxDepth, maxBins)</span><br><span class="line"><span class="comment">// Evaluate model on test instances and compute test error</span></span><br><span class="line"><span class="keyword">val</span> labelsAndPredictions = testData.map &#123; point =&gt;</span><br><span class="line">  <span class="keyword">val</span> prediction = model.predict(point.features)</span><br><span class="line">  (point.label, prediction)</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">val</span> testMSE = labelsAndPredictions.map&#123; <span class="keyword">case</span>(v, p) =&gt; math.pow((v - p), <span class="number">2</span>)&#125;.mean()</span><br><span class="line">println(<span class="string">&quot;Test Mean Squared Error = &quot;</span> + testMSE)</span><br><span class="line">println(<span class="string">&quot;Learned regression forest model:\n&quot;</span> + model.toDebugString)</span><br></pre></td></tr></table></figure>
<h2 id="源码分析"><a href="#源码分析" class="headerlink" title="源码分析"></a>源码分析</h2><h3 id="训练分析"><a href="#训练分析" class="headerlink" title="训练分析"></a>训练分析</h3><p>&emsp;&emsp;训练过程简单可以分为两步，第一步是初始化，第二步是迭代构建随机森林。这两大步还分为若干小步，下面会分别介绍这些内容。</p>
<h4 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> retaggedInput = input.retag(classOf[<span class="type">LabeledPoint</span>])</span><br><span class="line"><span class="comment">//建立决策树的元数据信息（分裂点位置、箱子数及各箱子包含特征属性的值等）</span></span><br><span class="line"><span class="keyword">val</span> metadata =</span><br><span class="line">    <span class="type">DecisionTreeMetadata</span>.buildMetadata(retaggedInput, strategy, numTrees, featureSubsetStrategy)</span><br><span class="line"><span class="comment">//找到切分点（splits）及箱子信息（Bins）</span></span><br><span class="line"><span class="comment">//对于连续型特征，利用切分点抽样统计简化计算</span></span><br><span class="line"><span class="comment">//对于离散型特征，如果是无序的，则最多有个 splits=2^(numBins-1)-1 划分</span></span><br><span class="line"><span class="comment">//如果是有序的，则最多有 splits=numBins-1 个划分</span></span><br><span class="line"><span class="keyword">val</span> (splits, bins) = <span class="type">DecisionTree</span>.findSplitsBins(retaggedInput, metadata)</span><br><span class="line"><span class="comment">//转换成树形的 RDD 类型，转换后，所有样本点已经按分裂点条件分到了各自的箱子中</span></span><br><span class="line"><span class="keyword">val</span> treeInput = <span class="type">TreePoint</span>.convertToTreeRDD(retaggedInput, bins, metadata)</span><br><span class="line"><span class="keyword">val</span> withReplacement = <span class="keyword">if</span> (numTrees &gt; <span class="number">1</span>) <span class="literal">true</span> <span class="keyword">else</span> <span class="literal">false</span></span><br><span class="line"><span class="comment">// convertToBaggedRDD 方法使得每棵树就是样本的一个子集</span></span><br><span class="line"><span class="keyword">val</span> baggedInput = <span class="type">BaggedPoint</span>.convertToBaggedRDD(treeInput,</span><br><span class="line">          strategy.subsamplingRate, numTrees,</span><br><span class="line">          withReplacement, seed).persist(<span class="type">StorageLevel</span>.<span class="type">MEMORY_AND_DISK</span>)</span><br><span class="line"><span class="comment">//决策树的深度，最大为30</span></span><br><span class="line"><span class="keyword">val</span> maxDepth = strategy.maxDepth</span><br><span class="line"><span class="comment">//聚合的最大内存</span></span><br><span class="line"><span class="keyword">val</span> maxMemoryUsage: <span class="type">Long</span> = strategy.maxMemoryInMB * <span class="number">1024</span>L * <span class="number">1024</span>L</span><br><span class="line"><span class="keyword">val</span> maxMemoryPerNode = &#123;</span><br><span class="line">    <span class="keyword">val</span> featureSubset: <span class="type">Option</span>[<span class="type">Array</span>[<span class="type">Int</span>]] = <span class="keyword">if</span> (metadata.subsamplingFeatures) &#123;</span><br><span class="line">        <span class="comment">// Find numFeaturesPerNode largest bins to get an upper bound on memory usage.</span></span><br><span class="line">        <span class="type">Some</span>(metadata.numBins.zipWithIndex.sortBy(- _._1)</span><br><span class="line">          .take(metadata.numFeaturesPerNode).map(_._2))</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="type">None</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//计算聚合操作时节点的内存</span></span><br><span class="line">    <span class="type">RandomForest</span>.aggregateSizeForNode(metadata, featureSubset) * <span class="number">8</span>L</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;初始化的第一步就是决策树元数据信息的构建。它的代码如下所示。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">buildMetadata</span></span>(</span><br><span class="line">      input: <span class="type">RDD</span>[<span class="type">LabeledPoint</span>],</span><br><span class="line">      strategy: <span class="type">Strategy</span>,</span><br><span class="line">      numTrees: <span class="type">Int</span>,</span><br><span class="line">      featureSubsetStrategy: <span class="type">String</span>): <span class="type">DecisionTreeMetadata</span> = &#123;</span><br><span class="line">    <span class="comment">//特征数</span></span><br><span class="line">    <span class="keyword">val</span> numFeatures = input.map(_.features.size).take(<span class="number">1</span>).headOption.getOrElse &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalArgumentException</span>(<span class="string">s&quot;DecisionTree requires size of input RDD &gt; 0, &quot;</span> +</span><br><span class="line">        <span class="string">s&quot;but was given by empty one.&quot;</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">val</span> numExamples = input.count()</span><br><span class="line">    <span class="keyword">val</span> numClasses = strategy.algo <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="type">Classification</span> =&gt; strategy.numClasses</span><br><span class="line">      <span class="keyword">case</span> <span class="type">Regression</span> =&gt; <span class="number">0</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//最大可能的装箱数</span></span><br><span class="line">    <span class="keyword">val</span> maxPossibleBins = math.min(strategy.maxBins, numExamples).toInt</span><br><span class="line">    <span class="keyword">if</span> (maxPossibleBins &lt; strategy.maxBins) &#123;</span><br><span class="line">      logWarning(<span class="string">s&quot;DecisionTree reducing maxBins from <span class="subst">$&#123;strategy.maxBins&#125;</span> to <span class="subst">$maxPossibleBins</span>&quot;</span> +</span><br><span class="line">        <span class="string">s&quot; (= number of training instances)&quot;</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// We check the number of bins here against maxPossibleBins.</span></span><br><span class="line">    <span class="comment">// This needs to be checked here instead of in Strategy since maxPossibleBins can be modified</span></span><br><span class="line">    <span class="comment">// based on the number of training examples.</span></span><br><span class="line">    <span class="comment">//最大分类数要小于最大可能装箱数</span></span><br><span class="line">    <span class="comment">//这里categoricalFeaturesInfo是传入的信息，这个map保存特征的类别信息。</span></span><br><span class="line">    <span class="comment">//例如，(n-&gt;k)表示特征k包含的类别有（0,1,...,k-1）</span></span><br><span class="line">    <span class="keyword">if</span> (strategy.categoricalFeaturesInfo.nonEmpty) &#123;</span><br><span class="line">      <span class="keyword">val</span> maxCategoriesPerFeature = strategy.categoricalFeaturesInfo.values.max</span><br><span class="line">      <span class="keyword">val</span> maxCategory =</span><br><span class="line">        strategy.categoricalFeaturesInfo.find(_._2 == maxCategoriesPerFeature).get._1</span><br><span class="line">      require(maxCategoriesPerFeature &lt;= maxPossibleBins,</span><br><span class="line">        <span class="string">s&quot;DecisionTree requires maxBins (= <span class="subst">$maxPossibleBins</span>) to be at least as large as the &quot;</span> +</span><br><span class="line">        <span class="string">s&quot;number of values in each categorical feature, but categorical feature <span class="subst">$maxCategory</span> &quot;</span> +</span><br><span class="line">        <span class="string">s&quot;has <span class="subst">$maxCategoriesPerFeature</span> values. Considering remove this and other categorical &quot;</span> +</span><br><span class="line">        <span class="string">&quot;features with a large number of values, or add more training examples.&quot;</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">val</span> unorderedFeatures = <span class="keyword">new</span> mutable.<span class="type">HashSet</span>[<span class="type">Int</span>]()</span><br><span class="line">    <span class="keyword">val</span> numBins = <span class="type">Array</span>.fill[<span class="type">Int</span>](numFeatures)(maxPossibleBins)</span><br><span class="line">    <span class="keyword">if</span> (numClasses &gt; <span class="number">2</span>) &#123;</span><br><span class="line">      <span class="comment">// 多分类</span></span><br><span class="line">      <span class="keyword">val</span> maxCategoriesForUnorderedFeature =</span><br><span class="line">        ((math.log(maxPossibleBins / <span class="number">2</span> + <span class="number">1</span>) / math.log(<span class="number">2.0</span>)) + <span class="number">1</span>).floor.toInt</span><br><span class="line">      strategy.categoricalFeaturesInfo.foreach &#123; <span class="keyword">case</span> (featureIndex, numCategories) =&gt;</span><br><span class="line">        <span class="comment">//如果类别特征只有1个类，我们把它看成连续的特征</span></span><br><span class="line">        <span class="keyword">if</span> (numCategories &gt; <span class="number">1</span>) &#123;</span><br><span class="line">          <span class="comment">// Decide if some categorical features should be treated as unordered features,</span></span><br><span class="line">          <span class="comment">//  which require 2 * ((1 &lt;&lt; numCategories - 1) - 1) bins.</span></span><br><span class="line">          <span class="comment">// We do this check with log values to prevent overflows in case numCategories is large.</span></span><br><span class="line">          <span class="comment">// The next check is equivalent to: 2 * ((1 &lt;&lt; numCategories - 1) - 1) &lt;= maxBins</span></span><br><span class="line">          <span class="keyword">if</span> (numCategories &lt;= maxCategoriesForUnorderedFeature) &#123;</span><br><span class="line">            unorderedFeatures.add(featureIndex)</span><br><span class="line">            numBins(featureIndex) = numUnorderedBins(numCategories)</span><br><span class="line">          &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            numBins(featureIndex) = numCategories</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">// 二分类或者回归</span></span><br><span class="line">      strategy.categoricalFeaturesInfo.foreach &#123; <span class="keyword">case</span> (featureIndex, numCategories) =&gt;</span><br><span class="line">        <span class="comment">//如果类别特征只有1个类，我们把它看成连续的特征</span></span><br><span class="line">        <span class="keyword">if</span> (numCategories &gt; <span class="number">1</span>) &#123;</span><br><span class="line">          numBins(featureIndex) = numCategories</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 设置每个节点的特征数 (对随机森林而言).</span></span><br><span class="line">    <span class="keyword">val</span> _featureSubsetStrategy = featureSubsetStrategy <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="string">&quot;auto&quot;</span> =&gt;</span><br><span class="line">        <span class="keyword">if</span> (numTrees == <span class="number">1</span>) &#123;<span class="comment">//决策树时，使用所有特征</span></span><br><span class="line">          <span class="string">&quot;all&quot;</span></span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          <span class="keyword">if</span> (strategy.algo == <span class="type">Classification</span>) &#123;<span class="comment">//分类时，使用开平方</span></span><br><span class="line">            <span class="string">&quot;sqrt&quot;</span></span><br><span class="line">          &#125; <span class="keyword">else</span> &#123; <span class="comment">//回归时，使用1/3的特征</span></span><br><span class="line">            <span class="string">&quot;onethird&quot;</span></span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      <span class="keyword">case</span> _ =&gt; featureSubsetStrategy</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">val</span> numFeaturesPerNode: <span class="type">Int</span> = _featureSubsetStrategy <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="string">&quot;all&quot;</span> =&gt; numFeatures</span><br><span class="line">      <span class="keyword">case</span> <span class="string">&quot;sqrt&quot;</span> =&gt; math.sqrt(numFeatures).ceil.toInt</span><br><span class="line">      <span class="keyword">case</span> <span class="string">&quot;log2&quot;</span> =&gt; math.max(<span class="number">1</span>, (math.log(numFeatures) / math.log(<span class="number">2</span>)).ceil.toInt)</span><br><span class="line">      <span class="keyword">case</span> <span class="string">&quot;onethird&quot;</span> =&gt; (numFeatures / <span class="number">3.0</span>).ceil.toInt</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">DecisionTreeMetadata</span>(numFeatures, numExamples, numClasses, numBins.max,</span><br><span class="line">      strategy.categoricalFeaturesInfo, unorderedFeatures.toSet, numBins,</span><br><span class="line">      strategy.impurity, strategy.quantileCalculationStrategy, strategy.maxDepth,</span><br><span class="line">      strategy.minInstancesPerNode, strategy.minInfoGain, numTrees, numFeaturesPerNode)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;初始化的第二步就是找到切分点（<code>splits</code>）及箱子信息（<code>Bins</code>）。这时，调用了<code>DecisionTree.findSplitsBins</code>方法，进入该方法了解详细信息。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Returns splits and bins for decision tree calculation.</span></span><br><span class="line"><span class="comment">   * Continuous and categorical features are handled differently.</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * Continuous features:</span></span><br><span class="line"><span class="comment">   *   For each feature, there are numBins - 1 possible splits representing the possible binary</span></span><br><span class="line"><span class="comment">   *   decisions at each node in the tree.</span></span><br><span class="line"><span class="comment">   *   This finds locations (feature values) for splits using a subsample of the data.</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * Categorical features:</span></span><br><span class="line"><span class="comment">   *   For each feature, there is 1 bin per split.</span></span><br><span class="line"><span class="comment">   *   Splits and bins are handled in 2 ways:</span></span><br><span class="line"><span class="comment">   *   (a) &quot;unordered features&quot;</span></span><br><span class="line"><span class="comment">   *       For multiclass classification with a low-arity feature</span></span><br><span class="line"><span class="comment">   *       (i.e., if isMulticlass &amp;&amp; isSpaceSufficientForAllCategoricalSplits),</span></span><br><span class="line"><span class="comment">   *       the feature is split based on subsets of categories.</span></span><br><span class="line"><span class="comment">   *   (b) &quot;ordered features&quot;</span></span><br><span class="line"><span class="comment">   *       For regression and binary classification,</span></span><br><span class="line"><span class="comment">   *       and for multiclass classification with a high-arity feature,</span></span><br><span class="line"><span class="comment">   *       there is one bin per category.</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * @param input Training data: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]]</span></span><br><span class="line"><span class="comment">   * @param metadata Learning and dataset metadata</span></span><br><span class="line"><span class="comment">   * @return A tuple of (splits, bins).</span></span><br><span class="line"><span class="comment">   *         Splits is an Array of [[org.apache.spark.mllib.tree.model.Split]]</span></span><br><span class="line"><span class="comment">   *          of size (numFeatures, numSplits).</span></span><br><span class="line"><span class="comment">   *         Bins is an Array of [[org.apache.spark.mllib.tree.model.Bin]]</span></span><br><span class="line"><span class="comment">   *          of size (numFeatures, numBins).</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">protected</span>[tree] <span class="function"><span class="keyword">def</span> <span class="title">findSplitsBins</span></span>(</span><br><span class="line">      input: <span class="type">RDD</span>[<span class="type">LabeledPoint</span>],</span><br><span class="line">      metadata: <span class="type">DecisionTreeMetadata</span>): (<span class="type">Array</span>[<span class="type">Array</span>[<span class="type">Split</span>]], <span class="type">Array</span>[<span class="type">Array</span>[<span class="type">Bin</span>]]) = &#123;</span><br><span class="line">    <span class="comment">//特征数</span></span><br><span class="line">    <span class="keyword">val</span> numFeatures = metadata.numFeatures</span><br><span class="line">    <span class="comment">// Sample the input only if there are continuous features.</span></span><br><span class="line">    <span class="comment">// 判断特征中是否存在连续特征</span></span><br><span class="line">    <span class="keyword">val</span> continuousFeatures = <span class="type">Range</span>(<span class="number">0</span>, numFeatures).filter(metadata.isContinuous)</span><br><span class="line">    <span class="keyword">val</span> sampledInput = <span class="keyword">if</span> (continuousFeatures.nonEmpty) &#123;</span><br><span class="line">      <span class="comment">// Calculate the number of samples for approximate quantile calculation.</span></span><br><span class="line">      <span class="comment">//采样样本数量，最少有 10000 个</span></span><br><span class="line">      <span class="keyword">val</span> requiredSamples = math.max(metadata.maxBins * metadata.maxBins, <span class="number">10000</span>)</span><br><span class="line">      <span class="comment">//计算采样比例</span></span><br><span class="line">      <span class="keyword">val</span> fraction = <span class="keyword">if</span> (requiredSamples &lt; metadata.numExamples) &#123;</span><br><span class="line">        requiredSamples.toDouble / metadata.numExamples</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="number">1.0</span></span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">//采样数据，有放回采样</span></span><br><span class="line">      input.sample(withReplacement = <span class="literal">false</span>, fraction, <span class="keyword">new</span> <span class="type">XORShiftRandom</span>().nextInt())</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      input.sparkContext.emptyRDD[<span class="type">LabeledPoint</span>]</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//分裂点策略，目前 Spark 中只实现了一种策略：排序 Sort</span></span><br><span class="line">    metadata.quantileStrategy <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="type">Sort</span> =&gt;</span><br><span class="line">        findSplitsBinsBySorting(sampledInput, metadata, continuousFeatures)</span><br><span class="line">      <span class="keyword">case</span> <span class="type">MinMax</span> =&gt;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">UnsupportedOperationException</span>(<span class="string">&quot;minmax not supported yet.&quot;</span>)</span><br><span class="line">      <span class="keyword">case</span> <span class="type">ApproxHist</span> =&gt;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">UnsupportedOperationException</span>(<span class="string">&quot;approximate histogram not supported yet.&quot;</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;我们进入<code>findSplitsBinsBySorting</code>方法了解<code>Sort</code>分裂策略的实现。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">findSplitsBinsBySorting</span></span>(</span><br><span class="line">      input: <span class="type">RDD</span>[<span class="type">LabeledPoint</span>],</span><br><span class="line">      metadata: <span class="type">DecisionTreeMetadata</span>,</span><br><span class="line">      continuousFeatures: <span class="type">IndexedSeq</span>[<span class="type">Int</span>]): (<span class="type">Array</span>[<span class="type">Array</span>[<span class="type">Split</span>]], <span class="type">Array</span>[<span class="type">Array</span>[<span class="type">Bin</span>]]) = &#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">findSplits</span></span>(</span><br><span class="line">        featureIndex: <span class="type">Int</span>,</span><br><span class="line">        featureSamples: <span class="type">Iterable</span>[<span class="type">Double</span>]): (<span class="type">Int</span>, (<span class="type">Array</span>[<span class="type">Split</span>], <span class="type">Array</span>[<span class="type">Bin</span>])) = &#123;</span><br><span class="line">      <span class="comment">//每个特征分别对应一组切分点位置，这里splits是有序的</span></span><br><span class="line">      <span class="keyword">val</span> splits = &#123;</span><br><span class="line">        <span class="comment">// findSplitsForContinuousFeature 返回连续特征的所有切分位置</span></span><br><span class="line">        <span class="keyword">val</span> featureSplits = findSplitsForContinuousFeature(</span><br><span class="line">          featureSamples.toArray,</span><br><span class="line">          metadata,</span><br><span class="line">          featureIndex)</span><br><span class="line">        featureSplits.map(threshold =&gt; <span class="keyword">new</span> <span class="type">Split</span>(featureIndex, threshold, <span class="type">Continuous</span>, <span class="type">Nil</span>))</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">//存放切分点位置对应的箱子信息</span></span><br><span class="line">      <span class="keyword">val</span> bins = &#123;</span><br><span class="line">        <span class="comment">//采用最小阈值 Double.MinValue 作为最左边的分裂位置并进行装箱</span></span><br><span class="line">        <span class="keyword">val</span> lowSplit = <span class="keyword">new</span> <span class="type">DummyLowSplit</span>(featureIndex, <span class="type">Continuous</span>)</span><br><span class="line">        <span class="comment">//最后一个箱子的计算采用最大阈值 Double.MaxValue 作为最右边的切分位置</span></span><br><span class="line">        <span class="keyword">val</span> highSplit = <span class="keyword">new</span> <span class="type">DummyHighSplit</span>(featureIndex, <span class="type">Continuous</span>)</span><br><span class="line">        <span class="comment">// tack the dummy splits on either side of the computed splits</span></span><br><span class="line">        <span class="keyword">val</span> allSplits = lowSplit +: splits.toSeq :+ highSplit</span><br><span class="line">        <span class="comment">//将切分点两两结合成一个箱子</span></span><br><span class="line">        allSplits.sliding(<span class="number">2</span>).map &#123;</span><br><span class="line">          <span class="keyword">case</span> <span class="type">Seq</span>(left, right) =&gt; <span class="keyword">new</span> <span class="type">Bin</span>(left, right, <span class="type">Continuous</span>, <span class="type">Double</span>.<span class="type">MinValue</span>)</span><br><span class="line">        &#125;.toArray</span><br><span class="line">      &#125;</span><br><span class="line">      (featureIndex, (splits, bins))</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">val</span> continuousSplits = &#123;</span><br><span class="line">      <span class="comment">// reduce the parallelism for split computations when there are less</span></span><br><span class="line">      <span class="comment">// continuous features than input partitions. this prevents tasks from</span></span><br><span class="line">      <span class="comment">// being spun up that will definitely do no work.</span></span><br><span class="line">      <span class="keyword">val</span> numPartitions = math.min(continuousFeatures.length, input.partitions.length)</span><br><span class="line">      input</span><br><span class="line">        .flatMap(point =&gt; continuousFeatures.map(idx =&gt; (idx, point.features(idx))))</span><br><span class="line">        .groupByKey(numPartitions)</span><br><span class="line">        .map &#123; <span class="keyword">case</span> (k, v) =&gt; findSplits(k, v) &#125;</span><br><span class="line">        .collectAsMap()</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">val</span> numFeatures = metadata.numFeatures</span><br><span class="line">    <span class="comment">//遍历所有特征</span></span><br><span class="line">    <span class="keyword">val</span> (splits, bins) = <span class="type">Range</span>(<span class="number">0</span>, numFeatures).unzip &#123;</span><br><span class="line">      <span class="comment">//处理连续特征的情况</span></span><br><span class="line">      <span class="keyword">case</span> i <span class="keyword">if</span> metadata.isContinuous(i) =&gt;</span><br><span class="line">        <span class="keyword">val</span> (split, bin) = continuousSplits(i)</span><br><span class="line">        metadata.setNumSplits(i, split.length)</span><br><span class="line">        (split, bin)</span><br><span class="line">      <span class="comment">//处理离散特征且无序的情况</span></span><br><span class="line">      <span class="keyword">case</span> i <span class="keyword">if</span> metadata.isCategorical(i) &amp;&amp; metadata.isUnordered(i) =&gt;</span><br><span class="line">        <span class="comment">// Unordered features</span></span><br><span class="line">        <span class="comment">// 2^(maxFeatureValue - 1) - 1 combinations</span></span><br><span class="line">        <span class="keyword">val</span> featureArity = metadata.featureArity(i)</span><br><span class="line">        <span class="keyword">val</span> split = <span class="type">Range</span>(<span class="number">0</span>, metadata.numSplits(i)).map &#123; splitIndex =&gt;</span><br><span class="line">          <span class="keyword">val</span> categories = extractMultiClassCategories(splitIndex + <span class="number">1</span>, featureArity)</span><br><span class="line">          <span class="keyword">new</span> <span class="type">Split</span>(i, <span class="type">Double</span>.<span class="type">MinValue</span>, <span class="type">Categorical</span>, categories)</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// For unordered categorical features, there is no need to construct the bins.</span></span><br><span class="line">        <span class="comment">// since there is a one-to-one correspondence between the splits and the bins.</span></span><br><span class="line">        (split.toArray, <span class="type">Array</span>.empty[<span class="type">Bin</span>])</span><br><span class="line">      <span class="comment">//处理离散特征且有序的情况</span></span><br><span class="line">      <span class="keyword">case</span> i <span class="keyword">if</span> metadata.isCategorical(i) =&gt;</span><br><span class="line">        <span class="comment">//有序特征无需处理，箱子与特征值对应</span></span><br><span class="line">        <span class="comment">// Ordered features</span></span><br><span class="line">        <span class="comment">// Bins correspond to feature values, so we do not need to compute splits or bins</span></span><br><span class="line">        <span class="comment">// beforehand.  Splits are constructed as needed during training.</span></span><br><span class="line">        (<span class="type">Array</span>.empty[<span class="type">Split</span>], <span class="type">Array</span>.empty[<span class="type">Bin</span>])</span><br><span class="line">    &#125;</span><br><span class="line">    (splits.toArray, bins.toArray)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;计算连续特征的所有切分位置需要调用方法<code>findSplitsForContinuousFeature</code>方法。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[tree] <span class="function"><span class="keyword">def</span> <span class="title">findSplitsForContinuousFeature</span></span>(</span><br><span class="line">      featureSamples: <span class="type">Array</span>[<span class="type">Double</span>],</span><br><span class="line">      metadata: <span class="type">DecisionTreeMetadata</span>,</span><br><span class="line">      featureIndex: <span class="type">Int</span>): <span class="type">Array</span>[<span class="type">Double</span>] = &#123;</span><br><span class="line">    <span class="keyword">val</span> splits = &#123;</span><br><span class="line">      <span class="comment">//切分数是bin的数量减1，即m-1</span></span><br><span class="line">      <span class="keyword">val</span> numSplits = metadata.numSplits(featureIndex)</span><br><span class="line">      <span class="comment">// （特征，特征出现的次数）</span></span><br><span class="line">      <span class="keyword">val</span> valueCountMap = featureSamples.foldLeft(<span class="type">Map</span>.empty[<span class="type">Double</span>, <span class="type">Int</span>]) &#123; (m, x) =&gt;</span><br><span class="line">        m + ((x, m.getOrElse(x, <span class="number">0</span>) + <span class="number">1</span>))</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">// 根据特征进行排序</span></span><br><span class="line">      <span class="keyword">val</span> valueCounts = valueCountMap.toSeq.sortBy(_._1).toArray</span><br><span class="line">      <span class="comment">// if possible splits is not enough or just enough, just return all possible splits</span></span><br><span class="line">      <span class="keyword">val</span> possibleSplits = valueCounts.length</span><br><span class="line">      <span class="comment">//如果特征数小于切分数，所有特征均作为切分点</span></span><br><span class="line">      <span class="keyword">if</span> (possibleSplits &lt;= numSplits) &#123;</span><br><span class="line">        valueCounts.map(_._1)</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// 等频切分</span></span><br><span class="line">        <span class="comment">// 切分点之间的步长</span></span><br><span class="line">        <span class="keyword">val</span> stride: <span class="type">Double</span> = featureSamples.length.toDouble / (numSplits + <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">val</span> splitsBuilder = <span class="type">Array</span>.newBuilder[<span class="type">Double</span>]</span><br><span class="line">        <span class="keyword">var</span> index = <span class="number">1</span></span><br><span class="line">        <span class="comment">// currentCount: sum of counts of values that have been visited</span></span><br><span class="line">        <span class="comment">//第一个特征的出现次数</span></span><br><span class="line">        <span class="keyword">var</span> currentCount = valueCounts(<span class="number">0</span>)._2</span><br><span class="line">        <span class="comment">// targetCount: target value for `currentCount`.</span></span><br><span class="line">        <span class="comment">// If `currentCount` is closest value to `targetCount`,</span></span><br><span class="line">        <span class="comment">// then current value is a split threshold.</span></span><br><span class="line">        <span class="comment">// After finding a split threshold, `targetCount` is added by stride.</span></span><br><span class="line">        <span class="comment">// 如果currentCount离targetCount最近，那么当前值是切分点</span></span><br><span class="line">        <span class="keyword">var</span> targetCount = stride</span><br><span class="line">        <span class="keyword">while</span> (index &lt; valueCounts.length) &#123;</span><br><span class="line">          <span class="keyword">val</span> previousCount = currentCount</span><br><span class="line">          currentCount += valueCounts(index)._2</span><br><span class="line">          <span class="keyword">val</span> previousGap = math.abs(previousCount - targetCount)</span><br><span class="line">          <span class="keyword">val</span> currentGap = math.abs(currentCount - targetCount)</span><br><span class="line">          <span class="comment">// If adding count of current value to currentCount</span></span><br><span class="line">          <span class="comment">// makes the gap between currentCount and targetCount smaller,</span></span><br><span class="line">          <span class="comment">// previous value is a split threshold.</span></span><br><span class="line">          <span class="keyword">if</span> (previousGap &lt; currentGap) &#123;</span><br><span class="line">            splitsBuilder += valueCounts(index - <span class="number">1</span>)._1</span><br><span class="line">            targetCount += stride</span><br><span class="line">          &#125;</span><br><span class="line">          index += <span class="number">1</span></span><br><span class="line">        &#125;</span><br><span class="line">        splitsBuilder.result()</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    splits</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp; 在if判断里每步前进<code>stride</code>个样本，累加在<code>targetCount</code>中。<code>while</code>循环逐次把每个特征值的个数加到<code>currentCount</code>里，计算前一次<code>previousCount</code>和这次<code>currentCount</code>到<code>targetCount</code>的距离，有3种情况，一种是<code>pre</code>和<code>cur</code>都在<code>target</code>左边，肯定是<code>cur</code>小，继续循环，进入第二种情况；第二种一左一右，如果<code>pre</code>小，肯定是<code>pre</code>是最好的分割点，如果<code>cur</code>还是小，继续循环步进，进入第三种情况；第三种就是都在右边，显然是<code>pre</code>小。因此<code>if</code>的判断条件<code>pre&lt;cur</code>，只要满足肯定就是<code>split</code>。整体下来的效果就能找到离<code>target</code>最近的一个特征值。 </p>
<h4 id="迭代构建随机森林"><a href="#迭代构建随机森林" class="headerlink" title="迭代构建随机森林"></a>迭代构建随机森林</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//节点是否使用缓存，节点 ID 从 1 开始，1 即为这颗树的根节点，左节点为 2，右节点为 3，依次递增下去</span></span><br><span class="line"><span class="keyword">val</span> nodeIdCache = <span class="keyword">if</span> (strategy.useNodeIdCache) &#123;</span><br><span class="line">   <span class="type">Some</span>(<span class="type">NodeIdCache</span>.init(</span><br><span class="line">        data = baggedInput,</span><br><span class="line">        numTrees = numTrees,</span><br><span class="line">        checkpointInterval = strategy.checkpointInterval,</span><br><span class="line">        initVal = <span class="number">1</span>))</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">   <span class="type">None</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// FIFO queue of nodes to train: (treeIndex, node)</span></span><br><span class="line"><span class="keyword">val</span> nodeQueue = <span class="keyword">new</span> mutable.<span class="type">Queue</span>[(<span class="type">Int</span>, <span class="type">Node</span>)]()</span><br><span class="line"><span class="keyword">val</span> rng = <span class="keyword">new</span> scala.util.<span class="type">Random</span>()</span><br><span class="line">rng.setSeed(seed)</span><br><span class="line"><span class="comment">// Allocate and queue root nodes.</span></span><br><span class="line"><span class="comment">//创建树的根节点</span></span><br><span class="line"><span class="keyword">val</span> topNodes: <span class="type">Array</span>[<span class="type">Node</span>] = <span class="type">Array</span>.fill[<span class="type">Node</span>](numTrees)(<span class="type">Node</span>.emptyNode(nodeIndex = <span class="number">1</span>))</span><br><span class="line"><span class="comment">//将（树的索引，树的根节点）入队，树索引从 0 开始，根节点从 1 开始</span></span><br><span class="line"><span class="type">Range</span>(<span class="number">0</span>, numTrees).foreach(treeIndex =&gt; nodeQueue.enqueue((treeIndex, topNodes(treeIndex))))</span><br><span class="line"><span class="keyword">while</span> (nodeQueue.nonEmpty) &#123;</span><br><span class="line">    <span class="comment">// Collect some nodes to split, and choose features for each node (if subsampling).</span></span><br><span class="line">    <span class="comment">// Each group of nodes may come from one or multiple trees, and at multiple levels.</span></span><br><span class="line">    <span class="comment">// 取得每个树所有需要切分的节点,nodesForGroup表示需要切分的节点</span></span><br><span class="line">    <span class="keyword">val</span> (nodesForGroup, treeToNodeToIndexInfo) =</span><br><span class="line">        <span class="type">RandomForest</span>.selectNodesToSplit(nodeQueue, maxMemoryUsage, metadata, rng)</span><br><span class="line">    <span class="comment">//找出最优切点</span></span><br><span class="line">    <span class="type">DecisionTree</span>.findBestSplits(baggedInput, metadata, topNodes, nodesForGroup,</span><br><span class="line">        treeToNodeToIndexInfo, splits, bins, nodeQueue, timer, nodeIdCache = nodeIdCache)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;这里有两点需要重点介绍，第一点是取得每个树所有需要切分的节点，通过<code>RandomForest.selectNodesToSplit</code>方法实现；第二点是找出最优的切分，通过<code>DecisionTree.findBestSplits</code>方法实现。下面分别介绍这两点。</p>
<ul>
<li>取得每个树所有需要切分的节点</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[tree] <span class="function"><span class="keyword">def</span> <span class="title">selectNodesToSplit</span></span>(</span><br><span class="line">     nodeQueue: mutable.<span class="type">Queue</span>[(<span class="type">Int</span>, <span class="type">Node</span>)],</span><br><span class="line">     maxMemoryUsage: <span class="type">Long</span>,</span><br><span class="line">     metadata: <span class="type">DecisionTreeMetadata</span>,</span><br><span class="line">     rng: scala.util.<span class="type">Random</span>): (<span class="type">Map</span>[<span class="type">Int</span>, <span class="type">Array</span>[<span class="type">Node</span>]], <span class="type">Map</span>[<span class="type">Int</span>, <span class="type">Map</span>[<span class="type">Int</span>, <span class="type">NodeIndexInfo</span>]]) = &#123;</span><br><span class="line">   <span class="comment">// nodesForGroup保存需要切分的节点，treeIndex --&gt; nodes</span></span><br><span class="line">   <span class="keyword">val</span> mutableNodesForGroup = <span class="keyword">new</span> mutable.<span class="type">HashMap</span>[<span class="type">Int</span>, mutable.<span class="type">ArrayBuffer</span>[<span class="type">Node</span>]]()</span><br><span class="line">   <span class="comment">// mutableTreeToNodeToIndexInfo保存每个节点中选中特征的索引</span></span><br><span class="line">   <span class="comment">// treeIndex --&gt; (global) node index --&gt; (node index in group, feature indices)</span></span><br><span class="line">   <span class="comment">//(global) node index是树中的索引，组中节点索引的范围是[0, numNodesInGroup)</span></span><br><span class="line">   <span class="keyword">val</span> mutableTreeToNodeToIndexInfo =</span><br><span class="line">     <span class="keyword">new</span> mutable.<span class="type">HashMap</span>[<span class="type">Int</span>, mutable.<span class="type">HashMap</span>[<span class="type">Int</span>, <span class="type">NodeIndexInfo</span>]]()</span><br><span class="line">   <span class="keyword">var</span> memUsage: <span class="type">Long</span> = <span class="number">0</span>L</span><br><span class="line">   <span class="keyword">var</span> numNodesInGroup = <span class="number">0</span></span><br><span class="line">   <span class="keyword">while</span> (nodeQueue.nonEmpty &amp;&amp; memUsage &lt; maxMemoryUsage) &#123;</span><br><span class="line">     <span class="keyword">val</span> (treeIndex, node) = nodeQueue.head</span><br><span class="line">     <span class="comment">// Choose subset of features for node (if subsampling).</span></span><br><span class="line">     <span class="comment">// 选中特征子集</span></span><br><span class="line">     <span class="keyword">val</span> featureSubset: <span class="type">Option</span>[<span class="type">Array</span>[<span class="type">Int</span>]] = <span class="keyword">if</span> (metadata.subsamplingFeatures) &#123;</span><br><span class="line">       <span class="type">Some</span>(<span class="type">SamplingUtils</span>.reservoirSampleAndCount(<span class="type">Range</span>(<span class="number">0</span>,</span><br><span class="line">         metadata.numFeatures).iterator, metadata.numFeaturesPerNode, rng.nextLong)._1)</span><br><span class="line">     &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">       <span class="type">None</span></span><br><span class="line">     &#125;</span><br><span class="line">     <span class="comment">// Check if enough memory remains to add this node to the group.</span></span><br><span class="line">     <span class="comment">// 检查是否有足够的内存</span></span><br><span class="line">     <span class="keyword">val</span> nodeMemUsage = <span class="type">RandomForest</span>.aggregateSizeForNode(metadata, featureSubset) * <span class="number">8</span>L</span><br><span class="line">     <span class="keyword">if</span> (memUsage + nodeMemUsage &lt;= maxMemoryUsage) &#123;</span><br><span class="line">       nodeQueue.dequeue()</span><br><span class="line">       mutableNodesForGroup.getOrElseUpdate(treeIndex, <span class="keyword">new</span> mutable.<span class="type">ArrayBuffer</span>[<span class="type">Node</span>]()) += node</span><br><span class="line">       mutableTreeToNodeToIndexInfo</span><br><span class="line">         .getOrElseUpdate(treeIndex, <span class="keyword">new</span> mutable.<span class="type">HashMap</span>[<span class="type">Int</span>, <span class="type">NodeIndexInfo</span>]())(node.id)</span><br><span class="line">         = <span class="keyword">new</span> <span class="type">NodeIndexInfo</span>(numNodesInGroup, featureSubset)</span><br><span class="line">     &#125;</span><br><span class="line">     numNodesInGroup += <span class="number">1</span></span><br><span class="line">     memUsage += nodeMemUsage</span><br><span class="line">   &#125;</span><br><span class="line">   <span class="comment">// 将可变map转换为不可变map</span></span><br><span class="line">   <span class="keyword">val</span> nodesForGroup: <span class="type">Map</span>[<span class="type">Int</span>, <span class="type">Array</span>[<span class="type">Node</span>]] = mutableNodesForGroup.mapValues(_.toArray).toMap</span><br><span class="line">   <span class="keyword">val</span> treeToNodeToIndexInfo = mutableTreeToNodeToIndexInfo.mapValues(_.toMap).toMap</span><br><span class="line">   (nodesForGroup, treeToNodeToIndexInfo)</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>选中最优切分</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//所有可切分的节点</span></span><br><span class="line"><span class="keyword">val</span> nodes = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">Node</span>](numNodes)</span><br><span class="line">nodesForGroup.foreach &#123; <span class="keyword">case</span> (treeIndex, nodesForTree) =&gt;</span><br><span class="line">   nodesForTree.foreach &#123; node =&gt;</span><br><span class="line">     nodes(treeToNodeToIndexInfo(treeIndex)(node.id).nodeIndexInGroup) = node</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// In each partition, iterate all instances and compute aggregate stats for each node,</span></span><br><span class="line"><span class="comment">// yield an (nodeIndex, nodeAggregateStats) pair for each node.</span></span><br><span class="line"><span class="comment">// After a `reduceByKey` operation,</span></span><br><span class="line"><span class="comment">// stats of a node will be shuffled to a particular partition and be combined together,</span></span><br><span class="line"><span class="comment">// then best splits for nodes are found there.</span></span><br><span class="line"><span class="comment">// Finally, only best Splits for nodes are collected to driver to construct decision tree.</span></span><br><span class="line"><span class="comment">//获取节点对应的特征</span></span><br><span class="line"><span class="keyword">val</span> nodeToFeatures = getNodeToFeatures(treeToNodeToIndexInfo)</span><br><span class="line"><span class="keyword">val</span> nodeToFeaturesBc = input.sparkContext.broadcast(nodeToFeatures)</span><br><span class="line"><span class="keyword">val</span> partitionAggregates : <span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">DTStatsAggregator</span>)] = <span class="keyword">if</span> (nodeIdCache.nonEmpty) &#123;</span><br><span class="line">    input.zip(nodeIdCache.get.nodeIdsForInstances).mapPartitions &#123; points =&gt;</span><br><span class="line">      <span class="comment">// Construct a nodeStatsAggregators array to hold node aggregate stats,</span></span><br><span class="line">      <span class="comment">// each node will have a nodeStatsAggregator</span></span><br><span class="line">      <span class="keyword">val</span> nodeStatsAggregators = <span class="type">Array</span>.tabulate(numNodes) &#123; nodeIndex =&gt;</span><br><span class="line">          <span class="comment">//节点对应的特征集</span></span><br><span class="line">          <span class="keyword">val</span> featuresForNode = nodeToFeaturesBc.value.flatMap &#123; nodeToFeatures =&gt;</span><br><span class="line">            <span class="type">Some</span>(nodeToFeatures(nodeIndex))</span><br><span class="line">          &#125;</span><br><span class="line">          <span class="comment">// DTStatsAggregator，其中引用了 ImpurityAggregator，给出计算不纯度 impurity 的逻辑</span></span><br><span class="line">          <span class="keyword">new</span> <span class="type">DTStatsAggregator</span>(metadata, featuresForNode)</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">// 迭代当前分区的所有对象，更新聚合统计信息，统计信息即采样数据的权重值</span></span><br><span class="line">      points.foreach(binSeqOpWithNodeIdCache(nodeStatsAggregators, _))</span><br><span class="line">      <span class="comment">// transform nodeStatsAggregators array to (nodeIndex, nodeAggregateStats) pairs,</span></span><br><span class="line">      <span class="comment">// which can be combined with other partition using `reduceByKey`</span></span><br><span class="line">      nodeStatsAggregators.view.zipWithIndex.map(_.swap).iterator</span><br><span class="line">    &#125;</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      input.mapPartitions &#123; points =&gt;</span><br><span class="line">        <span class="comment">// Construct a nodeStatsAggregators array to hold node aggregate stats,</span></span><br><span class="line">        <span class="comment">// each node will have a nodeStatsAggregator</span></span><br><span class="line">        <span class="keyword">val</span> nodeStatsAggregators = <span class="type">Array</span>.tabulate(numNodes) &#123; nodeIndex =&gt;</span><br><span class="line">          <span class="comment">//节点对应的特征集</span></span><br><span class="line">          <span class="keyword">val</span> featuresForNode = nodeToFeaturesBc.value.flatMap &#123; nodeToFeatures =&gt;</span><br><span class="line">            <span class="type">Some</span>(nodeToFeatures(nodeIndex))</span><br><span class="line">          &#125;</span><br><span class="line">          <span class="comment">// DTStatsAggregator，其中引用了 ImpurityAggregator，给出计算不纯度 impurity 的逻辑</span></span><br><span class="line">          <span class="keyword">new</span> <span class="type">DTStatsAggregator</span>(metadata, featuresForNode)</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 迭代当前分区的所有对象，更新聚合统计信息</span></span><br><span class="line">        points.foreach(binSeqOp(nodeStatsAggregators, _))</span><br><span class="line">        <span class="comment">// transform nodeStatsAggregators array to (nodeIndex, nodeAggregateStats) pairs,</span></span><br><span class="line">        <span class="comment">// which can be combined with other partition using `reduceByKey`</span></span><br><span class="line">        nodeStatsAggregators.view.zipWithIndex.map(_.swap).iterator</span><br><span class="line">      &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">val</span> nodeToBestSplits = partitionAggregates.reduceByKey((a, b) =&gt; a.merge(b))</span><br><span class="line">    .map &#123; <span class="keyword">case</span> (nodeIndex, aggStats) =&gt;</span><br><span class="line">          <span class="keyword">val</span> featuresForNode = nodeToFeaturesBc.value.map &#123; nodeToFeatures =&gt;</span><br><span class="line">            nodeToFeatures(nodeIndex)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// find best split for each node</span></span><br><span class="line">    <span class="keyword">val</span> (split: <span class="type">Split</span>, stats: <span class="type">InformationGainStats</span>, predict: <span class="type">Predict</span>) =</span><br><span class="line">        binsToBestSplit(aggStats, splits, featuresForNode, nodes(nodeIndex))</span><br><span class="line">    (nodeIndex, (split, stats, predict))</span><br><span class="line">&#125;.collectAsMap()</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;该方法中的关键是对<code>binsToBestSplit</code>方法的调用，<code>binsToBestSplit</code>方法代码如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">binsToBestSplit</span></span>(</span><br><span class="line">      binAggregates: <span class="type">DTStatsAggregator</span>,</span><br><span class="line">      splits: <span class="type">Array</span>[<span class="type">Array</span>[<span class="type">Split</span>]],</span><br><span class="line">      featuresForNode: <span class="type">Option</span>[<span class="type">Array</span>[<span class="type">Int</span>]],</span><br><span class="line">      node: <span class="type">Node</span>): (<span class="type">Split</span>, <span class="type">InformationGainStats</span>, <span class="type">Predict</span>) = &#123;</span><br><span class="line">    <span class="comment">// 如果当前节点是根节点，计算预测和不纯度</span></span><br><span class="line">    <span class="keyword">val</span> level = <span class="type">Node</span>.indexToLevel(node.id)</span><br><span class="line">    <span class="keyword">var</span> predictWithImpurity: <span class="type">Option</span>[(<span class="type">Predict</span>, <span class="type">Double</span>)] = <span class="keyword">if</span> (level == <span class="number">0</span>) &#123;</span><br><span class="line">      <span class="type">None</span></span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="type">Some</span>((node.predict, node.impurity))</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 对各特征及切分点，计算其信息增益并从中选择最优 (feature, split)</span></span><br><span class="line">    <span class="keyword">val</span> (bestSplit, bestSplitStats) =</span><br><span class="line">      <span class="type">Range</span>(<span class="number">0</span>, binAggregates.metadata.numFeaturesPerNode).map &#123; featureIndexIdx =&gt;</span><br><span class="line">      <span class="keyword">val</span> featureIndex = <span class="keyword">if</span> (featuresForNode.nonEmpty) &#123;</span><br><span class="line">        featuresForNode.get.apply(featureIndexIdx)</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        featureIndexIdx</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">val</span> numSplits = binAggregates.metadata.numSplits(featureIndex)</span><br><span class="line">       <span class="comment">//特征为连续值的情况</span></span><br><span class="line">      <span class="keyword">if</span> (binAggregates.metadata.isContinuous(featureIndex)) &#123;</span><br><span class="line">        <span class="comment">// Cumulative sum (scanLeft) of bin statistics.</span></span><br><span class="line">        <span class="comment">// Afterwards, binAggregates for a bin is the sum of aggregates for</span></span><br><span class="line">        <span class="comment">// that bin + all preceding bins.</span></span><br><span class="line">        <span class="keyword">val</span> nodeFeatureOffset = binAggregates.getFeatureOffset(featureIndexIdx)</span><br><span class="line">        <span class="keyword">var</span> splitIndex = <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> (splitIndex &lt; numSplits) &#123;</span><br><span class="line">          binAggregates.mergeForFeature(nodeFeatureOffset, splitIndex + <span class="number">1</span>, splitIndex)</span><br><span class="line">          splitIndex += <span class="number">1</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// Find best split.</span></span><br><span class="line">        <span class="keyword">val</span> (bestFeatureSplitIndex, bestFeatureGainStats) =</span><br><span class="line">          <span class="type">Range</span>(<span class="number">0</span>, numSplits).map &#123; <span class="keyword">case</span> splitIdx =&gt;</span><br><span class="line">            <span class="comment">//计算 leftChild 及 rightChild 子节点的 impurity</span></span><br><span class="line">            <span class="keyword">val</span> leftChildStats = binAggregates.getImpurityCalculator(nodeFeatureOffset, splitIdx)</span><br><span class="line">            <span class="keyword">val</span> rightChildStats = binAggregates.getImpurityCalculator(nodeFeatureOffset, numSplits)</span><br><span class="line">            rightChildStats.subtract(leftChildStats)</span><br><span class="line">            <span class="comment">//求 impurity 的预测值，采用的是平均值计算</span></span><br><span class="line">            predictWithImpurity = <span class="type">Some</span>(predictWithImpurity.getOrElse(</span><br><span class="line">              calculatePredictImpurity(leftChildStats, rightChildStats)))</span><br><span class="line">            <span class="comment">//求信息增益 information gain 值，用于评估切分点是否最优,请参考决策树中1.4.4章节的介绍</span></span><br><span class="line">            <span class="keyword">val</span> gainStats = calculateGainForSplit(leftChildStats,</span><br><span class="line">              rightChildStats, binAggregates.metadata, predictWithImpurity.get._2)</span><br><span class="line">            (splitIdx, gainStats)</span><br><span class="line">          &#125;.maxBy(_._2.gain)</span><br><span class="line">        (splits(featureIndex)(bestFeatureSplitIndex), bestFeatureGainStats)</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">//无序离散特征时的情况</span></span><br><span class="line">      <span class="keyword">else</span> <span class="keyword">if</span> (binAggregates.metadata.isUnordered(featureIndex)) &#123;</span><br><span class="line">        <span class="comment">// Unordered categorical feature</span></span><br><span class="line">        <span class="keyword">val</span> (leftChildOffset, rightChildOffset) =</span><br><span class="line">          binAggregates.getLeftRightFeatureOffsets(featureIndexIdx)</span><br><span class="line">        <span class="keyword">val</span> (bestFeatureSplitIndex, bestFeatureGainStats) =</span><br><span class="line">          <span class="type">Range</span>(<span class="number">0</span>, numSplits).map &#123; splitIndex =&gt;</span><br><span class="line">            <span class="keyword">val</span> leftChildStats = binAggregates.getImpurityCalculator(leftChildOffset, splitIndex)</span><br><span class="line">            <span class="keyword">val</span> rightChildStats = binAggregates.getImpurityCalculator(rightChildOffset, splitIndex)</span><br><span class="line">            predictWithImpurity = <span class="type">Some</span>(predictWithImpurity.getOrElse(</span><br><span class="line">              calculatePredictImpurity(leftChildStats, rightChildStats)))</span><br><span class="line">            <span class="keyword">val</span> gainStats = calculateGainForSplit(leftChildStats,</span><br><span class="line">              rightChildStats, binAggregates.metadata, predictWithImpurity.get._2)</span><br><span class="line">            (splitIndex, gainStats)</span><br><span class="line">          &#125;.maxBy(_._2.gain)</span><br><span class="line">        (splits(featureIndex)(bestFeatureSplitIndex), bestFeatureGainStats)</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;<span class="comment">//有序离散特征时的情况</span></span><br><span class="line">        <span class="comment">// Ordered categorical feature</span></span><br><span class="line">        <span class="keyword">val</span> nodeFeatureOffset = binAggregates.getFeatureOffset(featureIndexIdx)</span><br><span class="line">        <span class="keyword">val</span> numBins = binAggregates.metadata.numBins(featureIndex)</span><br><span class="line">        <span class="comment">/* Each bin is one category (feature value).</span></span><br><span class="line"><span class="comment">         * The bins are ordered based on centroidForCategories, and this ordering determines which</span></span><br><span class="line"><span class="comment">         * splits are considered.  (With K categories, we consider K - 1 possible splits.)</span></span><br><span class="line"><span class="comment">         *</span></span><br><span class="line"><span class="comment">         * centroidForCategories is a list: (category, centroid)</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="comment">//多元分类时的情况</span></span><br><span class="line">        <span class="keyword">val</span> centroidForCategories = <span class="keyword">if</span> (binAggregates.metadata.isMulticlass) &#123;</span><br><span class="line">          <span class="comment">// For categorical variables in multiclass classification,</span></span><br><span class="line">          <span class="comment">// the bins are ordered by the impurity of their corresponding labels.</span></span><br><span class="line">          <span class="type">Range</span>(<span class="number">0</span>, numBins).map &#123; <span class="keyword">case</span> featureValue =&gt;</span><br><span class="line">            <span class="keyword">val</span> categoryStats = binAggregates.getImpurityCalculator(nodeFeatureOffset, featureValue)</span><br><span class="line">            <span class="keyword">val</span> centroid = <span class="keyword">if</span> (categoryStats.count != <span class="number">0</span>) &#123;</span><br><span class="line">              <span class="comment">// impurity 求的就是均方差</span></span><br><span class="line">              categoryStats.calculate()</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">              <span class="type">Double</span>.<span class="type">MaxValue</span></span><br><span class="line">            &#125;</span><br><span class="line">            (featureValue, centroid)</span><br><span class="line">          &#125;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123; <span class="comment">// 回归或二元分类时的情况</span></span><br><span class="line">          <span class="comment">// For categorical variables in regression and binary classification,</span></span><br><span class="line">          <span class="comment">// the bins are ordered by the centroid of their corresponding labels.</span></span><br><span class="line">          <span class="type">Range</span>(<span class="number">0</span>, numBins).map &#123; <span class="keyword">case</span> featureValue =&gt;</span><br><span class="line">            <span class="keyword">val</span> categoryStats = binAggregates.getImpurityCalculator(nodeFeatureOffset, featureValue)</span><br><span class="line">            <span class="keyword">val</span> centroid = <span class="keyword">if</span> (categoryStats.count != <span class="number">0</span>) &#123;</span><br><span class="line">              <span class="comment">//求的就是平均值作为 impurity</span></span><br><span class="line">              categoryStats.predict</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">              <span class="type">Double</span>.<span class="type">MaxValue</span></span><br><span class="line">            &#125;</span><br><span class="line">            (featureValue, centroid)</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// bins sorted by centroids</span></span><br><span class="line">        <span class="keyword">val</span> categoriesSortedByCentroid = centroidForCategories.toList.sortBy(_._2)</span><br><span class="line">        <span class="comment">// Cumulative sum (scanLeft) of bin statistics.</span></span><br><span class="line">        <span class="comment">// Afterwards, binAggregates for a bin is the sum of aggregates for</span></span><br><span class="line">        <span class="comment">// that bin + all preceding bins.</span></span><br><span class="line">        <span class="keyword">var</span> splitIndex = <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> (splitIndex &lt; numSplits) &#123;</span><br><span class="line">          <span class="keyword">val</span> currentCategory = categoriesSortedByCentroid(splitIndex)._1</span><br><span class="line">          <span class="keyword">val</span> nextCategory = categoriesSortedByCentroid(splitIndex + <span class="number">1</span>)._1</span><br><span class="line">          <span class="comment">//将两个箱子的状态信息进行合并</span></span><br><span class="line">          binAggregates.mergeForFeature(nodeFeatureOffset, nextCategory, currentCategory)</span><br><span class="line">          splitIndex += <span class="number">1</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// lastCategory = index of bin with total aggregates for this (node, feature)</span></span><br><span class="line">        <span class="keyword">val</span> lastCategory = categoriesSortedByCentroid.last._1</span><br><span class="line">        <span class="comment">// Find best split.</span></span><br><span class="line">        <span class="comment">//通过信息增益值选择最优切分点</span></span><br><span class="line">        <span class="keyword">val</span> (bestFeatureSplitIndex, bestFeatureGainStats) =</span><br><span class="line">          <span class="type">Range</span>(<span class="number">0</span>, numSplits).map &#123; splitIndex =&gt;</span><br><span class="line">            <span class="keyword">val</span> featureValue = categoriesSortedByCentroid(splitIndex)._1</span><br><span class="line">            <span class="keyword">val</span> leftChildStats =</span><br><span class="line">              binAggregates.getImpurityCalculator(nodeFeatureOffset, featureValue)</span><br><span class="line">            <span class="keyword">val</span> rightChildStats =</span><br><span class="line">              binAggregates.getImpurityCalculator(nodeFeatureOffset, lastCategory)</span><br><span class="line">            rightChildStats.subtract(leftChildStats)</span><br><span class="line">            predictWithImpurity = <span class="type">Some</span>(predictWithImpurity.getOrElse(</span><br><span class="line">              calculatePredictImpurity(leftChildStats, rightChildStats)))</span><br><span class="line">            <span class="keyword">val</span> gainStats = calculateGainForSplit(leftChildStats,</span><br><span class="line">              rightChildStats, binAggregates.metadata, predictWithImpurity.get._2)</span><br><span class="line">            (splitIndex, gainStats)</span><br><span class="line">          &#125;.maxBy(_._2.gain)</span><br><span class="line">        <span class="keyword">val</span> categoriesForSplit =</span><br><span class="line">          categoriesSortedByCentroid.map(_._1.toDouble).slice(<span class="number">0</span>, bestFeatureSplitIndex + <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">val</span> bestFeatureSplit =</span><br><span class="line">          <span class="keyword">new</span> <span class="type">Split</span>(featureIndex, <span class="type">Double</span>.<span class="type">MinValue</span>, <span class="type">Categorical</span>, categoriesForSplit)</span><br><span class="line">        (bestFeatureSplit, bestFeatureGainStats)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;.maxBy(_._2.gain)</span><br><span class="line">    (bestSplit, bestSplitStats, predictWithImpurity.get._1)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<h3 id="预测分析"><a href="#预测分析" class="headerlink" title="预测分析"></a>预测分析</h3><p>&emsp;&emsp;在利用随机森林进行预测时，调用的<code>predict</code>方法扩展自<code>TreeEnsembleModel</code>，它是树结构组合模型的表示，其核心代码如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//不同的策略采用不同的预测方法</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span></span>(features: <span class="type">Vector</span>): <span class="type">Double</span> = &#123;</span><br><span class="line">    (algo, combiningStrategy) <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> (<span class="type">Regression</span>, <span class="type">Sum</span>) =&gt;</span><br><span class="line">        predictBySumming(features)</span><br><span class="line">      <span class="keyword">case</span> (<span class="type">Regression</span>, <span class="type">Average</span>) =&gt;</span><br><span class="line">        predictBySumming(features) / sumWeights</span><br><span class="line">      <span class="keyword">case</span> (<span class="type">Classification</span>, <span class="type">Sum</span>) =&gt; <span class="comment">// binary classification</span></span><br><span class="line">        <span class="keyword">val</span> prediction = predictBySumming(features)</span><br><span class="line">        <span class="comment">// <span class="doctag">TODO:</span> predicted labels are +1 or -1 for GBT. Need a better way to store this info.</span></span><br><span class="line">        <span class="keyword">if</span> (prediction &gt; <span class="number">0.0</span>) <span class="number">1.0</span> <span class="keyword">else</span> <span class="number">0.0</span></span><br><span class="line">      <span class="keyword">case</span> (<span class="type">Classification</span>, <span class="type">Vote</span>) =&gt;</span><br><span class="line">        predictByVoting(features)</span><br><span class="line">      <span class="keyword">case</span> _ =&gt;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalArgumentException</span>()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">predictBySumming</span></span>(features: <span class="type">Vector</span>): <span class="type">Double</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> treePredictions = trees.map(_.predict(features))</span><br><span class="line">    <span class="comment">//两个向量的内集</span></span><br><span class="line">    blas.ddot(numTrees, treePredictions, <span class="number">1</span>, treeWeights, <span class="number">1</span>)</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//通过投票选举</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">predictByVoting</span></span>(features: <span class="type">Vector</span>): <span class="type">Double</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> votes = mutable.<span class="type">Map</span>.empty[<span class="type">Int</span>, <span class="type">Double</span>]</span><br><span class="line">    trees.view.zip(treeWeights).foreach &#123; <span class="keyword">case</span> (tree, weight) =&gt;</span><br><span class="line">      <span class="keyword">val</span> prediction = tree.predict(features).toInt</span><br><span class="line">      votes(prediction) = votes.getOrElse(prediction, <span class="number">0.0</span>) + weight</span><br><span class="line">    &#125;</span><br><span class="line">    votes.maxBy(_._2)._1</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p>【1】机器学习.周志华</p>
<p>【2】<a target="_blank" rel="noopener" href="https://www.ibm.com/developerworks/cn/opensource/os-cn-spark-random-forest/">Spark 随机森林算法原理、源码分析及案例实战</a></p>
<p>【3】<a target="_blank" rel="noopener" href="https://spark-summit.org/wp-content/uploads/2014/07/Scalable-Distributed-Decision-Trees-in-Spark-Made-Das-Sparks-Talwalkar.pdf">Scalable Distributed Decision Trees in Spark MLlib</a></p>
</div><div class="article-licensing box"><div class="licensing-title"><p>随机森林</p><p><a href="https://huzhiliang.com/2019/07/25/random-forests/">https://huzhiliang.com/2019/07/25/random-forests/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>作者</h6><p>ฅ´ω`ฅ</p></div></div><div class="level-item is-narrow"><div><h6>发布于</h6><p>2019-07-25</p></div></div><div class="level-item is-narrow"><div><h6>更新于</h6><p>2019-07-26</p></div></div><div class="level-item is-narrow"><div><h6>许可协议</h6><p><a class="icon" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a><a class="icon" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a><a class="icon" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/spark/">spark</a><a class="link-muted mr-2" rel="tag" href="/tags/%E7%BB%84%E5%90%88%E6%A0%91/">组合树</a></div><div class="notification is-danger">You need to set <code>install_url</code> to use ShareThis. Please set it in <code>_config.yml</code>.</div></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">喜欢这篇文章？打赏一下作者吧</h3><div class="buttons is-centered"><a class="button donate" href="/" target="_blank" rel="noopener" data-type="afdian"><span class="icon is-small"><i class="fas fa-charging-station"></i></span><span>爱发电</span></a><a class="button donate" data-type="alipay"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>支付宝</span><span class="qrcode"><img src="/" alt="支付宝"></span></a><a class="button donate" href="/" target="_blank" rel="noopener" data-type="buymeacoffee"><span class="icon is-small"><i class="fas fa-coffee"></i></span><span>送我杯咖啡</span></a><a class="button donate" href="/" target="_blank" rel="noopener" data-type="patreon"><span class="icon is-small"><i class="fab fa-patreon"></i></span><span>Patreon</span></a><div class="notification is-danger">You forgot to set the <code>business</code> or <code>currency_code</code> for Paypal. Please set it in <code>_config.yml</code>.</div><a class="button donate" data-type="wechat"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>微信</span><span class="qrcode"><img src="/" alt="微信"></span></a></div></div></div><div class="card"><nav class="post-navigation mt-4 level is-mobile card-content"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2019/07/28/bisecting-k-means/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">二分`k-means`算法</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2019/07/24/gbts/"><span class="level-item">梯度提升树</span><i class="level-item fas fa-chevron-right"></i></a></div></nav></div><div class="card"><div class="card-content"><h3 class="title is-5">评论</h3><div class="notification is-danger">You forgot to set the <code>shortname</code> for Disqus. Please set it in <code>_config.yml</code>.</div></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/avatar.png" alt="Your name"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Your name</p><p class="is-size-6 is-block">Your title</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Your location</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives"><p class="title">161</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories"><p class="title">14</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags"><p class="title">97</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/ppoffice" target="_blank" rel="noopener" id="widget-follow">微博 Weibo</a></div><div class="level is-mobile"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/ppoffice"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">链接</h3><ul class="menu-list"><li><a class="level is-mobile is-mobile" href="https://hexo.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Hexo</span></span><span class="level-right"><span class="level-item tag">hexo.io</span></span></a></li><li><a class="level is-mobile is-mobile" href="https://bulma.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Bulma</span></span><span class="level-right"><span class="level-item tag">bulma.io</span></span></a></li></ul></div><a class="link-more button is-light is-small size-small" href="/friends/">查看更多</a></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/C/"><span class="level-start"><span class="level-item">C++</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Golang/"><span class="level-start"><span class="level-item">Golang</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Kaggle/"><span class="level-start"><span class="level-item">Kaggle</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/LeetCode/"><span class="level-start"><span class="level-item">LeetCode</span></span><span class="level-end"><span class="level-item tag">85</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/"><span class="level-start"><span class="level-item">Python</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"><span class="level-start"><span class="level-item">数据结构</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"><span class="level-start"><span class="level-item">机器学习</span></span><span class="level-end"><span class="level-item tag">26</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"><span class="level-start"><span class="level-item">深度学习</span></span><span class="level-end"><span class="level-item tag">29</span></span></a><ul><li><a class="level is-mobile" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/CNN/"><span class="level-start"><span class="level-item">CNN</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">16</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/"><span class="level-start"><span class="level-item">迁移学习</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%E7%A0%B4%E8%A7%A3/"><span class="level-start"><span class="level-item">破解</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E7%AE%97%E6%B3%95/"><span class="level-start"><span class="level-item">算法</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%8B%B1%E8%AF%AD%E5%AD%A6%E4%B9%A0/"><span class="level-start"><span class="level-item">英语学习</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">最新文章</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2020-03-05T05:41:28.000Z">2020-03-05</time></p><p class="title"><a href="/2020/03/05/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BACRF-%E4%B8%89-%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E4%B8%8E%E7%BB%B4%E7%89%B9%E6%AF%94%E7%AE%97%E6%B3%95%E8%A7%A3%E7%A0%81/">条件随机场CRF(三) 模型学习与维特比算法解码</a></p><p class="categories"><a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a> / <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP/">NLP</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2020-03-04T08:24:55.000Z">2020-03-04</time></p><p class="title"><a href="/2020/03/04/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BACRF-%E4%BA%8C-%E5%89%8D%E5%90%91%E5%90%8E%E5%90%91%E7%AE%97%E6%B3%95%E8%AF%84%E4%BC%B0%E6%A0%87%E8%AE%B0%E5%BA%8F%E5%88%97%E6%A6%82%E7%8E%87/">条件随机场CRF(二) 前向后向算法评估标记序列概率</a></p><p class="categories"><a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a> / <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP/">NLP</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2020-03-03T02:59:49.000Z">2020-03-03</time></p><p class="title"><a href="/2020/03/03/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BACRF-%E4%B8%80-%E4%BB%8E%E9%9A%8F%E6%9C%BA%E5%9C%BA%E5%88%B0%E7%BA%BF%E6%80%A7%E9%93%BE%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/">条件随机场CRF(一)从随机场到线性链条件随机场</a></p><p class="categories"><a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a> / <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP/">NLP</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2020-03-02T02:46:01.000Z">2020-03-02</time></p><p class="title"><a href="/2020/03/02/%E6%96%87%E6%9C%AC%E6%8C%96%E6%8E%98%E7%9A%84%E5%88%86%E8%AF%8D%E5%8E%9F%E7%90%86/">文本挖掘的分词原理</a></p><p class="categories"><a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a> / <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP/">NLP</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2020-02-20T01:01:00.000Z">2020-02-20</time></p><p class="title"><a href="/2020/02/20/optimize-water-distribution-in-a-village/">optimize water distribution in a village</a></p><p class="categories"><a href="/categories/LeetCode/">LeetCode</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">归档</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2020/03/"><span class="level-start"><span class="level-item">三月 2020</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/02/"><span class="level-start"><span class="level-item">二月 2020</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/01/"><span class="level-start"><span class="level-item">一月 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/12/"><span class="level-start"><span class="level-item">十二月 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/11/"><span class="level-start"><span class="level-item">十一月 2019</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/10/"><span class="level-start"><span class="level-item">十月 2019</span></span><span class="level-end"><span class="level-item tag">33</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/09/"><span class="level-start"><span class="level-item">九月 2019</span></span><span class="level-end"><span class="level-item tag">16</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/08/"><span class="level-start"><span class="level-item">八月 2019</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/07/"><span class="level-start"><span class="level-item">七月 2019</span></span><span class="level-end"><span class="level-item tag">17</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/05/"><span class="level-start"><span class="level-item">五月 2019</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/04/"><span class="level-start"><span class="level-item">四月 2019</span></span><span class="level-end"><span class="level-item tag">12</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/03/"><span class="level-start"><span class="level-item">三月 2019</span></span><span class="level-end"><span class="level-item tag">17</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/02/"><span class="level-start"><span class="level-item">二月 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/01/"><span class="level-start"><span class="level-item">一月 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/08/"><span class="level-start"><span class="level-item">八月 2018</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/07/"><span class="level-start"><span class="level-item">七月 2018</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/05/"><span class="level-start"><span class="level-item">五月 2018</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/03/"><span class="level-start"><span class="level-item">三月 2018</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/02/"><span class="level-start"><span class="level-item">二月 2018</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/01/"><span class="level-start"><span class="level-item">一月 2018</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2017/12/"><span class="level-start"><span class="level-item">十二月 2017</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2017/11/"><span class="level-start"><span class="level-item">十一月 2017</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2017/10/"><span class="level-start"><span class="level-item">十月 2017</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2017/09/"><span class="level-start"><span class="level-item">九月 2017</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2017/07/"><span class="level-start"><span class="level-item">七月 2017</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2017/06/"><span class="level-start"><span class="level-item">六月 2017</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2017/05/"><span class="level-start"><span class="level-item">五月 2017</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2017/03/"><span class="level-start"><span class="level-item">三月 2017</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2017/02/"><span class="level-start"><span class="level-item">二月 2017</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/archives/2015/04/"><span class="level-start"><span class="level-item">四月 2015</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2015/02/"><span class="level-start"><span class="level-item">二月 2015</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">标签</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/Array/"><span class="tag">Array</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Attention/"><span class="tag">Attention</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Binary-Tree/"><span class="tag">Binary Tree</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/C/"><span class="tag">C++</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CFG/"><span class="tag">CFG</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CNN/"><span class="tag">CNN</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CPU/"><span class="tag">CPU</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CYK/"><span class="tag">CYK</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Caffee/"><span class="tag">Caffee</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/English/"><span class="tag">English</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Facebook/"><span class="tag">Facebook</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/GAN/"><span class="tag">GAN</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/GBDT/"><span class="tag">GBDT</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/GPU/"><span class="tag">GPU</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/GRN/"><span class="tag">GRN</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/GloVe/"><span class="tag">GloVe</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Go/"><span class="tag">Go</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Google-apac/"><span class="tag">Google apac</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Go%E8%AF%AD%E8%A8%80/"><span class="tag">Go语言</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Greedy/"><span class="tag">Greedy</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/HMM/"><span class="tag">HMM</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/IBM-Modes/"><span class="tag">IBM Modes</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/KL%E6%95%A3%E5%BA%A6/"><span class="tag">KL散度</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/LSTM/"><span class="tag">LSTM</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/LeetCode/"><span class="tag">LeetCode</span><span class="tag">31</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Lexicalized-PCFG/"><span class="tag">Lexicalized PCFG</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Lexized-PCFG/"><span class="tag">Lexized PCFG</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Log-Linear-Models/"><span class="tag">Log-Linear Models</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Machine-Learning/"><span class="tag">Machine Learning</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/PCFG/"><span class="tag">PCFG</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Paper/"><span class="tag">Paper</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Programmer/"><span class="tag">Programmer</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Python/"><span class="tag">Python</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Pytorch/"><span class="tag">Pytorch</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Queue/"><span class="tag">Queue</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/RNN/"><span class="tag">RNN</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Seq2seq/"><span class="tag">Seq2seq</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Tagging/"><span class="tag">Tagging</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Tencent/"><span class="tag">Tencent</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Tensorflow/"><span class="tag">Tensorflow</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/VIP/"><span class="tag">VIP</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Viterbi/"><span class="tag">Viterbi</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Word-representation/"><span class="tag">Word representation</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/XGBoost/"><span class="tag">XGBoost</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/exhaustive-search/"><span class="tag">exhaustive search</span><span class="tag">7</span></a></div><div class="control"><a class="tags has-addons" href="/tags/git/"><span class="tag">git</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/github/"><span class="tag">github</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/numpy/"><span class="tag">numpy</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/python/"><span class="tag">python</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/spark/"><span class="tag">spark</span><span class="tag">23</span></a></div><div class="control"><a class="tags has-addons" href="/tags/turtle/"><span class="tag">turtle</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/word2vec/"><span class="tag">word2vec</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E4%B8%93%E9%A2%98/"><span class="tag">专题</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E4%BA%8C%E5%88%86%E6%90%9C%E7%B4%A2/"><span class="tag">二分搜索</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E4%BA%8C%E5%88%86%E6%90%9C%E7%B4%A2%E6%A0%91/"><span class="tag">二分搜索树</span><span class="tag">9</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E4%BA%8C%E5%88%86%E6%A0%91/"><span class="tag">二分树</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E4%BA%8C%E8%BF%9B%E5%88%B6/"><span class="tag">二进制</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/"><span class="tag">优化算法</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%88%86%E7%B1%BB/"><span class="tag">分类</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%88%86%E8%AF%8D/"><span class="tag">分词</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%89%91%E6%8C%87offer/"><span class="tag">剑指offer</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%8A%A8%E6%80%81%E5%9B%9E%E5%BD%92/"><span class="tag">动态回归</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"><span class="tag">动态规划</span><span class="tag">22</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%9B%9E%E5%BD%92/"><span class="tag">回归</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%9B%BE%E8%AE%BA/"><span class="tag">图论</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%8E%92%E5%BA%8F/"><span class="tag">排序</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%95%B0%E5%AD%A6/"><span class="tag">数学</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86/"><span class="tag">数学原理</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"><span class="tag">数据分析</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/"><span class="tag">条件随机场</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6/"><span class="tag">极大似然</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9/"><span class="tag">模型压缩</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/"><span class="tag">模型部署</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"><span class="tag">深度学习</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%89%88%E6%9D%83/"><span class="tag">版权</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%89%B9%E5%BE%81%E6%8A%BD%E5%8F%96/"><span class="tag">特征抽取</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%AC%94%E8%AE%B0/"><span class="tag">笔记</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%AE%97%E6%B3%95/"><span class="tag">算法</span><span class="tag">9</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86/"><span class="tag">算法原理</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%AE%97%E6%B3%95%E5%A4%8D%E6%9D%82%E5%BA%A6/"><span class="tag">算法复杂度</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/"><span class="tag">线性模型</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%BB%84%E5%90%88%E6%A0%91/"><span class="tag">组合树</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C/"><span class="tag">经典网络</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%BB%9F%E8%AE%A1%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91/"><span class="tag">统计机器翻译</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%BD%91%E6%98%93%E4%BA%91%E9%9F%B3%E4%B9%90/"><span class="tag">网易云音乐</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%81%9A%E7%B1%BB/"><span class="tag">聚类</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%83%8C%E5%8C%85%E9%97%AE%E9%A2%98/"><span class="tag">背包问题</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%84%9A%E6%9C%AC/"><span class="tag">脚本</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%AF%AD%E6%B3%95/"><span class="tag">语法</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/"><span class="tag">语言模型</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E6%95%B4/"><span class="tag">超参数调整</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/"><span class="tag">迁移学习</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E9%80%92%E5%BD%92/"><span class="tag">递归</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E9%82%AE%E4%BB%B6/"><span class="tag">邮件</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E9%93%BE%E8%A1%A8/"><span class="tag">链表</span><span class="tag">19</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E9%99%8D%E7%BB%B4/"><span class="tag">降维</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E9%9D%A2%E8%AF%95%E4%B8%93%E5%9C%BA/"><span class="tag">面试专场</span><span class="tag">2</span></a></div></div></div></div></div><div class="card widget" id="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">订阅更新</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button is-primary" type="submit" value="订阅"></div></div></form></div></div></div><div class="card widget"><div class="card-content"><div class="notification is-danger">You need to set <code>client_id</code> and <code>slot_id</code> to show this AD unit. Please set it in <code>_config.yml</code>.</div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img class="logo-img" src="/" alt="MCFON" height="28"><img class="logo-img-dark" src="/" alt="MCFON" height="28"></a><p class="is-size-7"><span>&copy; 2020 ฅ´ω`ฅ</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/imaegoo/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a><a href="http://www.miitbeian.gov.cn" target="_blank">豫ICP备18017229号</a> - </p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" async></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/mhchem.js" defer></script><script>window.addEventListener("load", function() {
            document.querySelectorAll('[role="article"] > .content').forEach(function(element) {
                renderMathInElement(element);
            });
        });</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><div class="searchbox-pinyin"><label class="checkbox"><input id="search-by-pinyin" type="checkbox" checked="checked"><span> 拼音检索</span></label></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/imaegoo/pinyin.js" defer></script><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script><script type="text/javascript" src="/js/imaegoo/imaegoo.js"></script><script type="text/javascript" src="/js/imaegoo/universe.js"></script><script type="text/javascript" src="/js/live2d/autoload.js"></script></body></html>