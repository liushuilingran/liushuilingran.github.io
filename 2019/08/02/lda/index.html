<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>隐式狄利克雷分布 - MCFON</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="MCFON"><meta name="msapplication-TileImage" content="/img/favicon.png"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="MCFON"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta description="&amp;amp;emsp;&amp;amp;emsp;LDA是一种概率主题模型：隐式狄利克雷分布（Latent Dirichlet Allocation，简称LDA）。LDA是2003年提出的一种主题模型，它可以将文档集中每篇文档的主题以概率分布的形式给出。通过分析一些文档，我们可以抽取出它们的主题（分布），根据主题（分布）进行主题聚类或文本分类。同时，它是一种典型的词袋模型，即一篇文档是由一组词构成，词与词之间没有先后顺序的"><meta property="og:type" content="blog"><meta property="og:title" content="隐式狄利克雷分布"><meta property="og:url" content="https://huzhiliang.com/2019/08/02/lda/"><meta property="og:site_name" content="MCFON"><meta property="og:description" content="&amp;amp;emsp;&amp;amp;emsp;LDA是一种概率主题模型：隐式狄利克雷分布（Latent Dirichlet Allocation，简称LDA）。LDA是2003年提出的一种主题模型，它可以将文档集中每篇文档的主题以概率分布的形式给出。通过分析一些文档，我们可以抽取出它们的主题（分布），根据主题（分布）进行主题聚类或文本分类。同时，它是一种典型的词袋模型，即一篇文档是由一组词构成，词与词之间没有先后顺序的"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://huzhiliang.com/gallery/7.jpg"><meta property="article:published_time" content="2019-08-02T08:00:26.000Z"><meta property="article:modified_time" content="2019-08-03T03:29:55.166Z"><meta property="article:author" content="ฅ´ω`ฅ"><meta property="article:tag" content="spark"><meta property="article:tag" content="聚类"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/gallery/7.jpg"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://huzhiliang.com/2019/08/02/lda/"},"headline":"MCFON","image":["https://huzhiliang.com/gallery/7.jpg"],"datePublished":"2019-08-02T08:00:26.000Z","dateModified":"2019-08-03T03:29:55.166Z","author":{"@type":"Person","name":"ฅ´ω`ฅ"},"description":"&amp;emsp;&amp;emsp;LDA是一种概率主题模型：隐式狄利克雷分布（Latent Dirichlet Allocation，简称LDA）。LDA是2003年提出的一种主题模型，它可以将文档集中每篇文档的主题以概率分布的形式给出。通过分析一些文档，我们可以抽取出它们的主题（分布），根据主题（分布）进行主题聚类或文本分类。同时，它是一种典型的词袋模型，即一篇文档是由一组词构成，词与词之间没有先后顺序的"}</script><link rel="canonical" href="https://huzhiliang.com/2019/08/02/lda/"><link rel="icon" href="/img/favicon.png"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.3.0"></head><body class="is-2-column"><script type="text/javascript" src="/js/imaegoo/night.js"></script><canvas id="universe"></canvas><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img class="logo-img" src="/" alt="MCFON" height="28"><img class="logo-img-dark" src="/" alt="MCFON" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">首页</a><a class="navbar-item" href="/archives/">时间轴</a><a class="navbar-item" href="/categories/">分类</a><a class="navbar-item" href="/tags/">标签</a><a class="navbar-item" target="_blank" rel="noopener" href="https://imaegoo.azurewebsites.net">网盘</a><a class="navbar-item" href="/messages/">留言板</a><a class="navbar-item" href="/friends/">友情链接</a><a class="navbar-item" href="/about/">关于</a></div><div class="navbar-end"><a class="navbar-item night" id="night-nav" title="Night Mode" href="javascript:;"><i class="fas fa-moon" id="night-icon"></i></a><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2019-08-02T08:00:26.000Z" title="2019-08-02T08:00:26.000Z">2019-08-02</time>发表</span><span class="level-item"><time dateTime="2019-08-03T03:29:55.166Z" title="2019-08-03T03:29:55.166Z">2019-08-03</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></span><span class="level-item">1 小时读完 (大约10993个字)</span><span class="level-item leancloud_visitors" id="/2019/08/02/lda/" data-flag-title="隐式狄利克雷分布"><i class="far fa-eye"></i>&nbsp;&nbsp;<span id="twikoo_visitors"><i class="fa fa-spinner fa-spin"></i></span>次访问</span></div></div><h1 class="title is-3 is-size-4-mobile">隐式狄利克雷分布</h1><div class="content"><p>&emsp;&emsp;<code>LDA</code>是一种概率主题模型：隐式狄利克雷分布（<code>Latent Dirichlet Allocation</code>，简称<code>LDA</code>）。<code>LDA</code>是2003年提出的一种<a target="_blank" rel="noopener" href="http://zh.wikipedia.org/wiki/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B">主题模型</a>，它可以将文档集中每篇文档的主题以概率分布的形式给出。<br>通过分析一些文档，我们可以抽取出它们的主题（分布），根据主题（分布）进行主题聚类或文本分类。同时，它是一种典型的词袋模型，即一篇文档是由一组词构成，词与词之间没有先后顺序的关系。一篇文档可以包含多个主题，文档中每一个词都由其中的一个主题生成。</p>
<a id="more"></a>
<p>&emsp;&emsp;举一个简单的例子，比如假设事先给定了这几个主题：<code>Arts、Budgets、Children、Education</code>，然后通过学习的方式，获取每个主题<code>Topic</code>对应的词语，如下图所示：</p>
<div  align="center"><img src="../images/imgs12/topic_words.png" width = "550" height = "300" alt="topic_words" align="center" /></div><br>

<p>&emsp;&emsp;然后以一定的概率选取上述某个主题，再以一定的概率选取那个主题下的某个单词，不断的重复这两步，最终生成如下图所示的一篇文章（不同颜色的词语分别表示不同主题）。</p>
<div  align="center"><img src="../images/imgs12/docs.png" width = "550" height = "310" alt="docs" align="center" /></div><br>

<p>&emsp;&emsp;我们看到一篇文章后，往往会推测这篇文章是如何生成的，我们通常认为作者会先确定几个主题，然后围绕这几个主题遣词造句写成全文。<code>LDA</code>要干的事情就是根据给定的文档，判断它的主题分布。在<code>LDA</code>模型中，生成文档的过程有如下几步：</p>
<ul>
<li><p>从狄利克雷分布$\alpha$中生成文档i的主题分布$\theta_{i}$ ；</p>
</li>
<li><p>从主题的多项式分布$\theta_{i}$中取样生成文档i第j个词的主题$Z_{i,j}$ ；</p>
</li>
<li><p>从狄利克雷分布$\eta$中取样生成主题$Z_{i,j}$对应的词语分布$\beta_{i,j}$ ；</p>
</li>
<li><p>从词语的多项式分布$\beta_{i,j}$中采样最终生成词语$W_{i,j}$ .</p>
</li>
</ul>
<p>&emsp;&emsp;<code>LDA</code>的图模型结构如下图所示：</p>
<div  align="center"><img src="../images/imgs12/LDA.png" width = "415" height = "195" alt="topic_words" align="center" /></div><br>

<p>&emsp;&emsp;<code>LDA</code>会涉及很多数学知识，后面的章节我会首先介绍<code>LDA</code>涉及的数学知识，然后在这些数学知识的基础上详细讲解<code>LDA</code>的原理。</p>
<h1 id="数学预备"><a href="#数学预备" class="headerlink" title="数学预备"></a>数学预备</h1><h2 id="Gamma函数"><a href="#Gamma函数" class="headerlink" title="Gamma函数"></a>Gamma函数</h2><p>&emsp;&emsp;在高等数学中，有一个长相奇特的<code>Gamma</code>函数</p>
<div  align="center"><img src="../images/imgs12/1.1.1.png" width = "240" height = "50" alt="gamma函数" align="center" /></div><br>

<p>&emsp;&emsp;通过分部积分，可以推导<code>gamma</code>函数有如下递归性质</p>
<div  align="center"><img src="../images/imgs12/1.1.2.png" width = "200" height = "30" alt="gamma函数" align="center" /></div><br>

<p>&emsp;&emsp;通过该递归性质，我们可以很容易证明，<code>gamma</code>函数可以被当成阶乘在实数集上的延拓，具有如下性质</p>
<div  align="center"><img src="../images/imgs12/1.1.3.png" width = "165" height = "30" alt="gamma函数" align="center" /></div><br>

<h2 id="Digamma函数"><a href="#Digamma函数" class="headerlink" title="Digamma函数"></a>Digamma函数</h2><p>&emsp;&emsp;如下函数被称为<code>Digamma</code>函数，它是<code>Gamma</code>函数对数的一阶导数</p>
<div  align="center"><img src="../images/imgs12/1.2.1.png" width = "150" height = "40" alt="digamma函数" align="center" /></div><br>

<p>&emsp;&emsp;这是一个很重要的函数，在涉及<code>Dirichlet</code>分布相关的参数的极大似然估计时，往往需要用到这个函数。<code>Digamma</code>函数具有如下一个漂亮的性质</p>
<div  align="center"><img src="../images/imgs12/1.2.2.png" width = "200" height = "50" alt="digamma函数" align="center" /></div><br>

<h2 id="二项分布（Binomial-distribution）"><a href="#二项分布（Binomial-distribution）" class="headerlink" title="二项分布（Binomial distribution）"></a>二项分布（Binomial distribution）</h2><p>&emsp;&emsp;二项分布是由伯努利分布推出的。伯努利分布，又称两点分布或<code>0-1</code>分布，是一个离散型的随机分布，其中的随机变量只有两类取值，即0或者1。二项分布是重复<code>n</code>次的伯努利试验。简言之，只做一次实验，是伯努利分布，重复做了n次，是二项分布。二项分布的概率密度函数为：</p>
<div  align="center"><img src="../images/imgs12/1.3.1.png" width = "260" height = "25" alt="二项分布密度函数" align="center" /></div><br>

<p>&emsp;&emsp;对于k=1,2，…,n，其中<code>C(n,k)</code>是二项式系数（这就是二项分布的名称的由来）</p>
<div  align="center"><img src="../images/imgs12/1.3.2.png" width = "180" height = "49" alt="二项分布密度函数" align="center" /></div><br>

<h2 id="多项分布"><a href="#多项分布" class="headerlink" title="多项分布"></a>多项分布</h2><p>&emsp;&emsp;多项分布是二项分布扩展到多维的情况。多项分布是指单次试验中的随机变量的取值不再是<code>0-1</code>，而是有多种离散值可能<code>（1,2,3...,k）</code>。比如投掷6个面的骰子实验，<code>N</code>次实验结果服从<code>K=6</code>的多项分布。其中：</p>
<div  align="center"><img src="../images/imgs12/1.4.1.png" width = "158" height = "69" alt="多项分布" align="center" /></div><br>

<p>&emsp;&emsp;多项分布的概率密度函数为：</p>
<div  align="center"><img src="../images/imgs12/1.4.2.png" width = "410" height = "55" alt="多项分布密度函数" align="center" /></div><br>

<h2 id="Beta分布"><a href="#Beta分布" class="headerlink" title="Beta分布"></a>Beta分布</h2><h3 id="Beta分布-1"><a href="#Beta分布-1" class="headerlink" title="Beta分布"></a>Beta分布</h3><p>&emsp;&emsp;首先看下面的问题1（问题1到问题4都取自于文献【1】）。</p>
<p>&emsp;&emsp;<strong>问题1：</strong></p>
<div  align="center"><img src="../images/imgs12/question1.png" width = "430" height = "70" alt="问题1" align="center" /></div><br>

<p>&emsp;&emsp; 为解决这个问题，可以尝试计算$x_{(k)}$落在区间<code>[x,x+delta x]</code>的概率。首先，把<code>[0,1]</code>区间分成三段<code>[0,x)</code>,<code>[x,x+delta x]</code>，<code>(x+delta x,1]</code>，然后考虑下简单的情形：即假设n个数中只有1个落在了区间<code>[x,x+delta x]</code>内，由于这个区间内的数<code>X(k)</code>是第k大的，所以<code>[0,x)</code>中应该有k−1个数，<code>(x+delta x,1]</code>这个区间中应该有n−k个数。<br>如下图所示：</p>
<div  align="center"><img src="../images/imgs12/1.5.1.png" width = "450" height = "140" alt="多项分布密度函数" align="center" /></div><br>

<p>&emsp;&emsp;上述问题可以转换为下述事件<code>E</code>：</p>
<div  align="center"><img src="../images/imgs12/1.5.2.png" width = "250" height = "75" alt="事件E" align="center" /></div><br>

<p>&emsp;&emsp;对于上述事件<code>E</code>，有：</p>
<div  align="center"><img src="../images/imgs12/1.5.3.png" width = "255" height = "100" alt="事件E" align="center" /></div><br>

<p>&emsp;&emsp;其中，<code>o(delta x)</code>表示<code>delta x</code>的高阶无穷小。显然，由于不同的排列组合，即n个数中有一个落在<code>[x,x+delta x]</code>区间的有n种取法，余下n−1个数中有k−1个落在<code>[0,x)</code>的有<code>C(n-1,k-1)</code>种组合。所以和事件E等价的事件一共有<code>nC(n-1,k-1)</code>个。</p>
<p>&emsp;&emsp;文献【1】中证明，只要落在<code>[x,x+delta x]</code>内的数字超过一个，则对应的事件的概率就是<code>o(delta x)</code>。所以$x_{(k)}$的概率密度函数为：</p>
<div  align="center"><img src="../images/imgs12/1.5.4.png" width = "340" height = "120" alt="概率密度函数" align="center" /></div><br>

<p>&emsp;&emsp;利用<code>Gamma</code>函数，我们可以将<code>f(x)</code>表示成如下形式：</p>
<div  align="center"><img src="../images/imgs12/1.5.5.png" width = "260" height = "40" alt="概率密度函数" align="center" /></div><br>

<p>&emsp;&emsp;在上式中，我们用<code>alpha=k</code>，<code>beta=n-k+1</code>替换，可以得到<code>beta</code>分布的概率密度函数</p>
<div  align="center"><img src="../images/imgs12/1.5.6.png" width = "230" height = "45" alt="beta分布概率密度函数" align="center" /></div><br>

<h3 id="共轭先验分布"><a href="#共轭先验分布" class="headerlink" title="共轭先验分布"></a>共轭先验分布</h3><p>&emsp;&emsp;什么是共轭呢？轭的意思是束缚、控制。共轭从字面上理解，则是共同约束，或互相约束。在贝叶斯概率理论中，如果后验概率<code>P(z|x)</code>和先验概率<code>p(z)</code>满足同样的分布，那么，先验分布和后验分布被叫做共轭分布，同时，先验分布叫做似然函数的共轭先验分布。</p>
<h3 id="Beta-Binomial-共轭"><a href="#Beta-Binomial-共轭" class="headerlink" title="Beta-Binomial 共轭"></a>Beta-Binomial 共轭</h3><p>&emsp;&emsp;我们在问题1的基础上增加一些观测数据，变成<strong>问题2</strong>：</p>
<div  align="center"><img src="../images/imgs12/question2.png" width = "500" height = "95" alt="问题2" align="center" /></div><br>

<p>&emsp;&emsp;第2步的条件可以用另外一句话来表述，即“<code>Yi</code>中有<code>m1</code>个比<code>X(k</code>)小，<code>m2</code>个比<code>X(k)</code>大”，所以<code>X(k)</code>是$X_{(1)},X_{(2)},…,X_{(n)};Y_{(1)},Y_{(2)},…,Y_{(m)}$中k+m1大的数。</p>
<p>&emsp;&emsp;根据1.5.1的介绍，我们知道事件p服从<code>beta</code>分布,它的概率密度函数为：</p>
<div  align="center"><img src="../images/imgs12/1.5.7.png" width = "200" height = "20" alt="问题2" align="center" /></div><br>

<p>&emsp;&emsp;按照贝叶斯推理的逻辑，把以上过程整理如下：</p>
<ul>
<li><p>1、p是我们要猜测的参数，我们推导出<code>p</code>的分布为<code>f(p)=Beta(p|k,n-k+1)</code>,称为<code>p</code>的先验分布</p>
</li>
<li><p>2、根据<code>Yi</code>中有<code>m1</code>个比<code>p</code>小，有<code>m2</code>个比<code>p</code>大，<code>Yi</code>相当是做了<code>m</code>次伯努利实验，所以<code>m1</code>服从二项分布<code>B(m,p)</code></p>
</li>
<li><p>3、在给定了来自数据提供<code>(m1,m2)</code>知识后，p的后验分布变为<code>f(p|m1,m2)=Beta(p|k+m1,n-k+1+m2)</code></p>
</li>
</ul>
<p>&emsp;&emsp;贝叶斯估计的基本过程是：</p>
<p>&emsp;&emsp;&emsp;&emsp; <strong>先验分布 + 数据的知识 = 后验分布</strong></p>
<p>&emsp;&emsp;以上贝叶斯分析过程的简单直观的表示就是：</p>
<p>&emsp;&emsp;&emsp;&emsp; <strong>Beta(p|k,n-k+1) + BinomCount(m1,m2) = Beta(p|k+m1,n-k+1+m2)</strong></p>
<p>&emsp;&emsp;更一般的，对于非负实数alpha和beta，我们有如下关系</p>
<p>&emsp;&emsp;&emsp;&emsp; <strong>Beta(p|alpha,beta) + BinomCount(m1,m2) = Beta(p|alpha+m1,beta+m2)</strong></p>
<p>&emsp;&emsp;针对于这种观测到的数据符合二项分布，参数的先验分布和后验分布都是<code>Beta</code>分布的情况，就是<code>Beta-Binomial</code>共轭。换言之，<code>Beta</code>分布是二项式分布的共轭先验概率分布。二项分布和Beta分布是共轭分布意味着，如果我们为二项分布的参数p选取的先验分布是<code>Beta</code>分布，那么以p为参数的二项分布用贝叶斯估计得到的后验分布仍然服从<code>Beta</code>分布。</p>
<h2 id="Dirichlet-分布"><a href="#Dirichlet-分布" class="headerlink" title="Dirichlet 分布"></a>Dirichlet 分布</h2><h3 id="Dirichlet-分布-1"><a href="#Dirichlet-分布-1" class="headerlink" title="Dirichlet 分布"></a>Dirichlet 分布</h3><p>&emsp;&emsp;<code>Dirichlet</code>分布，是<code>beta</code>分布在高维度上的推广。<code>Dirichlet</code>分布的的密度函数形式跟<code>beta</code>分布的密度函数类似：</p>
<div  align="center"><img src="../images/imgs12/1.6.1.png" width = "370" height = "75" alt="Dirichlet" align="center" /></div><br>

<p>&emsp;&emsp;其中</p>
<div  align="center"><img src="../images/imgs12/1.6.2.png" width = "260" height = "50" alt="Dirichlet" align="center" /></div><br>

<p>&emsp;&emsp;至此，我们可以看到二项分布和多项分布很相似，<code>Beta</code>分布和<code>Dirichlet</code>分布很相似。并且<code>Beta</code>分布是二项式分布的共轭先验概率分布。那么<code>Dirichlet</code>分布呢？<code>Dirichlet</code>分布是多项式分布的共轭先验概率分布。下文来论证这点。</p>
<h3 id="Dirichlet-Multinomial-共轭"><a href="#Dirichlet-Multinomial-共轭" class="headerlink" title="Dirichlet-Multinomial 共轭"></a>Dirichlet-Multinomial 共轭</h3><p>&emsp;&emsp;在1.5.3章问题2的基础上，我们更进一步引入<strong>问题3</strong>：</p>
<div  align="center"><img src="../images/imgs12/question3.png" width = "320" height = "75" alt="Dirichlet共轭" align="center" /></div><br>

<p>&emsp;&emsp;类似于问题1的推导，我们可以容易推导联合分布。为了简化计算，我们取<code>x3</code>满足<code>x1+x2+x3=1</code>,<code>x1</code>和<code>x2</code>是变量。如下图所示。</p>
<div  align="center"><img src="../images/imgs12/1.6.3.png" width = "435" height = "75" alt="Dirichlet共轭" align="center" /></div><br>

<p>&emsp;&emsp;概率计算如下：</p>
<div  align="center"><img src="../images/imgs12/1.6.4.png" width = "410" height = "120" alt="Dirichlet共轭" align="center" /></div><br>

<p>&emsp;&emsp;于是我们得到联合分布为：</p>
<div  align="center"><img src="../images/imgs12/1.6.5.png" width = "450" height = "85" alt="Dirichlet共轭" align="center" /></div><br>

<p>&emsp;&emsp;观察上述式子的最终结果，可以看出上面这个分布其实就是3维形式的<code>Dirichlet</code>分布。令<code>alpha1=k1,alpha2=k2,alpha3=n-k1-k2+1</code>，分布密度函数可以写为：</p>
<div  align="center"><img src="../images/imgs12/1.6.6.png" width = "340" height = "40" alt="Dirichlet共轭" align="center" /></div><br>

<p>&emsp;&emsp;为了论证<code>Dirichlet</code>分布是多项式分布的共轭先验概率分布，在上述问题3的基础上再进一步，提出<strong>问题4</strong>。</p>
<div  align="center"><img src="../images/imgs12/question4.png" width = "525" height = "165" alt="问题4" align="center" /></div><br>

<p>&emsp;&emsp;为了方便计算，我们记</p>
<div  align="center"><img src="../images/imgs12/1.6.7.png" width = "330" height = "23" alt="问题4" align="center" /></div><br>

<p>&emsp;&emsp;根据问题中的信息，我们可以推理得到<code>p1,p2</code>在<code>X;Y</code>这<code>m+n</code>个数中分别成为了第<code>k1+m1,k1+k2+m1+m2</code>大的数。后验分布p应该为</p>
<div  align="center"><img src="../images/imgs12/1.6.8.png" width = "640" height = "25" alt="问题4" align="center" /></div><br>

<p>&emsp;&emsp;同样的，按照贝叶斯推理的逻辑，可将上述过程整理如下：</p>
<ul>
<li><p>1 我们要猜测参数<code>P=(p1,p2,p3)</code>，其先验分布为<code>Dir(p|k)</code>;</p>
</li>
<li><p>2 数据<code>Yi</code>落到三个区间<code>[0,p1)</code>,<code>[p1,p2]</code>,<code>(p2,1]</code>的个数分别是<code>m1,m2,m3</code>,所以<code>m=(m1,m2,m3)</code>服从多项分布<code>Mult(m|p)</code>;</p>
</li>
<li><p>3 在给定了来自数据提供的知识<code>m</code>后，<code>p</code>的后验分布变为<code>Dir(P|k+m)</code></p>
</li>
</ul>
<p>&emsp;&emsp;上述贝叶斯分析过程的直观表述为：</p>
<p>&emsp;&emsp;&emsp;&emsp; <strong>Dir(p|k) + Multcount(m) = Dir(p|k+m)</strong></p>
<p>&emsp;&emsp;针对于这种观测到的数据符合多项分布，参数的先验分布和后验分布都是<code>Dirichlet</code>分布的情况，就是<code>Dirichlet-Multinomial</code>共轭。这意味着，如果我们为多项分布的参数p选取的先验分布是<code>Dirichlet</code>分布，那么以p为参数的多项分布用贝叶斯估计得到的后验分布仍然服从<code>Dirichlet</code>分布。</p>
<h2 id="Beta和Dirichlet分布的一个性质"><a href="#Beta和Dirichlet分布的一个性质" class="headerlink" title="Beta和Dirichlet分布的一个性质"></a>Beta和Dirichlet分布的一个性质</h2><p>&emsp;&emsp;如果<code>p=Beta(t|alpha,beta)</code>，那么</p>
<div  align="center"><img src="../images/imgs12/1.7.1.png" width = "285" height = "130" alt="性质" align="center" /></div><br>

<p>&emsp;&emsp;上式右边的积分对应到概率分布<code>Beta(t|alpha+1,beta)</code>，对于这个分布，我们有</p>
<div  align="center"><img src="../images/imgs12/1.7.2.png" width = "245" height = "47" alt="性质" align="center" /></div><br>

<p>&emsp;&emsp;把上式带人<code>E(p)</code>的计算式，可以得到：</p>
<div  align="center"><img src="../images/imgs12/1.7.3.png" width = "233" height = "126" alt="性质" align="center" /></div><br>

<p>&emsp;&emsp;这说明，对于<code>Beta</code>分布的随机变量，其期望可以用上式来估计。<code>Dirichlet</code>分布也有类似的结论。对于<code>p=Dir(t|alpha)</code>，有</p>
<div  align="center"><img src="../images/imgs12/1.7.4.png" width = "290" height = "42" alt="性质" align="center" /></div><br>

<p>&emsp;&emsp;这个结论在后文的推导中会用到。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>&emsp;&emsp;<code>LDA</code>涉及的数学知识较多，需要认真体会，以上大部分的知识来源于文献【1,2,3】,如有不清楚的地方，参见这些文献以了解更多。</p>
<h1 id="主题模型LDA"><a href="#主题模型LDA" class="headerlink" title="主题模型LDA"></a>主题模型LDA</h1><p>&emsp;&emsp;在介绍LDA之前，我们先介绍几个基础模型：<code>Unigram model</code>、<code>mixture of unigrams model</code>、<code>pLSA model</code>。为了方便描述，首先定义一些变量：</p>
<ul>
<li>1 <code>w</code>表示词，<code>V</code>表示所有词的个数</li>
<li>2 <code>z</code>表示主题，<code>k</code>表示主题的个数</li>
<li>3  $D=(W_{1},W_{2},…,W_{M})$表示语料库，<code>M</code>表示语料库中的文档数。</li>
<li>4  $W=(w_{1},w_{2},…,w_{N})$表示文档，<code>N</code>表示文档中词的个数。</li>
</ul>
<h2 id="一元模型-Unigram-model"><a href="#一元模型-Unigram-model" class="headerlink" title="一元模型(Unigram model)"></a>一元模型(Unigram model)</h2><p>&emsp;&emsp;对于文档$W=(w_{1},w_{2},…,w_{N})$ ,用$p(w_{n})$表示$w_{n}$的先验概率，生成文档W的概率为：</p>
<div  align="center"><img src="../images/imgs12/2.1.1.png" width = "165" height = "65" alt="一元模型" align="center" /></div><br>

<p>&emsp;&emsp;其图模型为（图中被涂色的<code>w</code>表示可观测变量，<code>N</code>表示一篇文档中总共<code>N</code>个单词，<code>M</code>表示<code>M</code>篇文档）：</p>
<div  align="center"><img src="../images/imgs12/2.1.2.png" width = "180" height = "110" alt="一元模型" align="center" /></div><br>

<h2 id="混合一元模型-Mixture-of-unigrams-model"><a href="#混合一元模型-Mixture-of-unigrams-model" class="headerlink" title="混合一元模型(Mixture of unigrams model)"></a>混合一元模型(Mixture of unigrams model)</h2><p>&emsp;&emsp;该模型的生成过程是：给某个文档先选择一个主题<code>Z</code>，再根据该主题生成文档，该文档中的所有词都来自一个主题。生成文档的概率为：</p>
<div  align="center"><img src="../images/imgs12/2.2.1.png" width = "200" height = "50" alt="混合一元模型" align="center" /></div><br>

<p>&emsp;&emsp;其图模型为（图中被涂色的<code>w</code>表示可观测变量，未被涂色的<code>z</code>表示未知的隐变量，<code>N</code>表示一篇文档中总共<code>N</code>个单词，<code>M</code>表示<code>M</code>篇文档）：</p>
<div  align="center"><img src="../images/imgs12/2.2.2.png" width = "210" height = "115" alt="混合一元模型" align="center" /></div><br>

<h2 id="pLSA模型"><a href="#pLSA模型" class="headerlink" title="pLSA模型"></a>pLSA模型</h2><p>&emsp;&emsp;在混合一元模型中，假定一篇文档只由一个主题生成，可实际中，一篇文章往往有多个主题，只是这多个主题各自在文档中出现的概率大小不一样。在<code>pLSA</code>中，假设文档由多个主题生成。下面通过一个投色子的游戏（取自文献【2】的例子）说明<code>pLSA</code>生成文档的过程。</p>
<p>&emsp;&emsp;首先，假定你一共有<code>K</code>个可选的主题，有<code>V</code>个可选的词。假设你每写一篇文档会制作一颗<code>K</code>面的“文档-主题”骰子（扔此骰子能得到<code>K</code>个主题中的任意一个），和<code>K</code>个<code>V</code>面的“主题-词项”骰子（每个骰子对应一个主题，<code>K</code>个骰子对应之前的<code>K</code>个主题，且骰子的每一面对应要选择的词项，<code>V</code>个面对应着<code>V</code>个可选的词）。<br>比如可令<code>K=3</code>，即制作1个含有3个主题的“文档-主题”骰子，这3个主题可以是：教育、经济、交通。然后令<code>V = 3</code>，制作3个有着3面的“主题-词项”骰子，其中，教育主题骰子的3个面上的词可以是：<strong>大学、老师、课程</strong>，经济主题骰子的3个面上的词可以是：<strong>市场、企业、金融</strong>，交通主题骰子的3个面上的词可以是：<strong>高铁、汽车、飞机</strong>。</p>
<p>&emsp;&emsp;其次，每写一个词，先扔该“文档-主题”骰子选择主题，得到主题的结果后，使用和主题结果对应的那颗“主题-词项”骰子，扔该骰子选择要写的词。先扔“文档-主题”的骰子，假设以一定的概率得到的主题是：<strong>教育</strong>，所以下一步便是扔<strong>教育</strong>主题筛子，以一定的概率得到<strong>教育</strong>主题筛子对应的某个词<strong>大学</strong>。</p>
<ul>
<li><p>上面这个投骰子产生词的过程简化一下便是：“先以一定的概率选取主题，再以一定的概率选取词”。事实上，一开始可供选择的主题有3个：教育、经济、交通，那为何偏偏选取教育这个主题呢？其实是随机选取的，只是这个随机遵循一定的概率分布。比如可能选取教育主题的概率是0.5，选取经济主题的概率是0.3，选取交通主题的概率是0.2，那么这3个主题的概率分布便是<code>&#123;教育：0.5，经济：0.3，交通：0.2&#125;</code>，我们把各个主题<code>z</code>在文档<code>d</code>中出现的概率分布称之为主题分布，且是一个多项分布。</p>
</li>
<li><p>同样的，从主题分布中随机抽取出教育主题后，依然面对着3个词：大学、老师、课程，这3个词都可能被选中，但它们被选中的概率也是不一样的。比如大学这个词被选中的概率是0.5，老师这个词被选中的概率是0.3，课程被选中的概率是0.2，那么这3个词的概率分布便是<code>&#123;大学：0.5，老师：0.3，课程：0.2&#125;</code>，我们把各个词语w在主题z下出现的概率分布称之为词分布，这个词分布也是一个多项分布。</p>
</li>
<li><p>所以，选主题和选词都是两个随机的过程，先从主题分布<code>&#123;教育：0.5，经济：0.3，交通：0.2&#125;</code>中抽取出主题：<strong>教育</strong>，然后从该主题对应的词分布<code>&#123;大学：0.5，老师：0.3，课程：0.2&#125;</code>中抽取出词：<strong>大学</strong>。</p>
</li>
</ul>
<p>&emsp;&emsp;最后，你不停的重复扔“文档-主题”骰子和”主题-词项“骰子，重复<code>N</code>次（产生<code>N</code>个词），完成一篇文档，重复这产生一篇文档的方法<code>M</code>次，则完成<code>M</code>篇文档。</p>
<p>&emsp;&emsp;上述过程抽象出来即是<code>pLSA</code>的文档生成模型。在这个过程中，我们并未关注词和词之间的出现顺序，所以<code>pLSA</code>是一种词袋模型。定义如下变量:</p>
<ul>
<li><p>$(z_{1},z_{2},…,z_{k})$表示隐藏的主题；</p>
</li>
<li><p>$P(d_{i})$表示海量文档中某篇文档被选中的概率；</p>
</li>
<li><p>$P(w_{j}|d_{i})$表示词$w_{j}$在文档$d_{i}$中出现的概率；针对海量文档，对所有文档进行分词后，得到一个词汇列表，这样每篇文档就是一个词语的集合。对于每个词语，用它在文档中出现的次数除以文档中词语总的数目便是它在文档中出现的概率；</p>
</li>
<li><p>$P(z_{k}|d_{i})$表示主题$z_{k}$在文档$d_{i}$中出现的概率；</p>
</li>
<li><p>$P(w_{j}|z_{k})$表示词$w_{j}$在主题$z_{k}$中出现的概率。与主题关系越密切的词其条件概率越大。</p>
</li>
</ul>
<p>&emsp;&emsp;我们可以按照如下的步骤得到“文档-词项”的生成模型：</p>
<ul>
<li><p>1 按照$P(d_{i})$选择一篇文档$d_{i}$ ；</p>
</li>
<li><p>2 选定文档$d_{i}$之后，从主题分布中按照概率$P(z_{k}|d_{i})$选择主题；</p>
</li>
<li><p>3 选定主题后，从词分布中按照概率$P(w_{j}|z_{k})$选择一个词。</p>
</li>
</ul>
<p>&emsp;&emsp;利用看到的文档推断其隐藏的主题（分布）的过程，就是主题建模的目的：自动地发现文档集中的主题（分布）。文档<code>d</code>和单词<code>w</code>是可被观察到的，但主题<code>z</code>却是隐藏的。如下图所示（图中被涂色的<code>d、w</code>表示可观测变量，未被涂色的<code>z</code>表示未知的隐变量，<code>N</code>表示一篇文档中总共<code>N</code>个单词，<code>M</code>表示<code>M</code>篇文档）。</p>
<div  align="center"><img src="../images/imgs12/2.3.1.png" width = "300" height = "110" alt="pLSA模型" align="center" /></div><br>

<p>&emsp;&emsp;上图中，文档<code>d</code>和词<code>w</code>是我们得到的样本，可观测得到，所以对于任意一篇文档，其$P(w_{j}|d_{i})$是已知的。根据这个概率可以训练得到<code>文档-主题</code>概率以及<code>主题-词项</code>概率。即：</p>
<div  align="center"><img src="../images/imgs12/2.3.2.png" width = "275" height = "65" alt="pLSA模型" align="center" /></div><br>

<p>&emsp;&emsp;故得到文档中每个词的生成概率为：</p>
<div  align="center"><img src="../images/imgs12/2.3.3.png" width = "450" height = "70" alt="pLSA模型" align="center" /></div><br>

<p>&emsp;&emsp;$P(d_{i})$可以直接得出，而$P(z_{k}|d_{i})$和$P(w_{j}|z_{k})$未知，所以<br>$\theta=(P(z_{k}|d_{i}),P(w_{j}|z_{k}))$就是我们要估计的参数,我们要最大化这个参数。因为该待估计的参数中含有隐变量<code>z</code>，所以我们可以用<code>EM</code>算法来估计这个参数。</p>
<p>&emsp;&emsp;</p>
<h2 id="LDA模型"><a href="#LDA模型" class="headerlink" title="LDA模型"></a>LDA模型</h2><p>&emsp;&emsp;<code>LDA</code>的不同之处在于，<code>pLSA</code>的主题的概率分布<code>P(z|d)</code>是一个确定的概率分布，也就是虽然主题<code>z</code>不确定，但是<code>z</code>符合的概率分布是确定的，比如符合某个多项分布，这个多项分布的各参数是确定的。<br>但是在<code>LDA</code>中，这个多项分布都是不确定的，这个多项式分布又服从一个狄利克雷先验分布<code>(Dirichlet prior)</code>。即<code>LDA</code>就是<code>pLSA</code>的贝叶斯版本,正因为<code>LDA</code>被贝叶斯化了，所以才会加两个先验参数。</p>
<p>&emsp;&emsp;<code>LDA</code>模型中一篇文档生成的方式如下所示:</p>
<ul>
<li><p>1 按照$P(d_{i})$选择一篇文档$d_{i}$ ；</p>
</li>
<li><p>2 从狄利克雷分布$\alpha$中生成文档$d_{i}$的主题分布$\theta_{i}$ ；</p>
</li>
<li><p>3 从主题的多项式分布$\theta_{i}$中取样生成文档$d_{i}$第<code>j</code>个词的主题$Z_{i,j}$ ；</p>
</li>
<li><p>4 从狄利克雷分布$\eta$中取样生成主题$Z_{i,j}$对应的词语分布$\beta_{i,j}$ ；</p>
</li>
<li><p>5 从词语的多项式分布$\beta_{i,j}$中采样最终生成词语$W_{i,j}$</p>
</li>
</ul>
<p>&emsp;&emsp;从上面的过程可以看出，<code>LDA</code>在<code>pLSA</code>的基础上，为主题分布和词分布分别加了两个<code>Dirichlet</code>先验。</p>
<p>&emsp;&emsp;拿之前讲解<code>pLSA</code>的例子进行具体说明。如前所述，在<code>pLSA</code>中，选主题和选词都是两个随机的过程，先从主题分布<code>&#123;教育：0.5，经济：0.3，交通：0.2&#125;</code>中抽取出主题：教育，然后从该主题对应的词分布<code>&#123;大学：0.5，老师：0.3，课程：0.2&#125;</code>中抽取出词：大学。<br>在<code>LDA</code>中，选主题和选词依然都是两个随机的过程。但在<code>LDA</code>中，主题分布和词分布不再唯一确定不变，即无法确切给出。例如主题分布可能是<code>&#123;教育：0.5，经济：0.3，交通：0.2&#125;</code>，也可能是<code>&#123;教育：0.6，经济：0.2，交通：0.2&#125;</code>，到底是哪个我们不能确定，因为它是随机的可变化的。<br>但再怎么变化，也依然服从一定的分布，主题分布和词分布由<code>Dirichlet</code>先验确定。</p>
<p>&emsp;&emsp;举个文档<code>d</code>产生主题<code>z</code>的例子。</p>
<p>&emsp;&emsp;在<code>pLSA</code>中，给定一篇文档<code>d</code>，主题分布是一定的，比如<code>&#123; P(zi|d), i = 1,2,3 &#125;</code>可能就是<code>&#123;0.4,0.5,0.1&#125;</code>，表示<code>z1、z2、z3</code>这3个主题被文档<code>d</code>选中的概率都是个固定的值：<code>P(z1|d) = 0.4、P(z2|d) = 0.5、P(z3|d) = 0.1</code>，如下图所示:</p>
<div  align="center"><img src="../images/imgs12/2.3.4.png" width = "490" height = "350" alt="LDA模型" align="center" /></div><br>

<p>&emsp;&emsp;在<code>pLSA</code>中，主题分布（各个主题在文档中出现的概率分布）和词分布（各个词语在某个主题下出现的概率分布）是唯一确定的，而<code>LDA</code>的主题分布和词分布是不固定的。<code>LDA</code>为提供了两个<code>Dirichlet</code>先验参数，<code>Dirichlet</code>先验为某篇文档随机抽取出主题分布和词分布。</p>
<p>&emsp;&emsp;给定一篇文档<code>d</code>，现在有多个主题<code>z1、z2、z3</code>，它们的主题分布<code>&#123; P(zi|d), i = 1,2,3 &#125;</code>可能是<code>&#123;0.4,0.5,0.1&#125;</code>，也可能是<code>&#123;0.2,0.2,0.6&#125;</code>，即这些主题被<code>d</code>选中的概率都不再是确定的值，可能是<code>P(z1|d) = 0.4、P(z2|d) = 0.5、P(z3|d) = 0.1</code>，也有可能是<code>P(z1|d) = 0.2、P(z2|d) = 0.2、P(z3|d) = 0.6</code>，而主题分布到底是哪个取值集合我们不确定，但其先验分布是<code>dirichlet</code>分布，所以可以从无穷多个主题分布中按照<code>dirichlet</code>先验随机抽取出某个主题分布出来。如下图所示</p>
<div  align="center"><img src="../images/imgs12/2.3.5.png" width = "490" height = "350" alt="LDA模型" align="center" /></div><br>

<p>&emsp;&emsp;<code>LDA</code>在<code>pLSA</code>的基础上给两参数$P(z_{k}|d_{i})$和$P(w_{j}|z_{k})$加了两个先验分布的参数。这两个分布都是<code>Dirichlet</code>分布。<br>下面是<code>LDA</code>的图模型结构：</p>
<div  align="center"><img src="../images/imgs12/LDA.png" width = "415" height = "195" alt="topic_words" align="center" /></div><br>


<h1 id="LDA-参数估计"><a href="#LDA-参数估计" class="headerlink" title="LDA 参数估计"></a>LDA 参数估计</h1><p>&emsp;&emsp; <code>spark</code>实现了2个版本的<code>LDA</code>，这里分别叫做<code>Spark EM LDA</code>（见文献【3】【4】）和<code>Spark Online LDA</code>（见文献【5】）。它们使用同样的数据输入，但是内部的实现和依据的原理完全不同。<code>Spark EM LDA</code>使用<code>GraphX</code>实现的，通过对图的边和顶点数据的操作来训练模型。而<code>Spark Online LDA</code>采用抽样的方式，每次抽取一些文档训练模型，通过多次训练，得到最终模型。在参数估计上，<code>Spark EM LDA</code>使用<code>gibbs</code>采样原理估计模型参数，<code>Spark Online LDA</code>使用贝叶斯变分推断原理估计参数。在模型存储上，<code>Spark EM LDA</code>将训练的主题-词模型存储在<code>GraphX</code>图顶点上，属于分布式存储方式。<code>Spark Online</code>使用矩阵来存储主题-词模型，属于本地模型。通过这些差异，可以看出<code>Spark EM LDA</code>和<code>Spark Online LDA</code>的不同之处，同时他们各自也有各自的瓶颈。<code>Spark EM LDA</code>在训练时<code>shuffle</code>量相当大，严重拖慢速度。而<code>Spark Online LDA</code>使用矩阵存储模型，矩阵规模直接限制训练文档集的主题数和词的数目。另外，<code>Spark EM LDA</code>每轮迭代完毕后更新模型，<code>Spark Online LDA</code>每训练完抽样的文本更新模型，因而<code>Spark Online LDA</code>模型更新更及时，收敛速度更快。</p>
<h2 id="变分EM算法"><a href="#变分EM算法" class="headerlink" title="变分EM算法"></a>变分EM算法</h2><p>&emsp;&emsp;变分贝叶斯算法的详细信息可以参考文献【9】。</p>
<p>&emsp;&emsp;在上文中，我们知道<code>LDA</code>将变量<code>theta</code>和<code>phi</code>（为了方便起见，我们将上文<code>LDA</code>图模型中的<code>beta</code>改为了<code>phi</code>）看做随机变量，并且为<code>theta</code>添加一个超参数为<code>alpha</code>的<code>Dirichlet</code>先验，为<code>phi</code>添加一个超参数为<code>eta</code>的<code>Dirichlet</code>先验来估计<code>theta</code>和<code>phi</code>的最大后验（<code>MAP</code>）。<br>可以通过最优化最大后验估计来估计参数。我们首先来定义几个变量：</p>
<ul>
<li>下式的<code>gamma</code>表示词为<code>w</code>，文档为<code>j</code>时，主题为<code>k</code>的概率，如公式 <strong>（3.1.1）</strong></li>
</ul>
<div  align="center"><img src="../images/imgs12/3.1.1.png" width = "255" height = "30" alt="topic_words" align="center" /></div><br>

<ul>
<li><p>$N_{wj}$表示词<code>w</code>在文档<code>j</code>中出现的次数；</p>
</li>
<li><p>$N_{wk}$表示词<code>w</code>在主题<code>k</code>中出现的次数，如公式 <strong>（3.1.2）</strong></p>
</li>
</ul>
<div  align="center"><img src="../images/imgs12/3.1.2.png" width = "170" height = "60" alt="topic_words" align="center" /></div><br>

<ul>
<li>$N_{kj}$表示主题<code>k</code>在文档<code>j</code>中出现的次数，如公式 <strong>（3.1.3）</strong></li>
</ul>
<div  align="center"><img src="../images/imgs12/3.1.3.png" width = "160" height = "55" alt="topic_words" align="center" /></div><br>

<ul>
<li>$N_{k}$表示主题<code>k</code>中包含的词出现的总次数，如公式 <strong>（3.1.4）</strong></li>
</ul>
<div  align="center"><img src="../images/imgs12/3.1.4.png" width = "120" height = "50" alt="topic_words" align="center" /></div><br>

<ul>
<li>$N_{j}$表示文档<code>j</code>中包含的主题出现的总次数,如公式 <strong>（3.1.5）</strong></li>
</ul>
<div  align="center"><img src="../images/imgs12/3.1.5.png" width = "120" height = "50" alt="topic_words" align="center" /></div><br>

<p>&emsp;&emsp;根据文献【4】中<code>2.2</code>章节的介绍，我们可以推导出如下更新公式 <strong>(3.1.6)</strong> ，其中<code>alpha</code>和<code>eta</code>均大于1：</p>
<div  align="center"><img src="../images/imgs12/3.1.6.png" width = "360" height = "60" alt="topic_words" align="center" /></div><br>

<p>&emsp;&emsp;收敛之后，最大后验估计可以得到公式 **(3.1.7)**：</p>
<div  align="center"><img src="../images/imgs12/3.1.7.png" width = "450" height = "60" alt="topic_words" align="center" /></div><br>

<p>&emsp;&emsp;变分<code>EM</code>算法的流程如下：</p>
<ul>
<li><p><strong>1 初始化状态</strong>，即随机初始化$N_{wk}$和$N_{kj}$</p>
</li>
<li><p><strong>2 E-步</strong>，对每一个<code>（文档，词汇）</code>对<code>i</code>，计算$P(z_{i}|w_{i},d_{i})$，更新<code>gamma</code>值</p>
</li>
<li><p><strong>3 M-步</strong>，计算隐藏变量<code>phi</code>和<code>theta</code>。即计算$N_{wk}$和$N_{kj}$</p>
</li>
<li><p><strong>4 重复以上2、3两步</strong>，直到满足最大迭代数</p>
</li>
</ul>
<p>&emsp;&emsp;第<code>4.2</code>章会从代码层面说明该算法的实现流程。</p>
<h2 id="在线学习算法"><a href="#在线学习算法" class="headerlink" title="在线学习算法"></a>在线学习算法</h2><h3 id="批量变分贝叶斯"><a href="#批量变分贝叶斯" class="headerlink" title="批量变分贝叶斯"></a>批量变分贝叶斯</h3><p>&emsp;&emsp;在变分贝叶斯推导(<code>VB</code>)中，根据文献【3】，使用一种更简单的分布<code>q(z,theta,beta)</code>来估计真正的后验分布，这个简单的分布使用一组自由变量(<code>free parameters</code>)来定义。<br>通过最大化对数似然的一个下界（<code>Evidence Lower Bound (ELBO)</code>）来最优化这些参数，如下公式 <strong>(3.2.1)</strong></p>
<div  align="center"><img src="../images/imgs12/3.2.1.png" width = "500" height = "25" alt="topic_words" align="center" /></div><br>

<p>&emsp;&emsp;最大化<code>ELBO</code>就是最小化<code>q(z,theta,beta)</code>和<code>p(z,theta,beta|w,alpha,eta)</code>的<code>KL</code>距离。根据文献【3】，我们将<code>q</code>因式分解为如下 <strong>（3.2.2）</strong> 的形式：</p>
<div  align="center"><img src="../images/imgs12/3.2.2.png" width = "500" height = "20" alt="topic_words" align="center" /></div><br>

<p>&emsp;&emsp;后验<code>z</code>通过<code>phi</code>来参数化，后验<code>theta</code>通过<code>gamma</code>来参数化，后验<code>beta</code>通过<code>lambda</code>来参数化。为了简单描述，我们把<code>lambda</code>当作“主题”来看待。公式 <strong>(3.2.2)</strong> 分解为如下 <strong>(3.2.3)</strong> 形式：</p>
<div  align="center"><img src="../images/imgs12/3.2.3.png" width = "550" height = "50" alt="topic_words" align="center" /></div><br>

<p>&emsp;&emsp;我们现在将上面的期望扩展为变分参数的函数形式。这反映了变分目标只依赖于$n_{dw}$ ，即词<code>w</code>出现在文档<code>d</code>中的次数。当使用<code>VB</code>算法时，文档可以通过它们的词频来汇总（<code>summarized</code>），如公式 <strong>(3.2.4)</strong></p>
<div  align="center"><img src="../images/imgs12/3.2.4.png" width = "500" height = "110" alt="topic_words" align="center" /></div><br>

<p>&emsp;&emsp;上面的公式中，<code>W</code>表示词的数量，<code>D</code>表示文档的数量。<code>l</code>表示文档<code>d</code>对<code>ELBO</code>所做的贡献。<code>L</code>可以通过坐标上升法来最优化，它的更新公式如 <strong>(3.2.5)</strong>:</p>
<div  align="center"><img src="../images/imgs12/3.2.5.png" width = "600" height = "25" alt="topic_words" align="center" /></div><br>

<p>&emsp;&emsp;<code>log(theta)</code>和<code>log(beta)</code>的期望通过下面的公式 <strong>(3.2.6)</strong> 计算：</p>
<div  align="center"><img src="../images/imgs12/3.2.6.png" width = "500" height = "25" alt="topic_words" align="center" /></div><br>

<p>&emsp;&emsp;通过<code>EM</code>算法，我们可以将这些更新分解成<code>E-步</code>和<code>M-步</code>。<code>E-步</code>固定<code>lambda</code>来更新<code>gamma</code>和<code>phi</code>；<code>M-步</code>通过给定<code>phi</code>来更新<code>lambda</code>。批<code>VB</code>算法的过程如下 <strong>(算法1)</strong> 所示：</p>
<div  align="center"><img src="../images/imgs12/alg1.png" width = "400" height = "250" alt="topic_words" align="center" /></div><br>

<h3 id="在线变分贝叶斯"><a href="#在线变分贝叶斯" class="headerlink" title="在线变分贝叶斯"></a>在线变分贝叶斯</h3><p>&emsp;&emsp;批量变分贝叶斯算法需要固定的内存，并且比吉布斯采样更快。但是它仍然需要在每次迭代时处理所有的文档，这在处理大规模文档时，速度会很慢，并且也不适合流式数据的处理。<br>文献【5】提出了一种在线变分推导算法。设定<code>gamma(n_d,lambda)</code>和<code>phi(n_d,lambda)</code>分别表示<code>gamma_d</code>和<code>phi_d</code>的值，我们的目的就是设定<code>phi</code>来最大化下面的公式 <strong>(3.2.7)</strong></p>
<div  align="center"><img src="../images/imgs12/3.2.7.png" width = "260" height = "25" alt="topic_words" align="center" /></div><br>

<p>&emsp;&emsp;我们在 <strong>算法2</strong> 中介绍了在线<code>VB</code>算法。因为词频的第t个向量$n_{t}$是可观察的，我们在<code>E-步</code>通过固定<code>lambda</code>来找到<code>gamma_t</code>和<code>phi_t</code>的局部最优解。<br>然后，我们计算<code>lambda_cap</code>。如果整个语料库由单个文档重复<code>D</code>次组成，那么这样的<code>lambda_cap</code>设置是最优的。之后，我们通过<code>lambda</code>之前的值以及<code>lambda_cap</code>来更新<code>lambda</code>。我们给<code>lambda_cap</code>设置的权重如公式 <strong>(3.2.8)</strong> 所示：</p>
<div  align="center"><img src="../images/imgs12/3.2.8.png" width = "220" height = "30" alt="topic_words" align="center" /></div><br>

<p>&emsp;&emsp;在线<code>VB</code>算法的实现流程如下 <strong>算法2</strong> 所示</p>
<div  align="center"><img src="../images/imgs12/alg2.png" width = "400" height = "250" alt="topic_words" align="center" /></div><br>

<p>&emsp;&emsp;那么在在线<code>VB</code>算法中，<code>alpha</code>和<code>eta</code>是如何更新的呢？参考文献【8】提供了计算方法。给定数据集，<code>dirichlet</code>参数的可以通过最大化下面的对数似然来估计</p>
<div  align="center"><img src="../images/imgs12/3.3.1.png" width = "500" height = "150" alt="3.3.1" align="center" /></div><br>

<p>&emsp;&emsp;其中，</p>
<div  align="center"><img src="../images/imgs12/3.3.2.png" width = "200" height = "40" alt="3.3.2" align="center" /></div><br>

<p>&emsp;&emsp;有多种方法可以最大化这个目标函数，如梯度上升，<code>Newton-Raphson</code>等。<code>Spark</code>使用<code>Newton-Raphson</code>方法估计参数，更新<code>alpha</code>。<code>Newton-Raphson</code>提供了一种参数二次收敛的方法，<br>它一般的更新规则如下公式 <strong>(3.3.3)</strong> :</p>
<div  align="center"><img src="../images/imgs12/3.3.3.png" width = "300" height = "30" alt="3.3.2" align="center" /></div><br>

<p>&emsp;&emsp;其中，<code>H</code>表示海森矩阵。对于这个特别的对数似然函数，可以应用<code>Newton-Raphson</code>去解决高维数据，因为它可以在线性时间求出海森矩阵的逆矩阵。一般情况下，海森矩阵可以用一个对角矩阵和一个元素都一样的矩阵的和来表示。<br>如下公式 <strong>(3.3.4)</strong> ，<code>Q</code>是对角矩阵，<code>C11</code>是元素相同的一个矩阵。</p>
<div  align="center"><img src="../images/imgs12/3.3.4.png" width = "300" height = "150" alt="3.3.2" align="center" /></div><br>

<p>&emsp;&emsp;为了计算海森矩阵的逆矩阵，我们观察到，对任意的可逆矩阵<code>Q</code>和非负标量<code>c</code>，有下列式子 <strong>(3.3.5)</strong>:</p>
<div  align="center"><img src="../images/imgs12/3.3.5.png" width = "550" height = "170" alt="3.3.5" align="center" /></div><br>

<p>&emsp;&emsp;因为<code>Q</code>是对角矩阵，所以<code>Q</code>的逆矩阵可以很容易的计算出来。所以<code>Newton-Raphson</code>的更新规则可以重写为如下 <strong>(3.3.6)</strong> 的形式</p>
<div  align="center"><img src="../images/imgs12/3.3.6.png" width = "250" height = "50" alt="3.3.6" align="center" /></div><br>

<p>&emsp;&emsp;其中<code>b</code>如下公式 **(3.3.7)**，</p>
<div  align="center"><img src="../images/imgs12/3.3.7.png" width = "300" height = "40" alt="3.3.7" align="center" /></div><br>

<h1 id="LDA代码实现"><a href="#LDA代码实现" class="headerlink" title="LDA代码实现"></a>LDA代码实现</h1><h2 id="LDA使用实例"><a href="#LDA使用实例" class="headerlink" title="LDA使用实例"></a>LDA使用实例</h2><p>&emsp;&emsp;我们从官方文档【6】给出的使用代码为起始点来详细分析<code>LDA</code>的实现。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.mllib.clustering.&#123;<span class="type">LDA</span>, <span class="type">DistributedLDAModel</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.mllib.linalg.<span class="type">Vectors</span></span><br><span class="line"><span class="comment">// 加载和处理数据</span></span><br><span class="line"><span class="keyword">val</span> data = sc.textFile(<span class="string">&quot;data/mllib/sample_lda_data.txt&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> parsedData = data.map(s =&gt; <span class="type">Vectors</span>.dense(s.trim.split(&#x27; &#x27;).map(_.toDouble)))</span><br><span class="line"><span class="comment">// 为文档编号，编号唯一。List（（id，vector）....）</span></span><br><span class="line"><span class="keyword">val</span> corpus = parsedData.zipWithIndex.map(_.swap).cache()</span><br><span class="line"><span class="comment">// 主题个数为3</span></span><br><span class="line"><span class="keyword">val</span> ldaModel = <span class="keyword">new</span> <span class="type">LDA</span>().setK(<span class="number">3</span>).run(corpus)</span><br><span class="line"><span class="keyword">val</span> topics = ldaModel.topicsMatrix</span><br><span class="line"><span class="keyword">for</span> (topic &lt;- <span class="type">Range</span>(<span class="number">0</span>, <span class="number">3</span>)) &#123;</span><br><span class="line">  print(<span class="string">&quot;Topic &quot;</span> + topic + <span class="string">&quot;:&quot;</span>)</span><br><span class="line">  <span class="keyword">for</span> (word &lt;- <span class="type">Range</span>(<span class="number">0</span>, ldaModel.vocabSize)) &#123; print(<span class="string">&quot; &quot;</span> + topics(word, topic)); &#125;</span><br><span class="line">  println()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;以上代码主要做了两件事：加载和切分数据、训练模型。在样本数据中，每一行代表一篇文档，经过处理后，<code>corpus</code>的类型为<code>List((id,vector)*)</code>，一个<code>(id,vector)</code>代表一篇文档。将处理后的数据传给<code>org.apache.spark.mllib.clustering.LDA</code>类的<code>run</code>方法，<br>就可以开始训练模型。<code>run</code>方法的代码如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(documents: <span class="type">RDD</span>[(<span class="type">Long</span>, <span class="type">Vector</span>)]): <span class="type">LDAModel</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> state = ldaOptimizer.initialize(documents, <span class="keyword">this</span>)</span><br><span class="line">    <span class="keyword">var</span> iter = <span class="number">0</span></span><br><span class="line">    <span class="keyword">val</span> iterationTimes = <span class="type">Array</span>.fill[<span class="type">Double</span>](maxIterations)(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">while</span> (iter &lt; maxIterations) &#123;</span><br><span class="line">      <span class="keyword">val</span> start = <span class="type">System</span>.nanoTime()</span><br><span class="line">      state.next()</span><br><span class="line">      <span class="keyword">val</span> elapsedSeconds = (<span class="type">System</span>.nanoTime() - start) / <span class="number">1e9</span></span><br><span class="line">      iterationTimes(iter) = elapsedSeconds</span><br><span class="line">      iter += <span class="number">1</span></span><br><span class="line">    &#125;</span><br><span class="line">    state.getLDAModel(iterationTimes)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;这段代码首先调用<code>initialize</code>方法初始化状态信息，然后循环迭代调用<code>next</code>方法直到满足最大的迭代次数。在我们没有指定的情况下，迭代次数默认为20。需要注意的是，<br><code>ldaOptimizer</code>有两个具体的实现类<code>EMLDAOptimizer</code>和<code>OnlineLDAOptimizer</code>，它们分别表示使用<code>EM</code>算法和在线学习算法实现参数估计。在未指定的情况下，默认使用<code>EMLDAOptimizer</code>。</p>
<h2 id="变分EM算法的实现"><a href="#变分EM算法的实现" class="headerlink" title="变分EM算法的实现"></a>变分EM算法的实现</h2><p>&emsp;&emsp;在<code>spark</code>中，使用<code>GraphX</code>来实现<code>EMLDAOptimizer</code>，这个图是有两种类型的顶点的二分图。这两类顶点分别是文档顶点（<code>Document vertices</code>）和词顶点（<code>Term vertices</code>）。</p>
<ul>
<li><p>文档顶点文档顶点<code>ID</code>编号从0递增，保存长度为<code>k</code>（主题个数）的向量</p>
</li>
<li><p>词顶点使用<code>&#123;-1, -2, ..., -vocabSize&#125;</code>来索引，词顶点编号从-1递减，保存长度为<code>k</code>（主题个数）的向量</p>
</li>
<li><p>边（<code>edges</code>）对应词出现在文档中的词频。边的方向是<code>document -&gt; term</code>，并且根据<code>document</code>进行分区</p>
</li>
</ul>
<p>&emsp;&emsp;我们可以根据3.1节中介绍的算法流程来解析源代码。</p>
<h3 id="初始化状态"><a href="#初始化状态" class="headerlink" title="初始化状态"></a>初始化状态</h3><p>&emsp;&emsp;<code>spark</code>在<code>EMLDAOptimizer</code>的<code>initialize</code>方法中实现初始化功能。包括初始化<code>Dirichlet</code>参数<code>alpha</code>和<code>eta</code>、初始化边、初始化顶点以及初始化图。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//对应超参数alpha</span></span><br><span class="line"><span class="keyword">val</span> docConcentration = lda.getDocConcentration</span><br><span class="line"><span class="comment">//对应超参数eta</span></span><br><span class="line"><span class="keyword">val</span> topicConcentration = lda.getTopicConcentration</span><br><span class="line"><span class="keyword">this</span>.docConcentration = <span class="keyword">if</span> (docConcentration == <span class="number">-1</span>) (<span class="number">50.0</span> / k) + <span class="number">1.0</span> <span class="keyword">else</span> docConcentration</span><br><span class="line"><span class="keyword">this</span>.topicConcentration = <span class="keyword">if</span> (topicConcentration == <span class="number">-1</span>) <span class="number">1.1</span> <span class="keyword">else</span> topicConcentration</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;上面的代码初始化了超参数<code>alpha</code>和<code>eta</code>，根据文献【4】，当<code>alpha</code>未指定时，初始化其为<code>(50.0 / k) + 1.0</code>，其中<code>k</code>表示主题个数。当<code>eta</code>未指定时，初始化其为1.1。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//对于每个文档，为每一个唯一的Term创建一个(Document-&gt;Term)的边</span></span><br><span class="line"><span class="keyword">val</span> edges: <span class="type">RDD</span>[<span class="type">Edge</span>[<span class="type">TokenCount</span>]] = docs.flatMap &#123; <span class="keyword">case</span> (docID: <span class="type">Long</span>, termCounts: <span class="type">Vector</span>) =&gt;</span><br><span class="line">      <span class="comment">// Add edges for terms with non-zero counts.</span></span><br><span class="line">      termCounts.toBreeze.activeIterator.filter(_._2 != <span class="number">0.0</span>).map &#123; <span class="keyword">case</span> (term, cnt) =&gt;</span><br><span class="line">        <span class="comment">//文档id，termindex，词频</span></span><br><span class="line">        <span class="type">Edge</span>(docID, term2index(term), cnt)</span><br><span class="line">      &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//term2index将term转为&#123;-1, -2, ..., -vocabSize&#125;索引</span></span><br><span class="line"> <span class="keyword">private</span>[clustering] <span class="function"><span class="keyword">def</span> <span class="title">term2index</span></span>(term: <span class="type">Int</span>): <span class="type">Long</span> = -(<span class="number">1</span> + term.toLong)</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;上面的这段代码处理每个文档，对文档中每个唯一的<code>Term</code>（词）创建一个边，边的格式为<code>(文档id，词索引，词频)</code>。词索引为<code>&#123;-1, -2, ..., -vocabSize&#125;</code>。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//创建顶点</span></span><br><span class="line"> <span class="keyword">val</span> docTermVertices: <span class="type">RDD</span>[(<span class="type">VertexId</span>, <span class="type">TopicCounts</span>)] = &#123;</span><br><span class="line">      <span class="keyword">val</span> verticesTMP: <span class="type">RDD</span>[(<span class="type">VertexId</span>, <span class="type">TopicCounts</span>)] =</span><br><span class="line">        edges.mapPartitionsWithIndex &#123; <span class="keyword">case</span> (partIndex, partEdges) =&gt;</span><br><span class="line">          <span class="keyword">val</span> random = <span class="keyword">new</span> <span class="type">Random</span>(partIndex + randomSeed)</span><br><span class="line">          partEdges.flatMap &#123; edge =&gt;</span><br><span class="line">            <span class="keyword">val</span> gamma = normalize(<span class="type">BDV</span>.fill[<span class="type">Double</span>](k)(random.nextDouble()), <span class="number">1.0</span>)</span><br><span class="line">            <span class="comment">//此处的sum是DenseVector，gamma*N_wj</span></span><br><span class="line">            <span class="keyword">val</span> sum = gamma * edge.attr</span><br><span class="line">            <span class="comment">//srcId表示文献id，dstId表是词索引</span></span><br><span class="line">            <span class="type">Seq</span>((edge.srcId, sum), (edge.dstId, sum))</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      verticesTMP.reduceByKey(_ + _)</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;上面的代码创建顶点。我们为每个主题随机初始化一个值，即<code>gamma</code>是随机的。<code>sum</code>为<code>gamma * edge.attr</code>，这里的<code>edge.attr</code>即<code>N_wj</code>,所以<code>sum</code>用<code>gamma * N_wj</code>作为顶点的初始值。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">this</span>.graph = <span class="type">Graph</span>(docTermVertices, edges).partitionBy(<span class="type">PartitionStrategy</span>.<span class="type">EdgePartition1D</span>)</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;上面的代码初始化<code>Graph</code>并通过文档分区。</p>
<h3 id="E-步：更新gamma"><a href="#E-步：更新gamma" class="headerlink" title="E-步：更新gamma"></a>E-步：更新gamma</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> eta = topicConcentration</span><br><span class="line"><span class="keyword">val</span> <span class="type">W</span> = vocabSize</span><br><span class="line"><span class="keyword">val</span> alpha = docConcentration</span><br><span class="line"><span class="keyword">val</span> <span class="type">N_k</span> = globalTopicTotals</span><br><span class="line"><span class="keyword">val</span> sendMsg: <span class="type">EdgeContext</span>[<span class="type">TopicCounts</span>, <span class="type">TokenCount</span>, (<span class="type">Boolean</span>, <span class="type">TopicCounts</span>)] =&gt; <span class="type">Unit</span> =</span><br><span class="line">  (edgeContext) =&gt; &#123;</span><br><span class="line">    <span class="comment">// 计算 N_&#123;wj&#125; gamma_&#123;wjk&#125;</span></span><br><span class="line">    <span class="keyword">val</span> <span class="type">N_wj</span> = edgeContext.attr</span><br><span class="line">    <span class="comment">// E-STEP: 计算 gamma_&#123;wjk&#125; 通过N_&#123;wj&#125;来计算</span></span><br><span class="line">    <span class="comment">//此处的edgeContext.srcAttr为当前迭代的N_kj , edgeContext.dstAttr为当前迭代的N_wk,</span></span><br><span class="line">    <span class="comment">//后面通过M-步，会更新这两个值,作为下一次迭代的当前值</span></span><br><span class="line">    <span class="keyword">val</span> scaledTopicDistribution: <span class="type">TopicCounts</span> = </span><br><span class="line">                computePTopic(edgeContext.srcAttr, edgeContext.dstAttr, <span class="type">N_k</span>, <span class="type">W</span>, eta, alpha) *= <span class="type">N_wj</span></span><br><span class="line">    edgeContext.sendToDst((<span class="literal">false</span>, scaledTopicDistribution))</span><br><span class="line">    edgeContext.sendToSrc((<span class="literal">false</span>, scaledTopicDistribution))</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;上述代码中，<code>W</code>表示词数，<code>N_k</code>表示所有文档中，出现在主题<code>k</code>中的词的词频总数，后续的实现会使用方法<code>computeGlobalTopicTotals</code>来更新这个值。<code>N_wj</code>表示词<code>w</code>出现在文档<code>j</code>中的词频数，为已知数。<code>E-步</code>就是利用公式 <strong>(3.1.6)</strong> 去更新<code>gamma</code>。<br>代码中使用<code>computePTopic</code>方法来实现这个更新。<code>edgeContext</code>通过方法<code>sendToDst</code>将<code>scaledTopicDistribution</code>发送到目标顶点，<br>通过方法<code>sendToSrc</code>发送到源顶点以便于后续的<code>M-步</code>更新的<code>N_kj</code>和<code>N_wk</code>。下面我们看看<code>computePTopic</code>方法。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[clustering] <span class="function"><span class="keyword">def</span> <span class="title">computePTopic</span></span>(</span><br><span class="line">      docTopicCounts: <span class="type">TopicCounts</span>,</span><br><span class="line">      termTopicCounts: <span class="type">TopicCounts</span>,</span><br><span class="line">      totalTopicCounts: <span class="type">TopicCounts</span>,</span><br><span class="line">      vocabSize: <span class="type">Int</span>,</span><br><span class="line">      eta: <span class="type">Double</span>,</span><br><span class="line">      alpha: <span class="type">Double</span>): <span class="type">TopicCounts</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> <span class="type">K</span> = docTopicCounts.length</span><br><span class="line">    <span class="keyword">val</span> <span class="type">N_j</span> = docTopicCounts.data</span><br><span class="line">    <span class="keyword">val</span> <span class="type">N_w</span> = termTopicCounts.data</span><br><span class="line">    <span class="keyword">val</span> <span class="type">N</span> = totalTopicCounts.data</span><br><span class="line">    <span class="keyword">val</span> eta1 = eta - <span class="number">1.0</span></span><br><span class="line">    <span class="keyword">val</span> alpha1 = alpha - <span class="number">1.0</span></span><br><span class="line">    <span class="keyword">val</span> <span class="type">Weta1</span> = vocabSize * eta1</span><br><span class="line">    <span class="keyword">var</span> sum = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">val</span> gamma_wj = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">Double</span>](<span class="type">K</span>)</span><br><span class="line">    <span class="keyword">var</span> k = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> (k &lt; <span class="type">K</span>) &#123;</span><br><span class="line">      <span class="keyword">val</span> gamma_wjk = (<span class="type">N_w</span>(k) + eta1) * (<span class="type">N_j</span>(k) + alpha1) / (<span class="type">N</span>(k) + <span class="type">Weta1</span>)</span><br><span class="line">      gamma_wj(k) = gamma_wjk</span><br><span class="line">      sum += gamma_wjk</span><br><span class="line">      k += <span class="number">1</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// normalize</span></span><br><span class="line">    <span class="type">BDV</span>(gamma_wj) /= sum</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;这段代码比较简单，完全按照公式 <strong>(3.1.6)</strong> 表示的样子来实现。<code>val gamma_wjk = (N_w(k) + eta1) * (N_j(k) + alpha1) / (N(k) + Weta1)</code>就是实现的更新逻辑。</p>
<h3 id="M-步：更新phi和theta"><a href="#M-步：更新phi和theta" class="headerlink" title="M-步：更新phi和theta"></a>M-步：更新phi和theta</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// M-STEP: 聚合计算新的 N_&#123;kj&#125;, N_&#123;wk&#125; counts.</span></span><br><span class="line"><span class="keyword">val</span> docTopicDistributions: <span class="type">VertexRDD</span>[<span class="type">TopicCounts</span>] =</span><br><span class="line">   graph.aggregateMessages[(<span class="type">Boolean</span>, <span class="type">TopicCounts</span>)](sendMsg, mergeMsg).mapValues(_._2)</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;我们由公式 <strong>(3.1.7)</strong> 可知，更新隐藏变量<code>phi</code>和<code>theta</code>就是更新相应的<code>N_kj</code>和<code>N_wk</code>。聚合更新使用<code>aggregateMessages</code>方法来实现。请参考文献【7】来了解该方法的作用。</p>
<h2 id="在线变分算法的代码实现"><a href="#在线变分算法的代码实现" class="headerlink" title="在线变分算法的代码实现"></a>在线变分算法的代码实现</h2><h3 id="初始化状态-1"><a href="#初始化状态-1" class="headerlink" title="初始化状态"></a>初始化状态</h3><p>&emsp;&emsp;在线学习算法首先使用方法<code>initialize</code>方法初始化参数值</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="keyword">private</span>[clustering] <span class="function"><span class="keyword">def</span> <span class="title">initialize</span></span>(</span><br><span class="line">      docs: <span class="type">RDD</span>[(<span class="type">Long</span>, <span class="type">Vector</span>)],</span><br><span class="line">      lda: <span class="type">LDA</span>): <span class="type">OnlineLDAOptimizer</span> = &#123;</span><br><span class="line">    <span class="keyword">this</span>.k = lda.getK</span><br><span class="line">    <span class="keyword">this</span>.corpusSize = docs.count()</span><br><span class="line">    <span class="keyword">this</span>.vocabSize = docs.first()._2.size</span><br><span class="line">    <span class="keyword">this</span>.alpha = <span class="keyword">if</span> (lda.getAsymmetricDocConcentration.size == <span class="number">1</span>) &#123;</span><br><span class="line">      <span class="keyword">if</span> (lda.getAsymmetricDocConcentration(<span class="number">0</span>) == <span class="number">-1</span>) <span class="type">Vectors</span>.dense(<span class="type">Array</span>.fill(k)(<span class="number">1.0</span> / k))</span><br><span class="line">      <span class="keyword">else</span> &#123;</span><br><span class="line">        require(lda.getAsymmetricDocConcentration(<span class="number">0</span>) &gt;= <span class="number">0</span>,</span><br><span class="line">          <span class="string">s&quot;all entries in alpha must be &gt;=0, got: <span class="subst">$alpha</span>&quot;</span>)</span><br><span class="line">        <span class="type">Vectors</span>.dense(<span class="type">Array</span>.fill(k)(lda.getAsymmetricDocConcentration(<span class="number">0</span>)))</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      require(lda.getAsymmetricDocConcentration.size == k,</span><br><span class="line">        <span class="string">s&quot;alpha must have length k, got: <span class="subst">$alpha</span>&quot;</span>)</span><br><span class="line">      lda.getAsymmetricDocConcentration.foreachActive &#123; <span class="keyword">case</span> (_, x) =&gt;</span><br><span class="line">        require(x &gt;= <span class="number">0</span>, <span class="string">s&quot;all entries in alpha must be &gt;= 0, got: <span class="subst">$alpha</span>&quot;</span>)</span><br><span class="line">      &#125;</span><br><span class="line">      lda.getAsymmetricDocConcentration</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">this</span>.eta = <span class="keyword">if</span> (lda.getTopicConcentration == <span class="number">-1</span>) <span class="number">1.0</span> / k <span class="keyword">else</span> lda.getTopicConcentration</span><br><span class="line">    <span class="keyword">this</span>.randomGenerator = <span class="keyword">new</span> <span class="type">Random</span>(lda.getSeed)</span><br><span class="line">    <span class="keyword">this</span>.docs = docs</span><br><span class="line">    <span class="comment">// 初始化变分分布 q(beta|lambda)</span></span><br><span class="line">    <span class="keyword">this</span>.lambda = getGammaMatrix(k, vocabSize)</span><br><span class="line">    <span class="keyword">this</span>.iteration = <span class="number">0</span></span><br><span class="line">    <span class="keyword">this</span></span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;根据文献【5】，<code>alpha</code>和<code>eta</code>的值大于等于0，并且默认为<code>1.0/k</code>。上文使用<code>getGammaMatrix</code>方法来初始化变分分布<code>q(beta|lambda)</code>。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">getGammaMatrix</span></span>(row: <span class="type">Int</span>, col: <span class="type">Int</span>): <span class="type">BDM</span>[<span class="type">Double</span>] = &#123;</span><br><span class="line">    <span class="keyword">val</span> randBasis = <span class="keyword">new</span> <span class="type">RandBasis</span>(<span class="keyword">new</span> org.apache.commons.math3.random.<span class="type">MersenneTwister</span>(</span><br><span class="line">      randomGenerator.nextLong()))</span><br><span class="line">    <span class="comment">//初始化一个gamma分布</span></span><br><span class="line">    <span class="keyword">val</span> gammaRandomGenerator = <span class="keyword">new</span> <span class="type">Gamma</span>(gammaShape, <span class="number">1.0</span> / gammaShape)(randBasis)</span><br><span class="line">    <span class="keyword">val</span> temp = gammaRandomGenerator.sample(row * col).toArray</span><br><span class="line">    <span class="keyword">new</span> <span class="type">BDM</span>[<span class="type">Double</span>](col, row, temp).t</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;<code>getGammaMatrix</code>方法使用<code>gamma</code>分布初始化一个随机矩阵。</p>
<h3 id="更新参数"><a href="#更新参数" class="headerlink" title="更新参数"></a>更新参数</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="keyword">private</span>[clustering] <span class="function"><span class="keyword">def</span> <span class="title">next</span></span>(): <span class="type">OnlineLDAOptimizer</span> = &#123;</span><br><span class="line">    <span class="comment">//返回文档集中采样的子集</span></span><br><span class="line">    <span class="comment">//默认情况下，文档可以被采样多次，且采样比例是0.05</span></span><br><span class="line">    <span class="keyword">val</span> batch = docs.sample(withReplacement = sampleWithReplacement, miniBatchFraction,</span><br><span class="line">      randomGenerator.nextLong())</span><br><span class="line">    <span class="keyword">if</span> (batch.isEmpty()) <span class="keyword">return</span> <span class="keyword">this</span></span><br><span class="line">    submitMiniBatch(batch)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;以上的<code>next</code>方法首先对文档进行采样，然后调用<code>submitMiniBatch</code>对采样的文档子集进行处理。下面我们详细分解<code>submitMiniBatch</code>方法。</p>
<ul>
<li><strong>1</strong> 计算<code>log(beta)</code>的期望，并将其作为广播变量广播到集群中</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> expElogbeta = exp(<span class="type">LDAUtils</span>.dirichletExpectation(lambda)).t</span><br><span class="line"><span class="comment">//广播变量</span></span><br><span class="line"><span class="keyword">val</span> expElogbetaBc = batch.sparkContext.broadcast(expElogbeta)</span><br><span class="line"><span class="comment">//参数alpha是dirichlet参数</span></span><br><span class="line"><span class="keyword">private</span>[clustering] <span class="function"><span class="keyword">def</span> <span class="title">dirichletExpectation</span></span>(alpha: <span class="type">BDM</span>[<span class="type">Double</span>]): <span class="type">BDM</span>[<span class="type">Double</span>] = &#123;</span><br><span class="line">    <span class="keyword">val</span> rowSum = sum(alpha(breeze.linalg.*, ::))</span><br><span class="line">    <span class="keyword">val</span> digAlpha = digamma(alpha)</span><br><span class="line">    <span class="keyword">val</span> digRowSum = digamma(rowSum)</span><br><span class="line">    <span class="keyword">val</span> result = digAlpha(::, breeze.linalg.*) - digRowSum</span><br><span class="line">    result</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;上述代码调用<code>exp(LDAUtils.dirichletExpectation(lambda))</code>方法实现参数为<code>lambda</code>的<code>log beta</code>的期望。实现原理参见公式 **(3.2.6)**。</p>
<ul>
<li><strong>2</strong> 计算<code>phi</code>以及<code>gamma</code>，即 <strong>算法2</strong> 中的<code>E-步</code></li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//对采样文档进行分区处理</span></span><br><span class="line"><span class="keyword">val</span> stats: <span class="type">RDD</span>[(<span class="type">BDM</span>[<span class="type">Double</span>], <span class="type">List</span>[<span class="type">BDV</span>[<span class="type">Double</span>]])] = batch.mapPartitions &#123; docs =&gt;</span><br><span class="line">      <span class="comment">//</span></span><br><span class="line">      <span class="keyword">val</span> nonEmptyDocs = docs.filter(_._2.numNonzeros &gt; <span class="number">0</span>)</span><br><span class="line">      <span class="keyword">val</span> stat = <span class="type">BDM</span>.zeros[<span class="type">Double</span>](k, vocabSize)</span><br><span class="line">      <span class="keyword">var</span> gammaPart = <span class="type">List</span>[<span class="type">BDV</span>[<span class="type">Double</span>]]()</span><br><span class="line">      nonEmptyDocs.foreach &#123; <span class="keyword">case</span> (_, termCounts: <span class="type">Vector</span>) =&gt;</span><br><span class="line">        <span class="keyword">val</span> ids: <span class="type">List</span>[<span class="type">Int</span>] = termCounts <span class="keyword">match</span> &#123;</span><br><span class="line">          <span class="keyword">case</span> v: <span class="type">DenseVector</span> =&gt; (<span class="number">0</span> until v.size).toList</span><br><span class="line">          <span class="keyword">case</span> v: <span class="type">SparseVector</span> =&gt; v.indices.toList</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">val</span> (gammad, sstats) = <span class="type">OnlineLDAOptimizer</span>.variationalTopicInference(</span><br><span class="line">          termCounts, expElogbetaBc.value, alpha, gammaShape, k)</span><br><span class="line">        stat(::, ids) := stat(::, ids).toDenseMatrix + sstats</span><br><span class="line">        gammaPart = gammad :: gammaPart</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="type">Iterator</span>((stat, gammaPart))</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;上面的代码调用<code>OnlineLDAOptimizer.variationalTopicInference</code>实现 <strong>算法2</strong> 中的<code>E-步</code>,迭代计算<code>phi</code>和<code>gamma</code>。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[clustering] <span class="function"><span class="keyword">def</span> <span class="title">variationalTopicInference</span></span>(</span><br><span class="line">      termCounts: <span class="type">Vector</span>,</span><br><span class="line">      expElogbeta: <span class="type">BDM</span>[<span class="type">Double</span>],</span><br><span class="line">      alpha: breeze.linalg.<span class="type">Vector</span>[<span class="type">Double</span>],</span><br><span class="line">      gammaShape: <span class="type">Double</span>,</span><br><span class="line">      k: <span class="type">Int</span>): (<span class="type">BDV</span>[<span class="type">Double</span>], <span class="type">BDM</span>[<span class="type">Double</span>]) = &#123;</span><br><span class="line">    <span class="keyword">val</span> (ids: <span class="type">List</span>[<span class="type">Int</span>], cts: <span class="type">Array</span>[<span class="type">Double</span>]) = termCounts <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> v: <span class="type">DenseVector</span> =&gt; ((<span class="number">0</span> until v.size).toList, v.values)</span><br><span class="line">      <span class="keyword">case</span> v: <span class="type">SparseVector</span> =&gt; (v.indices.toList, v.values)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 初始化变分分布 q(theta|gamma) </span></span><br><span class="line">    <span class="keyword">val</span> gammad: <span class="type">BDV</span>[<span class="type">Double</span>] = <span class="keyword">new</span> <span class="type">Gamma</span>(gammaShape, <span class="number">1.0</span> / gammaShape).samplesVector(k)   <span class="comment">// K</span></span><br><span class="line">    <span class="comment">//根据公式（3.2.6）计算 E(log theta)</span></span><br><span class="line">    <span class="keyword">val</span> expElogthetad: <span class="type">BDV</span>[<span class="type">Double</span>] = exp(<span class="type">LDAUtils</span>.dirichletExpectation(gammad))  <span class="comment">// K</span></span><br><span class="line">    <span class="keyword">val</span> expElogbetad = expElogbeta(ids, ::).toDenseMatrix                        <span class="comment">// ids * K</span></span><br><span class="line">    <span class="comment">//根据公式（3.2.5）计算phi，这里加1e-100表示并非严格等于</span></span><br><span class="line">    <span class="keyword">val</span> phiNorm: <span class="type">BDV</span>[<span class="type">Double</span>] = expElogbetad * expElogthetad :+ <span class="number">1e-100</span>            <span class="comment">// ids</span></span><br><span class="line">    <span class="keyword">var</span> meanGammaChange = <span class="number">1</span>D</span><br><span class="line">    <span class="keyword">val</span> ctsVector = <span class="keyword">new</span> <span class="type">BDV</span>[<span class="type">Double</span>](cts)                                         <span class="comment">// ids</span></span><br><span class="line">    <span class="comment">// 迭代直至收敛</span></span><br><span class="line">    <span class="keyword">while</span> (meanGammaChange &gt; <span class="number">1e-3</span>) &#123;</span><br><span class="line">      <span class="keyword">val</span> lastgamma = gammad.copy</span><br><span class="line">      <span class="comment">//依据公式(3.2.5)计算gamma</span></span><br><span class="line">      gammad := (expElogthetad :* (expElogbetad.t * (ctsVector :/ phiNorm))) :+ alpha</span><br><span class="line">      <span class="comment">//根据更新的gamma，计算E(log theta)</span></span><br><span class="line">      expElogthetad := exp(<span class="type">LDAUtils</span>.dirichletExpectation(gammad))</span><br><span class="line">      <span class="comment">// 更新phi</span></span><br><span class="line">      phiNorm := expElogbetad * expElogthetad :+ <span class="number">1e-100</span></span><br><span class="line">      <span class="comment">//计算两次gamma的差值</span></span><br><span class="line">      meanGammaChange = sum(abs(gammad - lastgamma)) / k</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">val</span> sstatsd = expElogthetad.asDenseMatrix.t * (ctsVector :/ phiNorm).asDenseMatrix</span><br><span class="line">    (gammad, sstatsd)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>3</strong> 更新<code>lambda</code></li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> statsSum: <span class="type">BDM</span>[<span class="type">Double</span>] = stats.map(_._1).reduce(_ += _)</span><br><span class="line"><span class="keyword">val</span> gammat: <span class="type">BDM</span>[<span class="type">Double</span>] = breeze.linalg.<span class="type">DenseMatrix</span>.vertcat(</span><br><span class="line">  stats.map(_._2).reduce(_ ++ _).map(_.toDenseMatrix): _*)</span><br><span class="line"><span class="keyword">val</span> batchResult = statsSum :* expElogbeta.t</span><br><span class="line"><span class="comment">// 更新lambda和alpha</span></span><br><span class="line">updateLambda(batchResult, (miniBatchFraction * corpusSize).ceil.toInt)</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;<code>updateLambda</code>方法实现 <strong>算法2</strong> 中的<code>M-步</code>,更新<code>lambda</code>。实现代码如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">updateLambda</span></span>(stat: <span class="type">BDM</span>[<span class="type">Double</span>], batchSize: <span class="type">Int</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// 根据公式（3.2.8）计算权重</span></span><br><span class="line">    <span class="keyword">val</span> weight = rho()</span><br><span class="line">    <span class="comment">// 更新lambda，其中stat * (corpusSize.toDouble / batchSize.toDouble)+eta表示rho_cap</span></span><br><span class="line">    lambda := (<span class="number">1</span> - weight) * lambda +</span><br><span class="line">      weight * (stat * (corpusSize.toDouble / batchSize.toDouble) + eta)</span><br><span class="line">  &#125;</span><br><span class="line"><span class="comment">// 根据公式（3.2.8）计算rho</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">rho</span></span>(): <span class="type">Double</span> = &#123;</span><br><span class="line">    math.pow(getTau0 + <span class="keyword">this</span>.iteration, -getKappa)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>4</strong> 更新<code>alpha</code></li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">updateAlpha</span></span>(gammat: <span class="type">BDM</span>[<span class="type">Double</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//计算rho</span></span><br><span class="line">    <span class="keyword">val</span> weight = rho()</span><br><span class="line">    <span class="keyword">val</span> <span class="type">N</span> = gammat.rows.toDouble</span><br><span class="line">    <span class="keyword">val</span> alpha = <span class="keyword">this</span>.alpha.toBreeze.toDenseVector</span><br><span class="line">    <span class="comment">//计算log p_hat</span></span><br><span class="line">    <span class="keyword">val</span> logphat: <span class="type">BDM</span>[<span class="type">Double</span>] = sum(<span class="type">LDAUtils</span>.dirichletExpectation(gammat)(::, breeze.linalg.*)) / <span class="type">N</span></span><br><span class="line">    <span class="comment">//计算梯度为N（-phi(alpha)+log p_hat）</span></span><br><span class="line">    <span class="keyword">val</span> gradf = <span class="type">N</span> * (-<span class="type">LDAUtils</span>.dirichletExpectation(alpha) + logphat.toDenseVector)</span><br><span class="line">    <span class="comment">//计算公式（3.3.4）中的c，trigamma表示gamma函数的二阶导数</span></span><br><span class="line">    <span class="keyword">val</span> c = <span class="type">N</span> * trigamma(sum(alpha))</span><br><span class="line">    <span class="comment">//计算公式（3.3.4）中的q</span></span><br><span class="line">    <span class="keyword">val</span> q = -<span class="type">N</span> * trigamma(alpha)</span><br><span class="line">    <span class="comment">//根据公式(3.3.7)计算b</span></span><br><span class="line">    <span class="keyword">val</span> b = sum(gradf / q) / (<span class="number">1</span>D / c + sum(<span class="number">1</span>D / q))</span><br><span class="line">    <span class="keyword">val</span> dalpha = -(gradf - b) / q</span><br><span class="line">    <span class="keyword">if</span> (all((weight * dalpha + alpha) :&gt; <span class="number">0</span>D)) &#123;</span><br><span class="line">      alpha :+= weight * dalpha</span><br><span class="line">      <span class="keyword">this</span>.alpha = <span class="type">Vectors</span>.dense(alpha.toArray)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p>【1】<a target="_blank" rel="noopener" href="http://www.52nlp.cn/lda-math-%E6%B1%87%E6%80%BB-lda%E6%95%B0%E5%AD%A6%E5%85%AB%E5%8D%A6">LDA数学八卦</a></p>
<p>【2】<a target="_blank" rel="noopener" href="http://blog.csdn.net/v_july_v/article/details/41209515">通俗理解LDA主题模型</a></p>
<p>【3】[Latent Dirichlet Allocation](docs/Latent Dirichlet Allocation.pdf)</p>
<p>【4】[On Smoothing and Inference for Topic Models](docs/On Smoothing and Inference for Topic Models.pdf)</p>
<p>【5】[Online Learning for Latent Dirichlet Allocation](docs/Online Learning for Latent Dirichlet Allocation.pdf)</p>
<p>【6】<a target="_blank" rel="noopener" href="https://spark.apache.org/docs/latest/mllib-clustering.html#latent-dirichlet-allocation-lda">Spark官方文档</a></p>
<p>【7】<a target="_blank" rel="noopener" href="https://github.com/endymecy/spark-graphx-source-analysis">Spark GraphX介绍</a></p>
<p>【8】<a href="docs/dirichlet.pdf">Maximum Likelihood Estimation of Dirichlet Distribution Parameters</a></p>
<p>【9】<a target="_blank" rel="noopener" href="http://www.blog.huajh7.com/variational-bayes/">Variational Bayes</a></p>
</div><div class="article-licensing box"><div class="licensing-title"><p>隐式狄利克雷分布</p><p><a href="https://huzhiliang.com/2019/08/02/lda/">https://huzhiliang.com/2019/08/02/lda/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>作者</h6><p>ฅ´ω`ฅ</p></div></div><div class="level-item is-narrow"><div><h6>发布于</h6><p>2019-08-02</p></div></div><div class="level-item is-narrow"><div><h6>更新于</h6><p>2019-08-03</p></div></div><div class="level-item is-narrow"><div><h6>许可协议</h6><p><a class="icon" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a><a class="icon" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a><a class="icon" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/spark/">spark</a><a class="link-muted mr-2" rel="tag" href="/tags/%E8%81%9A%E7%B1%BB/">聚类</a></div><div class="notification is-danger">You need to set <code>install_url</code> to use ShareThis. Please set it in <code>_config.yml</code>.</div></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">喜欢这篇文章？打赏一下作者吧</h3><div class="buttons is-centered"><a class="button donate" href="/" target="_blank" rel="noopener" data-type="afdian"><span class="icon is-small"><i class="fas fa-charging-station"></i></span><span>爱发电</span></a><a class="button donate" data-type="alipay"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>支付宝</span><span class="qrcode"><img src="/" alt="支付宝"></span></a><a class="button donate" href="/" target="_blank" rel="noopener" data-type="buymeacoffee"><span class="icon is-small"><i class="fas fa-coffee"></i></span><span>送我杯咖啡</span></a><a class="button donate" href="/" target="_blank" rel="noopener" data-type="patreon"><span class="icon is-small"><i class="fab fa-patreon"></i></span><span>Patreon</span></a><div class="notification is-danger">You forgot to set the <code>business</code> or <code>currency_code</code> for Paypal. Please set it in <code>_config.yml</code>.</div><a class="button donate" data-type="wechat"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>微信</span><span class="qrcode"><img src="/" alt="微信"></span></a></div></div></div><div class="card"><nav class="post-navigation mt-4 level is-mobile card-content"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2019/08/03/lsvm/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">线性支持向量机</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2019/08/01/pic/"><span class="level-item">快速迭代聚类</span><i class="level-item fas fa-chevron-right"></i></a></div></nav></div><div class="card"><div class="card-content"><h3 class="title is-5">评论</h3><div class="notification is-danger">You forgot to set the <code>shortname</code> for Disqus. Please set it in <code>_config.yml</code>.</div></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/avatar.png" alt="Your name"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Your name</p><p class="is-size-6 is-block">Your title</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Your location</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives"><p class="title">161</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories"><p class="title">14</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags"><p class="title">97</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/ppoffice" target="_blank" rel="noopener" id="widget-follow">微博 Weibo</a></div><div class="level is-mobile"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/ppoffice"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">链接</h3><ul class="menu-list"><li><a class="level is-mobile is-mobile" href="https://hexo.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Hexo</span></span><span class="level-right"><span class="level-item tag">hexo.io</span></span></a></li><li><a class="level is-mobile is-mobile" href="https://bulma.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Bulma</span></span><span class="level-right"><span class="level-item tag">bulma.io</span></span></a></li></ul></div><a class="link-more button is-light is-small size-small" href="/friends/">查看更多</a></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/C/"><span class="level-start"><span class="level-item">C++</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Golang/"><span class="level-start"><span class="level-item">Golang</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Kaggle/"><span class="level-start"><span class="level-item">Kaggle</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/LeetCode/"><span class="level-start"><span class="level-item">LeetCode</span></span><span class="level-end"><span class="level-item tag">85</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/"><span class="level-start"><span class="level-item">Python</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"><span class="level-start"><span class="level-item">数据结构</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"><span class="level-start"><span class="level-item">机器学习</span></span><span class="level-end"><span class="level-item tag">26</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"><span class="level-start"><span class="level-item">深度学习</span></span><span class="level-end"><span class="level-item tag">29</span></span></a><ul><li><a class="level is-mobile" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/CNN/"><span class="level-start"><span class="level-item">CNN</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">16</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/"><span class="level-start"><span class="level-item">迁移学习</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%E7%A0%B4%E8%A7%A3/"><span class="level-start"><span class="level-item">破解</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E7%AE%97%E6%B3%95/"><span class="level-start"><span class="level-item">算法</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%8B%B1%E8%AF%AD%E5%AD%A6%E4%B9%A0/"><span class="level-start"><span class="level-item">英语学习</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">最新文章</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2020-03-05T05:41:28.000Z">2020-03-05</time></p><p class="title"><a href="/2020/03/05/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BACRF-%E4%B8%89-%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E4%B8%8E%E7%BB%B4%E7%89%B9%E6%AF%94%E7%AE%97%E6%B3%95%E8%A7%A3%E7%A0%81/">条件随机场CRF(三) 模型学习与维特比算法解码</a></p><p class="categories"><a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a> / <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP/">NLP</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2020-03-04T08:24:55.000Z">2020-03-04</time></p><p class="title"><a href="/2020/03/04/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BACRF-%E4%BA%8C-%E5%89%8D%E5%90%91%E5%90%8E%E5%90%91%E7%AE%97%E6%B3%95%E8%AF%84%E4%BC%B0%E6%A0%87%E8%AE%B0%E5%BA%8F%E5%88%97%E6%A6%82%E7%8E%87/">条件随机场CRF(二) 前向后向算法评估标记序列概率</a></p><p class="categories"><a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a> / <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP/">NLP</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2020-03-03T02:59:49.000Z">2020-03-03</time></p><p class="title"><a href="/2020/03/03/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BACRF-%E4%B8%80-%E4%BB%8E%E9%9A%8F%E6%9C%BA%E5%9C%BA%E5%88%B0%E7%BA%BF%E6%80%A7%E9%93%BE%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/">条件随机场CRF(一)从随机场到线性链条件随机场</a></p><p class="categories"><a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a> / <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP/">NLP</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2020-03-02T02:46:01.000Z">2020-03-02</time></p><p class="title"><a href="/2020/03/02/%E6%96%87%E6%9C%AC%E6%8C%96%E6%8E%98%E7%9A%84%E5%88%86%E8%AF%8D%E5%8E%9F%E7%90%86/">文本挖掘的分词原理</a></p><p class="categories"><a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a> / <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP/">NLP</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2020-02-20T01:01:00.000Z">2020-02-20</time></p><p class="title"><a href="/2020/02/20/optimize-water-distribution-in-a-village/">optimize water distribution in a village</a></p><p class="categories"><a href="/categories/LeetCode/">LeetCode</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">归档</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2020/03/"><span class="level-start"><span class="level-item">三月 2020</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/02/"><span class="level-start"><span class="level-item">二月 2020</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/01/"><span class="level-start"><span class="level-item">一月 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/12/"><span class="level-start"><span class="level-item">十二月 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/11/"><span class="level-start"><span class="level-item">十一月 2019</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/10/"><span class="level-start"><span class="level-item">十月 2019</span></span><span class="level-end"><span class="level-item tag">33</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/09/"><span class="level-start"><span class="level-item">九月 2019</span></span><span class="level-end"><span class="level-item tag">16</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/08/"><span class="level-start"><span class="level-item">八月 2019</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/07/"><span class="level-start"><span class="level-item">七月 2019</span></span><span class="level-end"><span class="level-item tag">17</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/05/"><span class="level-start"><span class="level-item">五月 2019</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/04/"><span class="level-start"><span class="level-item">四月 2019</span></span><span class="level-end"><span class="level-item tag">12</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/03/"><span class="level-start"><span class="level-item">三月 2019</span></span><span class="level-end"><span class="level-item tag">17</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/02/"><span class="level-start"><span class="level-item">二月 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/01/"><span class="level-start"><span class="level-item">一月 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/08/"><span class="level-start"><span class="level-item">八月 2018</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/07/"><span class="level-start"><span class="level-item">七月 2018</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/05/"><span class="level-start"><span class="level-item">五月 2018</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/03/"><span class="level-start"><span class="level-item">三月 2018</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/02/"><span class="level-start"><span class="level-item">二月 2018</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/01/"><span class="level-start"><span class="level-item">一月 2018</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2017/12/"><span class="level-start"><span class="level-item">十二月 2017</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2017/11/"><span class="level-start"><span class="level-item">十一月 2017</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2017/10/"><span class="level-start"><span class="level-item">十月 2017</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2017/09/"><span class="level-start"><span class="level-item">九月 2017</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2017/07/"><span class="level-start"><span class="level-item">七月 2017</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2017/06/"><span class="level-start"><span class="level-item">六月 2017</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2017/05/"><span class="level-start"><span class="level-item">五月 2017</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2017/03/"><span class="level-start"><span class="level-item">三月 2017</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2017/02/"><span class="level-start"><span class="level-item">二月 2017</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/archives/2015/04/"><span class="level-start"><span class="level-item">四月 2015</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2015/02/"><span class="level-start"><span class="level-item">二月 2015</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">标签</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/Array/"><span class="tag">Array</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Attention/"><span class="tag">Attention</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Binary-Tree/"><span class="tag">Binary Tree</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/C/"><span class="tag">C++</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CFG/"><span class="tag">CFG</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CNN/"><span class="tag">CNN</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CPU/"><span class="tag">CPU</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CYK/"><span class="tag">CYK</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Caffee/"><span class="tag">Caffee</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/English/"><span class="tag">English</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Facebook/"><span class="tag">Facebook</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/GAN/"><span class="tag">GAN</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/GBDT/"><span class="tag">GBDT</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/GPU/"><span class="tag">GPU</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/GRN/"><span class="tag">GRN</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/GloVe/"><span class="tag">GloVe</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Go/"><span class="tag">Go</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Google-apac/"><span class="tag">Google apac</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Go%E8%AF%AD%E8%A8%80/"><span class="tag">Go语言</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Greedy/"><span class="tag">Greedy</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/HMM/"><span class="tag">HMM</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/IBM-Modes/"><span class="tag">IBM Modes</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/KL%E6%95%A3%E5%BA%A6/"><span class="tag">KL散度</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/LSTM/"><span class="tag">LSTM</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/LeetCode/"><span class="tag">LeetCode</span><span class="tag">31</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Lexicalized-PCFG/"><span class="tag">Lexicalized PCFG</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Lexized-PCFG/"><span class="tag">Lexized PCFG</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Log-Linear-Models/"><span class="tag">Log-Linear Models</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Machine-Learning/"><span class="tag">Machine Learning</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/PCFG/"><span class="tag">PCFG</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Paper/"><span class="tag">Paper</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Programmer/"><span class="tag">Programmer</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Python/"><span class="tag">Python</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Pytorch/"><span class="tag">Pytorch</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Queue/"><span class="tag">Queue</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/RNN/"><span class="tag">RNN</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Seq2seq/"><span class="tag">Seq2seq</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Tagging/"><span class="tag">Tagging</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Tencent/"><span class="tag">Tencent</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Tensorflow/"><span class="tag">Tensorflow</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/VIP/"><span class="tag">VIP</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Viterbi/"><span class="tag">Viterbi</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Word-representation/"><span class="tag">Word representation</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/XGBoost/"><span class="tag">XGBoost</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/exhaustive-search/"><span class="tag">exhaustive search</span><span class="tag">7</span></a></div><div class="control"><a class="tags has-addons" href="/tags/git/"><span class="tag">git</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/github/"><span class="tag">github</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/numpy/"><span class="tag">numpy</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/python/"><span class="tag">python</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/spark/"><span class="tag">spark</span><span class="tag">23</span></a></div><div class="control"><a class="tags has-addons" href="/tags/turtle/"><span class="tag">turtle</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/word2vec/"><span class="tag">word2vec</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E4%B8%93%E9%A2%98/"><span class="tag">专题</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E4%BA%8C%E5%88%86%E6%90%9C%E7%B4%A2/"><span class="tag">二分搜索</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E4%BA%8C%E5%88%86%E6%90%9C%E7%B4%A2%E6%A0%91/"><span class="tag">二分搜索树</span><span class="tag">9</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E4%BA%8C%E5%88%86%E6%A0%91/"><span class="tag">二分树</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E4%BA%8C%E8%BF%9B%E5%88%B6/"><span class="tag">二进制</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/"><span class="tag">优化算法</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%88%86%E7%B1%BB/"><span class="tag">分类</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%88%86%E8%AF%8D/"><span class="tag">分词</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%89%91%E6%8C%87offer/"><span class="tag">剑指offer</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%8A%A8%E6%80%81%E5%9B%9E%E5%BD%92/"><span class="tag">动态回归</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"><span class="tag">动态规划</span><span class="tag">22</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%9B%9E%E5%BD%92/"><span class="tag">回归</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%9B%BE%E8%AE%BA/"><span class="tag">图论</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%8E%92%E5%BA%8F/"><span class="tag">排序</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%95%B0%E5%AD%A6/"><span class="tag">数学</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86/"><span class="tag">数学原理</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"><span class="tag">数据分析</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/"><span class="tag">条件随机场</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6/"><span class="tag">极大似然</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9/"><span class="tag">模型压缩</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/"><span class="tag">模型部署</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"><span class="tag">深度学习</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%89%88%E6%9D%83/"><span class="tag">版权</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%89%B9%E5%BE%81%E6%8A%BD%E5%8F%96/"><span class="tag">特征抽取</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%AC%94%E8%AE%B0/"><span class="tag">笔记</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%AE%97%E6%B3%95/"><span class="tag">算法</span><span class="tag">9</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86/"><span class="tag">算法原理</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%AE%97%E6%B3%95%E5%A4%8D%E6%9D%82%E5%BA%A6/"><span class="tag">算法复杂度</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/"><span class="tag">线性模型</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%BB%84%E5%90%88%E6%A0%91/"><span class="tag">组合树</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C/"><span class="tag">经典网络</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%BB%9F%E8%AE%A1%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91/"><span class="tag">统计机器翻译</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%BD%91%E6%98%93%E4%BA%91%E9%9F%B3%E4%B9%90/"><span class="tag">网易云音乐</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%81%9A%E7%B1%BB/"><span class="tag">聚类</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%83%8C%E5%8C%85%E9%97%AE%E9%A2%98/"><span class="tag">背包问题</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%84%9A%E6%9C%AC/"><span class="tag">脚本</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%AF%AD%E6%B3%95/"><span class="tag">语法</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/"><span class="tag">语言模型</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E6%95%B4/"><span class="tag">超参数调整</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/"><span class="tag">迁移学习</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E9%80%92%E5%BD%92/"><span class="tag">递归</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E9%82%AE%E4%BB%B6/"><span class="tag">邮件</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E9%93%BE%E8%A1%A8/"><span class="tag">链表</span><span class="tag">19</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E9%99%8D%E7%BB%B4/"><span class="tag">降维</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E9%9D%A2%E8%AF%95%E4%B8%93%E5%9C%BA/"><span class="tag">面试专场</span><span class="tag">2</span></a></div></div></div></div></div><div class="card widget" id="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">订阅更新</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button is-primary" type="submit" value="订阅"></div></div></form></div></div></div><div class="card widget"><div class="card-content"><div class="notification is-danger">You need to set <code>client_id</code> and <code>slot_id</code> to show this AD unit. Please set it in <code>_config.yml</code>.</div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img class="logo-img" src="/" alt="MCFON" height="28"><img class="logo-img-dark" src="/" alt="MCFON" height="28"></a><p class="is-size-7"><span>&copy; 2020 ฅ´ω`ฅ</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/imaegoo/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a><a href="http://www.miitbeian.gov.cn" target="_blank">豫ICP备18017229号</a> - </p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" async></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><div class="searchbox-pinyin"><label class="checkbox"><input id="search-by-pinyin" type="checkbox" checked="checked"><span> 拼音检索</span></label></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/imaegoo/pinyin.js" defer></script><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script><script type="text/javascript" src="/js/imaegoo/imaegoo.js"></script><script type="text/javascript" src="/js/imaegoo/universe.js"></script><script type="text/javascript" src="/js/live2d/autoload.js"></script></body></html>