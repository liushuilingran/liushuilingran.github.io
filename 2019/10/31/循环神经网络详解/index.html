<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>循环神经网络详解 - MCFON</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="MCFON"><meta name="msapplication-TileImage" content="/images/favicon.png"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="MCFON"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta description="概述 CNN等传统神经网络的局限在于：将固定大小的向量作为输入（比如一张图片），然后输出一个固定大小的向量（比如不同分类的概率）。不仅如此，CNN还按照固定的计算步骤（比如模型中层的数量）来实现这样的输入输出。这样的神经网络没有持久性：假设你希望对电影中每一帧的事件类型进行分类，传统的神经网络就没有办法使用电影中先前的事件推断后续的事件。"><meta property="og:type" content="blog"><meta property="og:title" content="循环神经网络详解"><meta property="og:url" content="https://huzhiliang.com/2019/10/31/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/"><meta property="og:site_name" content="MCFON"><meta property="og:description" content="概述 CNN等传统神经网络的局限在于：将固定大小的向量作为输入（比如一张图片），然后输出一个固定大小的向量（比如不同分类的概率）。不仅如此，CNN还按照固定的计算步骤（比如模型中层的数量）来实现这样的输入输出。这样的神经网络没有持久性：假设你希望对电影中每一帧的事件类型进行分类，传统的神经网络就没有办法使用电影中先前的事件推断后续的事件。"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://tva1.sinaimg.cn/large/006y8mN6gy1g8grd7diyzj30cq0jr74y.jpg"><meta property="og:image" content="https://tva1.sinaimg.cn/large/006y8mN6gy1g8grdf83inj30jg054jrt.jpg"><meta property="og:image" content="https://tva1.sinaimg.cn/large/006y8mN6gy1g8gre48ytdj30jg08zwfc.jpg"><meta property="og:image" content="https://tva1.sinaimg.cn/large/006y8mN6gy1g8gred9mstj30jg06p74y.jpg"><meta property="og:image" content="https://tva1.sinaimg.cn/large/006y8mN6gy1g8grepjvvhj30jg07a74r.jpg"><meta property="og:image" content="https://tva1.sinaimg.cn/large/006y8mN6gy1g8grexgi2dj30jg07baar.jpg"><meta property="og:image" content="https://tva1.sinaimg.cn/large/006y8mN6gy1g8grfb9mm0j30jg03m3yr.jpg"><meta property="og:image" content="https://tva1.sinaimg.cn/large/006y8mN6gy1g8grfmoxanj30jg0600sv.jpg"><meta property="og:image" content="https://tva1.sinaimg.cn/large/006y8mN6gy1g8grg142wbj305i06qwei.jpg"><meta property="og:image" content="https://tva1.sinaimg.cn/large/006y8mN6gy1g8grgdk95kj30jg060aaa.jpg"><meta property="og:image" content="https://latex.codecogs.com/svg.latex?\dpi{300}&amp;space;W_{f}"><meta property="og:image" content="https://latex.codecogs.com/svg.latex?\dpi{300}&amp;space;[h_{t-1},x_{t}]"><meta property="og:image" content="https://latex.codecogs.com/svg.latex?\dpi{300}&amp;space;b_{f}"><meta property="og:image" content="https://latex.codecogs.com/svg.latex?\dpi{300}&amp;space;\sigma"><meta property="og:image" content="https://latex.codecogs.com/svg.latex?\dpi{300}&amp;space;d_{x}"><meta property="og:image" content="https://latex.codecogs.com/svg.latex?\dpi{300}&amp;space;d_{h}"><meta property="og:image" content="https://latex.codecogs.com/svg.latex?\dpi{300}&amp;space;d_{c}"><meta property="og:image" content="https://latex.codecogs.com/svg.latex?\dpi{300}&amp;space;d_{c}=d_{h}"><meta property="og:image" content="https://latex.codecogs.com/svg.latex?\dpi{300}&amp;space;W_{f}"><meta property="og:image" content="https://latex.codecogs.com/svg.latex?\dpi{300}&amp;space;d_{c}&amp;space;\times&amp;space;(d_{h}&amp;plus;d_{x})"><meta property="og:image" content="https://latex.codecogs.com/svg.latex?\dpi{300}&amp;space;W_{f}"><meta property="og:image" content="https://latex.codecogs.com/svg.latex?\dpi{300}&amp;space;W_{fh}"><meta property="og:image" content="https://latex.codecogs.com/svg.latex?\dpi{300}&amp;space;h_{t-1}"><meta property="og:image" content="https://latex.codecogs.com/svg.latex?\dpi{300}&amp;space;d_{c}\times&amp;space;d_{h}"><meta property="og:image" content="https://latex.codecogs.com/svg.latex?\dpi{300}&amp;space;W_{fx}"><meta property="og:image" content="https://latex.codecogs.com/svg.latex?\dpi{300}&amp;space;x_{t}"><meta property="og:image" content="https://latex.codecogs.com/svg.latex?\dpi{300}&amp;space;d_{c}\times&amp;space;d_{x}"><meta property="og:image" content="https://latex.codecogs.com/svg.latex?\dpi{300}&amp;space;W_{f}"><meta property="og:image" content="https://latex.codecogs.com/svg.latex?\dpi{300}&amp;space;[W_{f}]\begin{bmatrix}&amp;space;h_{t-1}\\&amp;space;x_{t}&amp;space;\end{bmatrix}=\begin{bmatrix}&amp;space;W_{fh}&amp;space;&amp;&amp;space;W_{fx}&amp;space;\end{bmatrix}\begin{bmatrix}&amp;space;h_{t-1}\\&amp;space;x_{t}&amp;space;\end{bmatrix}=W_{fh}h_{t-1}&amp;plus;W_{fx}x_{t}"><meta property="og:image" content="https://latex.codecogs.com/svg.latex?\dpi{300}&amp;space;h_{t-1}"><meta property="og:image" content="https://latex.codecogs.com/svg.latex?\dpi{300}&amp;space;x_{t}"><meta property="og:image" content="https://latex.codecogs.com/svg.latex?\dpi{300}&amp;space;i_{t}"><meta property="og:image" content="https://latex.codecogs.com/svg.latex?\dpi{300}&amp;space;h_{t-1}"><meta property="og:image" content="https://latex.codecogs.com/svg.latex?\dpi{300}&amp;space;x_{t}"><meta property="og:image" content="https://latex.codecogs.com/svg.latex?\dpi{300}&amp;space;\widetilde{C_{t}}"><meta property="og:image" content="https://tva1.sinaimg.cn/large/006y8mN6gy1g8grhmz9f1j30yg0anzl9.jpg"><meta property="og:image" content="https://latex.codecogs.com/svg.latex?\dpi{300}&amp;space;C_{t}=f_{t}*C_{t-1}&amp;plus;i_{t}*\widetilde{C_{t}}"><meta property="og:image" content="https://tva1.sinaimg.cn/large/006y8mN6gy1g8grirv3emj30yg0ant9g.jpg"><meta property="og:image" content="https://latex.codecogs.com/svg.latex?\dpi{300}&amp;space;o_{t}"><meta property="og:image" content="https://latex.codecogs.com/svg.latex?\dpi{300}&amp;space;o_{t}"><meta property="og:image" content="https://latex.codecogs.com/svg.latex?\dpi{300}&amp;space;h_{t}"><meta property="og:image" content="https://tva1.sinaimg.cn/large/006y8mN6gy1g8grizvmnzj30jg0600t2.jpg"><meta property="og:image" content="https://tva1.sinaimg.cn/large/006y8mN6gy1g8grjmb6bwj319p0u0n1j.jpg"><meta property="og:image" content="https://tva1.sinaimg.cn/large/006y8mN6gy1g8grkc3cguj30o90f53zv.jpg"><meta property="og:image" content="https://latex.codecogs.com/svg.latex?LSTM_{L}"><meta property="og:image" content="https://latex.codecogs.com/svg.latex?\{h_{L0},&amp;space;h_{L1},&amp;space;h_{L2}\}"><meta property="og:image" content="https://latex.codecogs.com/svg.latex?LSTM_{R}"><meta property="og:image" content="https://latex.codecogs.com/svg.latex?\{h_{R0},&amp;space;h_{R1},&amp;space;h_{R2}\}"><meta property="og:image" content="https://latex.codecogs.com/svg.latex?\{[h_{L0},&amp;space;h_{R2}],&amp;space;[h_{L1},&amp;space;h_{R1}],&amp;space;[h_{L2},&amp;space;h_{R0}]\}"><meta property="og:image" content="https://latex.codecogs.com/svg.latex?\{h_{0},&amp;space;h_{1},&amp;space;h_{2}\}"><meta property="og:image" content="https://latex.codecogs.com/svg.latex?[h_{L0},&amp;space;h_{R2}]"><meta property="og:image" content="https://tva1.sinaimg.cn/large/006y8mN6gy1g8grkjrwirj30r80f6q4a.jpg"><meta property="article:published_time" content="2019-10-30T17:47:16.000Z"><meta property="article:modified_time" content="2019-11-02T12:29:07.361Z"><meta property="article:author" content="ฅ´ω`ฅ"><meta property="article:tag" content="RNN"><meta property="article:tag" content="GRN"><meta property="article:tag" content="LSTM"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="https://tva1.sinaimg.cn/large/006y8mN6gy1g8grd7diyzj30cq0jr74y.jpg"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://huzhiliang.com/2019/10/31/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/"},"headline":"MCFON","image":["https://tva1.sinaimg.cn/large/006y8mN6gy1g8grd7diyzj30cq0jr74y.jpg","https://tva1.sinaimg.cn/large/006y8mN6gy1g8grdf83inj30jg054jrt.jpg","https://tva1.sinaimg.cn/large/006y8mN6gy1g8gre48ytdj30jg08zwfc.jpg","https://tva1.sinaimg.cn/large/006y8mN6gy1g8gred9mstj30jg06p74y.jpg","https://tva1.sinaimg.cn/large/006y8mN6gy1g8grepjvvhj30jg07a74r.jpg","https://tva1.sinaimg.cn/large/006y8mN6gy1g8grexgi2dj30jg07baar.jpg","https://tva1.sinaimg.cn/large/006y8mN6gy1g8grfb9mm0j30jg03m3yr.jpg","https://tva1.sinaimg.cn/large/006y8mN6gy1g8grfmoxanj30jg0600sv.jpg","https://tva1.sinaimg.cn/large/006y8mN6gy1g8grg142wbj305i06qwei.jpg","https://tva1.sinaimg.cn/large/006y8mN6gy1g8grgdk95kj30jg060aaa.jpg","https://tva1.sinaimg.cn/large/006y8mN6gy1g8grhmz9f1j30yg0anzl9.jpg","https://tva1.sinaimg.cn/large/006y8mN6gy1g8grirv3emj30yg0ant9g.jpg","https://tva1.sinaimg.cn/large/006y8mN6gy1g8grizvmnzj30jg0600t2.jpg","https://tva1.sinaimg.cn/large/006y8mN6gy1g8grjmb6bwj319p0u0n1j.jpg","https://tva1.sinaimg.cn/large/006y8mN6gy1g8grkc3cguj30o90f53zv.jpg","https://tva1.sinaimg.cn/large/006y8mN6gy1g8grkjrwirj30r80f6q4a.jpg"],"datePublished":"2019-10-30T17:47:16.000Z","dateModified":"2019-11-02T12:29:07.361Z","author":{"@type":"Person","name":"ฅ´ω`ฅ"},"description":"概述 CNN等传统神经网络的局限在于：将固定大小的向量作为输入（比如一张图片），然后输出一个固定大小的向量（比如不同分类的概率）。不仅如此，CNN还按照固定的计算步骤（比如模型中层的数量）来实现这样的输入输出。这样的神经网络没有持久性：假设你希望对电影中每一帧的事件类型进行分类，传统的神经网络就没有办法使用电影中先前的事件推断后续的事件。"}</script><link rel="canonical" href="https://huzhiliang.com/2019/10/31/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/"><link rel="icon" href="/images/favicon.png"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.3.0"></head><body class="is-2-column"><script type="text/javascript" src="/js/imaegoo/night.js"></script><canvas id="universe"></canvas><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img class="logo-img" src="/" alt="MCFON" height="28"><img class="logo-img-dark" src="/" alt="MCFON" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item night" id="night-nav" title="Night Mode" href="javascript:;"><i class="fas fa-moon" id="night-icon"></i></a><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2019-10-30T17:47:16.000Z" title="2019-10-30T17:47:16.000Z">2019-10-31</time>发表</span><span class="level-item"><time dateTime="2019-11-02T12:29:07.361Z" title="2019-11-02T12:29:07.361Z">2019-11-02</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><span> / </span><a class="link-muted" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP/">NLP</a></span><span class="level-item">40 分钟读完 (大约5945个字)</span><span class="level-item leancloud_visitors" id="/2019/10/31/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/" data-flag-title="循环神经网络详解"><i class="far fa-eye"></i>&nbsp;&nbsp;<span id="twikoo_visitors"><i class="fa fa-spinner fa-spin"></i></span>次访问</span></div></div><h1 class="title is-3 is-size-4-mobile">循环神经网络详解</h1><div class="content"><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><blockquote>
<p>CNN等传统神经网络的局限在于：将固定大小的向量作为输入（比如一张图片），然后输出一个固定大小的向量（比如不同分类的概率）。不仅如此，CNN还按照固定的计算步骤（比如模型中层的数量）来实现这样的输入输出。这样的神经网络没有持久性：假设你希望对电影中每一帧的事件类型进行分类，传统的神经网络就没有办法使用电影中先前的事件推断后续的事件。</p>
</blockquote>
<a id="more"></a>
<blockquote>
<p>RNN 解决了这个问题。RNN 是包含循环的网络，允许信息的持久化。在自然语言处理(NLP)领域，RNN已经可以做语音识别、机器翻译、生成手写字符，以及构建强大的语言模型 (Sutskever et al.)，(Graves)，(Mikolov et al.)（字符级别和单词级别的都有。在机器视觉领域，RNN也非常流行。包括帧级别的视频分类，图像描述，视频描述以及基于图像的Q&amp;A等等。</p>
</blockquote>
<h2 id="结构"><a href="#结构" class="headerlink" title="结构"></a>结构</h2><blockquote>
<p>RNN结构如下图所示：</p>
</blockquote>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g8grd7diyzj30cq0jr74y.jpg"></p>
<blockquote>
<p>神经网络的模块A正在读取某个输入${x_t}$，并输出一个值${h_t}$，循环可以使得信息从当前步传递到下一步，将这个循环展开，如下所示。链式的特征揭示了 RNN 本质上是与序列和列表相关的，它们是对于这类数据的最自然的神经网络架构。</p>
</blockquote>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g8grdf83inj30jg054jrt.jpg"></p>
<h2 id="长期依赖问题"><a href="#长期依赖问题" class="headerlink" title="长期依赖问题"></a>长期依赖问题</h2><blockquote>
<p>RNN 的关键点之一就是他们可以用来连接先前的信息到当前的任务上，例如使用过去的视频段来推测对当前段的理解。如果 RNN 可以做到这个，他们就变得非常有用。但是真的可以么？答案是，还有很多依赖因素。有时候，我们仅仅需要知道先前的信息来执行当前的任务。例如，我们有一个语言模型用来基于先前的词来预测下一个词。如果我们试着预测 “the clouds are in the sky” 最后的词，我们并不需要任何其他的上下文 —— 因此下一个词很显然就应该是 sky。在这样的场景中，相关的信息和预测的词位置之间的间隔是非常小的，RNN 可以学会使用先前的信息。</p>
</blockquote>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g8gre48ytdj30jg08zwfc.jpg"></p>
<blockquote>
<p>但是同样会有一些更加复杂的场景。假设我们试着去预测“I grew up in France… I speak fluent French”最后的词。当前的信息建议下一个词可能是一种语言的名字，但是如果我们需要弄清楚是什么语言，我们是需要先前提到的离当前位置很远的 France 的上下文的。这说明相关信息和当前预测位置之间的间隔就肯定变得相当的大。不幸的是，在这个间隔不断增大时，RNN 会丧失学习到连接如此远的信息的能力。</p>
</blockquote>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g8gred9mstj30jg06p74y.jpg"></p>
<blockquote>
<p>在理论上，RNN 绝对可以处理这样的长期依赖问题。人们可以仔细挑选参数来解决这类问题中的最初级形式，但在实践中，RNN 肯定不能够成功学习到这些知识。Bengio, et al. (1994)等人对该问题进行了深入的研究，他们发现一些使训练 RNN 变得非常困难的相当根本的原因。然而，幸运的是，LSTM 并没有这个问题！</p>
</blockquote>
<h2 id="LSTM网络"><a href="#LSTM网络" class="headerlink" title="LSTM网络"></a>LSTM网络</h2><blockquote>
<p>Long Short Term 网络—— 一般就叫做 LSTM，是一种 RNN 特殊的类型，可以学习长期依赖信息。LSTM 由Hochreiter &amp; Schmidhuber (1997)提出，并在近期被Alex Graves进行了改良和推广。在很多问题，LSTM 都取得相当巨大的成功，并得到了广泛的使用。LSTM 通过刻意的设计来避免长期依赖问题。记住长期的信息在实践中是 LSTM 的默认行为，而非需要付出很大代价才能获得的能力！</p>
</blockquote>
<blockquote>
<p>所有 RNN 都具有一种重复神经网络模块的链式的形式。在标准的 RNN 中，这个重复的模块只有一个非常简单的结构，例如一个 tanh 层。</p>
</blockquote>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g8grepjvvhj30jg07a74r.jpg"></p>
<blockquote>
<p>LSTM 同样是这样的结构，但是重复的模块拥有一个不同的结构。不同于 单一神经网络层，这里是有四个，以一种非常特殊的方式进行交互。</p>
</blockquote>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g8grexgi2dj30jg07baar.jpg"></p>
<h2 id="LSTM网络详解"><a href="#LSTM网络详解" class="headerlink" title="LSTM网络详解"></a>LSTM网络详解</h2><blockquote>
<p>下面对LSTM网络进行详细说明，首先说明一下图中使用的图标，如下：</p>
</blockquote>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g8grfb9mm0j30jg03m3yr.jpg"></p>
<blockquote>
<p>在上面的图例中，每一条黑线传输着一整个向量，从一个节点的输出到其他节点的输入。粉色的圈代表按位 pointwise 的操作，诸如向量的和，而黄色的矩阵就是学习到的神经网络层。合在一起的线表示向量的连接，分开的线表示内容被复制，然后分发到不同的位置。</p>
</blockquote>
<blockquote>
<p>LSTM 的关键就是细胞状态cell state，水平线在图上方贯穿运行，也就是贯穿每个重复结构的上面这条flow。细胞状态类似于传送带，直接在整个链上运行，只有一些少量的线性交互。信息在上面流传保持不变会很容易。这条flow其实就承载着之前所有状态的信息，每当flow流经一个重复结构A的时候，都会有相应的操作来决定舍弃什么旧的信息以及添加什么新的信息。</p>
</blockquote>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g8grfmoxanj30jg0600sv.jpg"></p>
<blockquote>
<p>LSTM 有通过精心设计对信息增减进行控制的结构，称作为“门”。门是一种让信息选择式通过的方法。他们包含一个 sigmoid 神经网络层和一个按位的乘法操作。Sigmoid 层输出 0 到 1 之间的数值，描述每个部分有多少量可以通过。0 代表“不许任何量通过”，1 就指“允许任意量通过”！</p>
</blockquote>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g8grg142wbj305i06qwei.jpg"></p>
<blockquote>
<p>LSTM 拥有三个门，来保护和控制细胞状态，分别是遗忘门 (forget gate)、输入门 (input gate)、输出门 (output gate)。下面对这三个门进行详细讲解</p>
</blockquote>
<h3 id="遗忘门-forget-gate"><a href="#遗忘门-forget-gate" class="headerlink" title="遗忘门 (forget gate)"></a>遗忘门 (forget gate)</h3><blockquote>
<p>遗忘门决定了要从cell state中舍弃什么信息。其通过输入上一状态的输出ht-1、当前状态输入信息Xt到一个Sigmoid函数中，产生一个介于0到1之间的数值，与cell state相乘之后来确定舍弃（保留）多少信息。0 表示“完全舍弃”，1 表示“完全保留”。</p>
</blockquote>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g8grgdk95kj30jg060aaa.jpg"></p>
<blockquote>
<p>上式中，<img src="https://latex.codecogs.com/svg.latex?\dpi{300}&space;W_{f}" title="W_{f}" /><br>是遗忘门的权重矩阵，<img src="https://latex.codecogs.com/svg.latex?\dpi{300}&space;[h_{t-1},x_{t}]" title="[h_{t-1},x_{t}]" />表示把两个向量连接成一个更长的向量，<img src="https://latex.codecogs.com/svg.latex?\dpi{300}&space;b_{f}" title="b_{f}" />是遗忘门的偏置项，<img src="https://latex.codecogs.com/svg.latex?\dpi{300}&space;\sigma" title="\sigma" />是sigmoid函数。如果输入的维度是<img src="https://latex.codecogs.com/svg.latex?\dpi{300}&space;d_{x}" title="d_{x}" />，隐藏层的维度是<img src="https://latex.codecogs.com/svg.latex?\dpi{300}&space;d_{h}" title="d_{h}" />，单元状态的维度是<img src="https://latex.codecogs.com/svg.latex?\dpi{300}&space;d_{c}" title="d_{c}" /> （通常 <img src="https://latex.codecogs.com/svg.latex?\dpi{300}&space;d_{c}=d_{h}" title="\d_{c}=d_{h}" />），则遗忘门的权重矩阵<img src="https://latex.codecogs.com/svg.latex?\dpi{300}&space;W_{f}" title="W_{f}" />维度是<img src="https://latex.codecogs.com/svg.latex?\dpi{300}&space;d_{c}&space;\times&space;(d_{h}&plus;d_{x})" title="d_{c} \times (d_{h}+d_{x})" />。事实上，权重矩阵<img src="https://latex.codecogs.com/svg.latex?\dpi{300}&space;W_{f}" title="W_{f}" />都是两个矩阵拼接而成的：一个是<img src="https://latex.codecogs.com/svg.latex?\dpi{300}&space;W_{fh}" title="W_{fh}" />，它对应着输入项<img src="https://latex.codecogs.com/svg.latex?\dpi{300}&space;h_{t-1}" title="h_{t-1}" />，其维度为<img src="https://latex.codecogs.com/svg.latex?\dpi{300}&space;d_{c}\times&space;d_{h}" title="d_{c}\times d_{h}" />；一个是<img src="https://latex.codecogs.com/svg.latex?\dpi{300}&space;W_{fx}" title="W_{fx}" />，它对应着输入项<img src="https://latex.codecogs.com/svg.latex?\dpi{300}&space;x_{t}" title="x_{t}" />，其维度为<img src="https://latex.codecogs.com/svg.latex?\dpi{300}&space;d_{c}\times&space;d_{x}" title="d_{c}\times d_{x}" />。<img src="https://latex.codecogs.com/svg.latex?\dpi{300}&space;W_{f}" title="W_{f}" />可以写为：<br><img src="https://latex.codecogs.com/svg.latex?\dpi{300}&space;[W_{f}]\begin{bmatrix}&space;h_{t-1}\\&space;x_{t}&space;\end{bmatrix}=\begin{bmatrix}&space;W_{fh}&space;&&space;W_{fx}&space;\end{bmatrix}\begin{bmatrix}&space;h_{t-1}\\&space;x_{t}&space;\end{bmatrix}=W_{fh}h_{t-1}&plus;W_{fx}x_{t}" title="[W_{f}]\begin{bmatrix} h_{t-1}\\ x_{t} \end{bmatrix}=\begin{bmatrix} W_{fh} & W_{fx} \end{bmatrix}\begin{bmatrix} h_{t-1}\\ x_{t} \end{bmatrix}=W_{fh}h_{t-1}+W_{fx}x_{t}" /></p>
</blockquote>
<h3 id="输入门-input-gate"><a href="#输入门-input-gate" class="headerlink" title="输入门 (input gate)"></a>输入门 (input gate)</h3><blockquote>
<p>输入门决定了要往cell state中保存什么新的信息。其通过输入上一状态的输出<img src="https://latex.codecogs.com/svg.latex?\dpi{300}&space;h_{t-1}" title="h_{t-1}" />、当前状态输入信息<img src="https://latex.codecogs.com/svg.latex?\dpi{300}&space;x_{t}" title="x_{t}" />到一个Sigmoid函数中，产生一个介于0到1之间的数值<img src="https://latex.codecogs.com/svg.latex?\dpi{300}&space;i_{t}" title="i_{t}" />来确定我们需要保留多少的新信息。同时，一个tanh层会通过上一状态的输出<img src="https://latex.codecogs.com/svg.latex?\dpi{300}&space;h_{t-1}" title="h_{t-1}" />、当前状态输入信息<img src="https://latex.codecogs.com/svg.latex?\dpi{300}&space;x_{t}" title="x_{t}" />来得到一个将要加入到cell state中的“候选新信息”<img src="https://latex.codecogs.com/svg.latex?\dpi{300}&space;\widetilde{C_{t}}" title="\widetilde{C_{t}}" />。<br><img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g8grhmz9f1j30yg0anzl9.jpg"></p>
</blockquote>
<p>现在计算当前时刻的单元状态。它是由上一次的单元状态按元素乘以遗忘门，丢弃掉我们确定需要丢弃的信息；然后把当前输入的单元状态按元素乘以输入门，将两个积加和，这就是新的候选值：</p>
<img src="https://latex.codecogs.com/svg.latex?\dpi{300}&space;C_{t}=f_{t}*C_{t-1}&plus;i_{t}*\widetilde{C_{t}}" title="C_{t}=f_{t}*C_{t-1}+i_{t}*\widetilde{C_{t}}" />

<p><img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g8grirv3emj30yg0ant9g.jpg"></p>
<h3 id="输出门-output-gate"><a href="#输出门-output-gate" class="headerlink" title="输出门 (output gate)"></a>输出门 (output gate)</h3><blockquote>
<p>输出门决定了要从cell state中输出什么信息。这个输出将会基于我们的细胞状态，但是也是一个过滤后的版本，会先有一个Sigmoid函数产生一个介于0到1之间的数值<img src="https://latex.codecogs.com/svg.latex?\dpi{300}&space;o_{t}" title="o_{t}" />来确定我们需要输出多少cell state中的信息。cell state的信息再与<img src="https://latex.codecogs.com/svg.latex?\dpi{300}&space;o_{t}" title="o_{t}" />相乘时首先会经过一个tanh层进行“激活”（非线性变换）。得到的就是这个LSTM block的输出信息<img src="https://latex.codecogs.com/svg.latex?\dpi{300}&space;h_{t}" title="h_{t}" />。</p>
</blockquote>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g8grizvmnzj30jg0600t2.jpg"></p>
<h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><blockquote>
<p>采用单层LSTM实现MNIST分类判别，MNIST的输入为影像，影像的行排列需要有一定的顺序，如果胡乱排列，则无法判断数字，因此可以将此问题看作是RNN。设置时间序列长度为28，每次输入影像的一行（28个维度），batch size为128，隐层结点数为128，代码如下：</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">########## load packages ##########</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment">##################### load data ##########################</span></span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line">mnist=input_data.read_data_sets(<span class="string">&quot;mnist_sets&quot;</span>,one_hot=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">########## set net hyperparameters ##########</span></span><br><span class="line">learning_rate=<span class="number">0.001</span></span><br><span class="line"></span><br><span class="line">epochs=<span class="number">1</span></span><br><span class="line">batch_size=<span class="number">128</span></span><br><span class="line"></span><br><span class="line">display_step=<span class="number">20</span></span><br><span class="line"></span><br><span class="line"><span class="comment">########## set net parameters ##########</span></span><br><span class="line">n_inputs = <span class="number">28</span>   <span class="comment"># 输入向量的维度，每个时刻的输入特征是28维的，就是每个时刻输入一行，一行有 28 个像素</span></span><br><span class="line">n_steps = <span class="number">28</span>    <span class="comment"># 循环层长度，即时序持续长度为28，即每做一次预测，需要先输入28行</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#### 0-9 digits ####</span></span><br><span class="line">n_classes=<span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#### neurons in hidden layer 隐含层的结点数 ####</span></span><br><span class="line">n_hidden_units=<span class="number">128</span></span><br><span class="line"></span><br><span class="line"><span class="comment">########## placeholder ##########</span></span><br><span class="line">x=tf.placeholder(tf.float32,[<span class="literal">None</span>, n_steps, n_inputs])</span><br><span class="line">y=tf.placeholder(tf.float32,[<span class="literal">None</span>, n_classes])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">######### Define weights and biases #########</span></span><br><span class="line"><span class="comment"># in:每个cell输入的全连接层参数</span></span><br><span class="line"><span class="comment"># out:定义用于输出的全连接层参数</span></span><br><span class="line">weights = &#123;</span><br><span class="line">    <span class="comment"># (28, 128)</span></span><br><span class="line">    <span class="string">&#x27;in&#x27;</span>: tf.Variable(tf.random_normal([n_inputs, n_hidden_units])),</span><br><span class="line">    <span class="comment"># (128, 10)</span></span><br><span class="line">    <span class="string">&#x27;out&#x27;</span>: tf.Variable(tf.random_normal([n_hidden_units, n_classes]))</span><br><span class="line">&#125;</span><br><span class="line">biases = &#123;</span><br><span class="line">    <span class="comment"># (128, )</span></span><br><span class="line">    <span class="string">&#x27;in&#x27;</span>: tf.Variable(tf.constant(<span class="number">0.1</span>, shape=[n_hidden_units, ])),</span><br><span class="line">    <span class="comment"># (10, )</span></span><br><span class="line">    <span class="string">&#x27;out&#x27;</span>: tf.Variable(tf.constant(<span class="number">0.1</span>, shape=[n_classes, ]))</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">##################### build net model ##########################</span></span><br><span class="line"><span class="comment">##### RNN LSTM 单层LSTM #######</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">RNN</span>(<span class="params">x, weights, biases</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># hidden layer for input to cell</span></span><br><span class="line">    <span class="comment"># x (128 batch,28 steps,28 inputs) ==&gt; (128 batch * 28 steps, 28 inputs)</span></span><br><span class="line">    x=tf.reshape(x,shape=[-<span class="number">1</span>, n_inputs])</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># into hidden</span></span><br><span class="line">    <span class="comment"># x_in =[128 bach*28 steps,28 inputs]*[28 inputs,128 hidden_units]=[128 batch * 28 steps, 128 hidden]</span></span><br><span class="line">    x_in = tf.matmul(x, weights[<span class="string">&#x27;in&#x27;</span>]) + biases[<span class="string">&#x27;in&#x27;</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># x_in ==&gt; (128 batch, 28 steps, 128 hidden)</span></span><br><span class="line">    x_in = tf.reshape(x_in, [-<span class="number">1</span>, n_steps, n_hidden_units])</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">    <span class="comment"># cell</span></span><br><span class="line">    <span class="comment"># basic LSTM Cell.初始的forget_bias=1,不希望遗忘任何信息</span></span><br><span class="line">    cell = tf.contrib.rnn.BasicLSTMCell(n_hidden_units,forget_bias=<span class="number">1.0</span>,state_is_tuple=<span class="literal">True</span>)</span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">    <span class="comment"># lstm cell is divided into two parts (c_state, h_state)</span></span><br><span class="line">    init_state = cell.zero_state(batch_size, dtype=tf.float32)</span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">    <span class="comment"># dynamic_rnn receive Tensor (batch, steps, inputs) or (steps, batch, inputs) as x_in.</span></span><br><span class="line">    <span class="comment"># n_steps位于次要维度 time_major=False   outputs shape 128, 28, 128</span></span><br><span class="line">    outputs, final_state = tf.nn.dynamic_rnn(cell, x_in, initial_state=init_state, time_major=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line">    <span class="comment"># hidden layer for output as the final results</span></span><br><span class="line">    <span class="comment"># unpack to list [(batch, outputs)..] * steps   </span></span><br><span class="line">    <span class="comment"># steps即时间序列长度，此时输出28个ht，由于输入的是 batch steps inputs，因此需要对outputs做调整，从而取到最后一个ht</span></span><br><span class="line">    <span class="comment"># permute time_step_size and batch_size  outputs shape from [128, 28, 128] to [28, 128, 128]</span></span><br><span class="line">    outputs = tf.unstack(tf.transpose(outputs, [<span class="number">1</span>,<span class="number">0</span>,<span class="number">2</span>]))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">#选择最后一个output与输出的全连接weights相乘再加上biases</span></span><br><span class="line">    results = tf.matmul(outputs[-<span class="number">1</span>], weights[<span class="string">&#x27;out&#x27;</span>]) + biases[<span class="string">&#x27;out&#x27;</span>]    <span class="comment"># shape = (128, 10)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> results</span><br><span class="line"></span><br><span class="line"><span class="comment">########## define model, loss and optimizer ##########</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#### model pred 影像判断结果 ####</span></span><br><span class="line">pred=RNN(x,weights,biases)</span><br><span class="line"></span><br><span class="line"><span class="comment">#### loss 损失计算 ####</span></span><br><span class="line">cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))</span><br><span class="line"></span><br><span class="line"><span class="comment">#### optimization 优化 ####</span></span><br><span class="line">optimizer=tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)</span><br><span class="line"></span><br><span class="line"><span class="comment">#### accuracy 准确率 ####</span></span><br><span class="line">correct_pred=tf.equal(tf.argmax(pred,<span class="number">1</span>),tf.argmax(y,<span class="number">1</span>))</span><br><span class="line">accuracy=tf.reduce_mean(tf.cast(correct_pred,tf.float32))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">##################### train and evaluate model ##########################</span></span><br><span class="line"></span><br><span class="line"><span class="comment">########## initialize variables ##########</span></span><br><span class="line">init=tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    step=<span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#### epoch 世代循环 ####</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs+<span class="number">1</span>):</span><br><span class="line"></span><br><span class="line">        <span class="comment">#### iteration ####</span></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(mnist.train.num_examples//batch_size):</span><br><span class="line"></span><br><span class="line">            step += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            <span class="comment">##### get x,y #####</span></span><br><span class="line">            batch_x, batch_y=mnist.train.next_batch(batch_size)</span><br><span class="line"></span><br><span class="line">            batch_x = batch_x.reshape([batch_size, n_steps, n_inputs])</span><br><span class="line"></span><br><span class="line">            <span class="comment">##### optimizer ####</span></span><br><span class="line">            sess.run(optimizer,feed_dict=&#123;x:batch_x, y:batch_y&#125;)</span><br><span class="line"></span><br><span class="line">            </span><br><span class="line">            <span class="comment">##### show loss and acc ##### </span></span><br><span class="line">            <span class="keyword">if</span> step % display_step==<span class="number">0</span>:</span><br><span class="line">                loss,acc=sess.run([cost, accuracy],feed_dict=&#123;x: batch_x, y: batch_y&#125;)</span><br><span class="line">                print(<span class="string">&quot;Epoch &quot;</span>+ <span class="built_in">str</span>(epoch) + <span class="string">&quot;, Minibatch Loss=&quot;</span> + \</span><br><span class="line">                    <span class="string">&quot;&#123;:.6f&#125;&quot;</span>.<span class="built_in">format</span>(loss) + <span class="string">&quot;, Training Accuracy= &quot;</span>+ \</span><br><span class="line">                    <span class="string">&quot;&#123;:.5f&#125;&quot;</span>.<span class="built_in">format</span>(acc))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    print(<span class="string">&quot;Optimizer Finished!&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">##### test accuracy #####</span></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(mnist.test.num_examples//batch_size):</span><br><span class="line">        batch_x,batch_y=mnist.test.next_batch(batch_size)</span><br><span class="line">        batch_x = batch_x.reshape([batch_size, n_steps, n_inputs])</span><br><span class="line">        print(<span class="string">&quot;Testing Accuracy:&quot;</span>, sess.run(accuracy, feed_dict=&#123;x: batch_x, y: batch_y&#125;))</span><br></pre></td></tr></table></figure>
<h2 id="LSTM文本分类"><a href="#LSTM文本分类" class="headerlink" title="LSTM文本分类"></a>LSTM文本分类</h2><blockquote>
<p>LSTM由于其设计的特点，非常适合用于对时序数据的建模，如文本数据。将词的表示组合成句子的表示，可以采用相加的方法，即将所有词的表示进行加和，或者取平均等方法，但是这些方法没有考虑到词语在句子中前后顺序。如句子“我不觉得他好”。“不”字是对后面“好”的否定，即该句子的情感极性是贬义。使用LSTM模型可以更好的捕捉到较长距离的依赖关系。因为LSTM通过训练过程可以学到记忆哪些信息和遗忘哪些信息。</p>
<p>基于Keras框架，采用LSTM实现文本分类。文本采用imdb影评分类语料，共25,000条影评，label标记为正面/负面两种评价。影评已被预处理为词下标构成的序列。方便起见，单词的下标基于它在数据集中出现的频率标定，例如整数3所编码的词为数据集中第3常出现的词。这样的组织方法使得用户可以快速完成诸如“只考虑最常出现的10,000个词，但不考虑最常出现的20个词”这样的操作。词向量没有采用预训练好的向量，训练中生成，采用的网络结构如图所示：</p>
</blockquote>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g8grjmb6bwj319p0u0n1j.jpg"></p>
<blockquote>
<p>具体代码如下：</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">###################### load packages ####################</span></span><br><span class="line"><span class="keyword">from</span> keras.datasets <span class="keyword">import</span> imdb</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> preprocessing</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense, Embedding, Dropout, LSTM</span><br><span class="line"><span class="keyword">from</span> keras.utils.np_utils <span class="keyword">import</span> to_categorical</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">###################### load data ####################</span></span><br><span class="line"><span class="comment">######### 只考虑最常见的1000个词 ########</span></span><br><span class="line">num_words = <span class="number">1000</span></span><br><span class="line"></span><br><span class="line"><span class="comment">######### 导入数据 #########</span></span><br><span class="line">(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=num_words)</span><br><span class="line"></span><br><span class="line">print(x_train.shape)</span><br><span class="line">print(x_train[<span class="number">0</span>][:<span class="number">5</span>])</span><br><span class="line"></span><br><span class="line">print(y_train.shape)</span><br><span class="line">print(y_train[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">###################### preprocess data ####################</span></span><br><span class="line"><span class="comment">######## 句子长度最长设置为20 ########</span></span><br><span class="line">max_len = <span class="number">20</span></span><br><span class="line"></span><br><span class="line"><span class="comment">######## 对文本进行填充，将文本转成相同长度 ########</span></span><br><span class="line">x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=max_len)</span><br><span class="line">x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=max_len)</span><br><span class="line"></span><br><span class="line">print(x_train.shape)</span><br><span class="line">print(x_train[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">######## 对label做one-hot处理 ########</span></span><br><span class="line">num_class = <span class="number">2</span></span><br><span class="line">y_train = to_categorical(y_train, num_class)</span><br><span class="line">y_test = to_categorical(y_test, num_class)</span><br><span class="line"></span><br><span class="line">print(y_train.shape)</span><br><span class="line">print(y_train[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">###################### build network ####################</span></span><br><span class="line"><span class="comment">######## word dim 词向量维度 ########</span></span><br><span class="line">word_dim = <span class="number">8</span></span><br><span class="line"></span><br><span class="line"><span class="comment">######## network structure ########</span></span><br><span class="line">model = Sequential()</span><br><span class="line"></span><br><span class="line"><span class="comment">#### Embedding层 ####</span></span><br><span class="line">model.add(Embedding(input_dim=<span class="number">1000</span>, output_dim=word_dim, input_length=max_len))</span><br><span class="line"></span><br><span class="line"><span class="comment">#### 两层LSTM，第一层，设置return_sequences参数为True ####</span></span><br><span class="line">model.add(LSTM(<span class="number">256</span>, return_sequences=<span class="literal">True</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">#### dropout ####</span></span><br><span class="line">model.add(Dropout(<span class="number">0.5</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">#### 两层LSTM，第二层，设置return_sequences参数为False ####</span></span><br><span class="line">model.add(LSTM(<span class="number">256</span>, return_sequences=<span class="literal">False</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">#### dropout ####</span></span><br><span class="line">model.add(Dropout(<span class="number">0.5</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">#### 输出层 ####</span></span><br><span class="line">model.add(Dense(num_class, activation=<span class="string">&#x27;softmax&#x27;</span>))</span><br><span class="line"></span><br><span class="line">print(model.summary())</span><br><span class="line"></span><br><span class="line"><span class="comment">######## optimization and train ########</span></span><br><span class="line">model.<span class="built_in">compile</span>(loss=<span class="string">&#x27;categorical_crossentropy&#x27;</span>, optimizer=<span class="string">&#x27;adam&#x27;</span>, metrics=[<span class="string">&#x27;acc&#x27;</span>])</span><br><span class="line">model.fit(x_train, y_train, batch_size=<span class="number">512</span>, epochs=<span class="number">20</span>, verbose=<span class="number">1</span>, validation_data=(x_test, y_test))</span><br></pre></td></tr></table></figure>
<blockquote>
<p>运行结果如下：</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line">(25000,)</span><br><span class="line">[1, 14, 22, 16, 43]</span><br><span class="line">(25000,)</span><br><span class="line">1</span><br><span class="line">(25000, 20)</span><br><span class="line">[ 65  16  38   2  88  12  16 283   5  16   2 113 103  32  15  16   2  19</span><br><span class="line"> 178  32]</span><br><span class="line">(25000, 2)</span><br><span class="line">[0. 1.]</span><br><span class="line">_________________________________________________________________</span><br><span class="line">Layer (type)                 Output Shape              Param #   </span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">embedding_33 (Embedding)     (None, 20, 8)             8000      </span><br><span class="line">_________________________________________________________________</span><br><span class="line">lstm_27 (LSTM)               (None, 20, 256)           271360    </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dropout_5 (Dropout)          (None, 20, 256)           0         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">lstm_28 (LSTM)               (None, 256)               525312    </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dropout_6 (Dropout)          (None, 256)               0         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dense_27 (Dense)             (None, 2)                 514       </span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">Total params: 805,186</span><br><span class="line">Trainable params: 805,186</span><br><span class="line">Non-trainable params: 0</span><br><span class="line">_________________________________________________________________</span><br><span class="line">None</span><br><span class="line">Train on 25000 samples, validate on 25000 samples</span><br><span class="line">Epoch 1&#x2F;20</span><br><span class="line">25000&#x2F;25000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 47s 2ms&#x2F;step - loss: 0.6618 - acc: 0.5817 - val_loss: 0.5914 - val_acc: 0.6820</span><br><span class="line">Epoch 2&#x2F;20</span><br><span class="line">25000&#x2F;25000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 40s 2ms&#x2F;step - loss: 0.5493 - acc: 0.7170 - val_loss: 0.5316 - val_acc: 0.7281</span><br><span class="line">Epoch 3&#x2F;20</span><br><span class="line">25000&#x2F;25000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 41s 2ms&#x2F;step - loss: 0.5085 - acc: 0.7484 - val_loss: 0.5245 - val_acc: 0.7322</span><br><span class="line">Epoch 4&#x2F;20</span><br><span class="line">25000&#x2F;25000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 40s 2ms&#x2F;step - loss: 0.5012 - acc: 0.7548 - val_loss: 0.5160 - val_acc: 0.7381</span><br><span class="line">Epoch 5&#x2F;20</span><br><span class="line">25000&#x2F;25000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 40s 2ms&#x2F;step - loss: 0.4946 - acc: 0.7559 - val_loss: 0.5165 - val_acc: 0.7384</span><br><span class="line">Epoch 6&#x2F;20</span><br><span class="line">25000&#x2F;25000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 41s 2ms&#x2F;step - loss: 0.4924 - acc: 0.7577 - val_loss: 0.5166 - val_acc: 0.7388</span><br><span class="line">Epoch 7&#x2F;20</span><br><span class="line">25000&#x2F;25000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 40s 2ms&#x2F;step - loss: 0.4867 - acc: 0.7596 - val_loss: 0.5264 - val_acc: 0.7292</span><br><span class="line">Epoch 8&#x2F;20</span><br><span class="line">25000&#x2F;25000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 40s 2ms&#x2F;step - loss: 0.4851 - acc: 0.7614 - val_loss: 0.5262 - val_acc: 0.7400</span><br><span class="line">Epoch 9&#x2F;20</span><br><span class="line">25000&#x2F;25000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 40s 2ms&#x2F;step - loss: 0.4803 - acc: 0.7643 - val_loss: 0.5250 - val_acc: 0.7392</span><br><span class="line">Epoch 10&#x2F;20</span><br><span class="line">25000&#x2F;25000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 40s 2ms&#x2F;step - loss: 0.4774 - acc: 0.7651 - val_loss: 0.5220 - val_acc: 0.7376</span><br><span class="line">Epoch 11&#x2F;20</span><br><span class="line">25000&#x2F;25000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 40s 2ms&#x2F;step - loss: 0.4729 - acc: 0.7696 - val_loss: 0.5225 - val_acc: 0.7365</span><br><span class="line">Epoch 12&#x2F;20</span><br><span class="line">25000&#x2F;25000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 39s 2ms&#x2F;step - loss: 0.4704 - acc: 0.7698 - val_loss: 0.5279 - val_acc: 0.7385</span><br><span class="line">Epoch 13&#x2F;20</span><br><span class="line">25000&#x2F;25000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 46s 2ms&#x2F;step - loss: 0.4682 - acc: 0.7713 - val_loss: 0.5303 - val_acc: 0.7343</span><br><span class="line">Epoch 14&#x2F;20</span><br><span class="line">25000&#x2F;25000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 44s 2ms&#x2F;step - loss: 0.4683 - acc: 0.7729 - val_loss: 0.5297 - val_acc: 0.7325</span><br><span class="line">Epoch 15&#x2F;20</span><br><span class="line">25000&#x2F;25000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 43s 2ms&#x2F;step - loss: 0.4659 - acc: 0.7739 - val_loss: 0.5402 - val_acc: 0.7331</span><br><span class="line">Epoch 16&#x2F;20</span><br><span class="line">25000&#x2F;25000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 41s 2ms&#x2F;step - loss: 0.4587 - acc: 0.7759 - val_loss: 0.5350 - val_acc: 0.7312</span><br><span class="line">Epoch 17&#x2F;20</span><br><span class="line">25000&#x2F;25000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 42s 2ms&#x2F;step - loss: 0.4577 - acc: 0.7771 - val_loss: 0.5488 - val_acc: 0.7334</span><br><span class="line">Epoch 18&#x2F;20</span><br><span class="line">25000&#x2F;25000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 42s 2ms&#x2F;step - loss: 0.4524 - acc: 0.7796 - val_loss: 0.5356 - val_acc: 0.7284</span><br><span class="line">Epoch 19&#x2F;20</span><br><span class="line">25000&#x2F;25000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 40s 2ms&#x2F;step - loss: 0.4492 - acc: 0.7829 - val_loss: 0.5357 - val_acc: 0.7332</span><br><span class="line">Epoch 20&#x2F;20</span><br><span class="line">25000&#x2F;25000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 40s 2ms&#x2F;step - loss: 0.4472 - acc: 0.7840 - val_loss: 0.5532 - val_acc: 0.7216</span><br></pre></td></tr></table></figure>
<h2 id="双向RNN"><a href="#双向RNN" class="headerlink" title="双向RNN"></a>双向RNN</h2><h3 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h3><blockquote>
<p>但是利用LSTM对句子进行建模还存在一个问题：无法编码从后到前的信息。在更细粒度的分类时，如对于强程度的褒义、弱程度的褒义、中性、弱程度的贬义、强程度的贬义的五分类任务需要注意情感词、程度词、否定词之间的交互。举一个例子，“这个餐厅脏得不行，没有隔壁好”，这里的“不行”是对“脏”的程度的一种修饰，通过BiLSTM可以更好的捕捉双向的语义依赖。BiLSTM是Bi-directional Long Short-Term Memory的缩写，是由前向LSTM与后向LSTM组合而成。比如，我们对“我爱中国”这句话进行编码，模型如图所示：</p>
</blockquote>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g8grkc3cguj30o90f53zv.jpg"></p>
<blockquote>
<p>前向<img src="https://latex.codecogs.com/svg.latex?LSTM_{L}" title="LSTM_{L}" />依次输入“我”，“爱”，“中国”得到三个向量<img src="https://latex.codecogs.com/svg.latex?\{h_{L0},&space;h_{L1},&space;h_{L2}\}" title="\{h_{L0}, h_{L1}, h_{L2}\}" />，后向<img src="https://latex.codecogs.com/svg.latex?LSTM_{R}" title="LSTM_{R}" />依次输入“中国”，“爱”，“我”得到三个向量<img src="https://latex.codecogs.com/svg.latex?\{h_{R0},&space;h_{R1},&space;h_{R2}\}" title="\{h_{R0}, h_{R1}, h_{R2}\}" />。最后将前向和后向的隐向量进行拼接得到<img src="https://latex.codecogs.com/svg.latex?\{[h_{L0},&space;h_{R2}],&space;[h_{L1},&space;h_{R1}],&space;[h_{L2},&space;h_{R0}]\}" title="\{[h_{L0}, h_{R2}], [h_{L1}, h_{R1}], [h_{L2}, h_{R0}]\}" />，即<img src="https://latex.codecogs.com/svg.latex?\{h_{0},&space;h_{1},&space;h_{2}\}" title="\{h_{0}, h_{1}, h_{2}\}" />。对于情感分类任务来说，采用的句子的表示往往是<img src="https://latex.codecogs.com/svg.latex?[h_{L0},&space;h_{R2}]" title="[h_{L0}, h_{R2}]" />，因为其包含了前向与后向的所有信息，如图所示：</p>
</blockquote>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g8grkjrwirj30r80f6q4a.jpg"></p>
<h3 id="Tensorflow的Bi-RNN实现"><a href="#Tensorflow的Bi-RNN实现" class="headerlink" title="Tensorflow的Bi-RNN实现"></a>Tensorflow的Bi-RNN实现</h3><h4 id="tensorflow的Bi-RNN代码"><a href="#tensorflow的Bi-RNN代码" class="headerlink" title="tensorflow的Bi-RNN代码"></a>tensorflow的Bi-RNN代码</h4><blockquote>
<p>tensorflow的Bi-RNN代码如下：</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bidirectional_dynamic_rnn</span>(<span class="params">cell_fw, cell_bw, inputs, sequence_length=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                              initial_state_fw=<span class="literal">None</span>, initial_state_bw=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                              dtype=<span class="literal">None</span>, parallel_iterations=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                              swap_memory=<span class="literal">False</span>, time_major=<span class="literal">False</span>, scope=<span class="literal">None</span></span>):</span></span><br><span class="line">  <span class="keyword">if</span> <span class="keyword">not</span> _like_rnncell(cell_fw):</span><br><span class="line">    <span class="keyword">raise</span> TypeError(<span class="string">&quot;cell_fw must be an instance of RNNCell&quot;</span>)</span><br><span class="line">  <span class="keyword">if</span> <span class="keyword">not</span> _like_rnncell(cell_bw):</span><br><span class="line">    <span class="keyword">raise</span> TypeError(<span class="string">&quot;cell_bw must be an instance of RNNCell&quot;</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">with</span> vs.variable_scope(scope <span class="keyword">or</span> <span class="string">&quot;bidirectional_rnn&quot;</span>):</span><br><span class="line">    <span class="comment"># Forward direction</span></span><br><span class="line">    <span class="keyword">with</span> vs.variable_scope(<span class="string">&quot;fw&quot;</span>) <span class="keyword">as</span> fw_scope:</span><br><span class="line">      output_fw, output_state_fw = dynamic_rnn(</span><br><span class="line">          cell=cell_fw, inputs=inputs, sequence_length=sequence_length,</span><br><span class="line">          initial_state=initial_state_fw, dtype=dtype,</span><br><span class="line">          parallel_iterations=parallel_iterations, swap_memory=swap_memory,</span><br><span class="line">          time_major=time_major, scope=fw_scope)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Backward direction</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> time_major:</span><br><span class="line">      time_dim = <span class="number">1</span></span><br><span class="line">      batch_dim = <span class="number">0</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      time_dim = <span class="number">0</span></span><br><span class="line">      batch_dim = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_reverse</span>(<span class="params">input_, seq_lengths, seq_dim, batch_dim</span>):</span></span><br><span class="line">      <span class="keyword">if</span> seq_lengths <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">return</span> array_ops.reverse_sequence(</span><br><span class="line">            <span class="built_in">input</span>=input_, seq_lengths=seq_lengths,</span><br><span class="line">            seq_dim=seq_dim, batch_dim=batch_dim)</span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> array_ops.reverse(input_, axis=[seq_dim])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> vs.variable_scope(<span class="string">&quot;bw&quot;</span>) <span class="keyword">as</span> bw_scope:</span><br><span class="line">      inputs_reverse = _reverse(</span><br><span class="line">          inputs, seq_lengths=sequence_length,</span><br><span class="line">          seq_dim=time_dim, batch_dim=batch_dim)</span><br><span class="line">      tmp, output_state_bw = dynamic_rnn(</span><br><span class="line">          cell=cell_bw, inputs=inputs_reverse, sequence_length=sequence_length,</span><br><span class="line">          initial_state=initial_state_bw, dtype=dtype,</span><br><span class="line">          parallel_iterations=parallel_iterations, swap_memory=swap_memory,</span><br><span class="line">          time_major=time_major, scope=bw_scope)</span><br><span class="line"></span><br><span class="line">  output_bw = _reverse(</span><br><span class="line">      tmp, seq_lengths=sequence_length,</span><br><span class="line">      seq_dim=time_dim, batch_dim=batch_dim)</span><br><span class="line"></span><br><span class="line">  outputs = (output_fw, output_bw)</span><br><span class="line">  output_states = (output_state_fw, output_state_bw)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> (outputs, output_states)</span><br></pre></td></tr></table></figure>
<h4 id="代码解读"><a href="#代码解读" class="headerlink" title="代码解读"></a>代码解读</h4><h5 id="前向输入"><a href="#前向输入" class="headerlink" title="前向输入"></a>前向输入</h5><blockquote>
<p>首先是对输入数据inputs，调用dynamic_rnn从前往后跑一下，得到output_fw和output_state_fw，其中output_fw是所有inputs的LSTM输出状态，output_state_fw是最终的输出状态，</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> vs.variable_scope(scope <span class="keyword">or</span> <span class="string">&quot;bidirectional_rnn&quot;</span>):</span><br><span class="line">    <span class="comment"># Forward direction</span></span><br><span class="line">    <span class="keyword">with</span> vs.variable_scope(<span class="string">&quot;fw&quot;</span>) <span class="keyword">as</span> fw_scope:</span><br><span class="line">      output_fw, output_state_fw = dynamic_rnn(</span><br><span class="line">          cell=cell_fw, inputs=inputs, sequence_length=sequence_length,</span><br><span class="line">          initial_state=initial_state_fw, dtype=dtype,</span><br><span class="line">          parallel_iterations=parallel_iterations, swap_memory=swap_memory,</span><br><span class="line">          time_major=time_major, scope=fw_scope)</span><br></pre></td></tr></table></figure>
<h5 id="反向输入"><a href="#反向输入" class="headerlink" title="反向输入"></a>反向输入</h5><blockquote>
<p>定义一个局部函数：把输入的input_ 按照长度为seq_lengths 调用array_ops.rerverse_sequence 做一次转置：</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_reverse</span>(<span class="params">input_, seq_lengths, seq_dim, batch_dim</span>):</span></span><br><span class="line">      <span class="keyword">if</span> seq_lengths <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">return</span> array_ops.reverse_sequence(</span><br><span class="line">            <span class="built_in">input</span>=input_, seq_lengths=seq_lengths,</span><br><span class="line">            seq_dim=seq_dim, batch_dim=batch_dim)</span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> array_ops.reverse(input_, axis=[seq_dim])</span><br></pre></td></tr></table></figure>
<blockquote>
<p>之后把inputs转置成inputs_reverse，然后对这个inputs_reverse跑一下dynamic_rnn得到tmp和output_state_bw：</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> vs.variable_scope(<span class="string">&quot;bw&quot;</span>) <span class="keyword">as</span> bw_scope:</span><br><span class="line">      inputs_reverse = _reverse(</span><br><span class="line">          inputs, seq_lengths=sequence_length,</span><br><span class="line">          seq_dim=time_dim, batch_dim=batch_dim)</span><br><span class="line">      tmp, output_state_bw = dynamic_rnn(</span><br><span class="line">          cell=cell_bw, inputs=inputs_reverse, sequence_length=sequence_length,</span><br><span class="line">          initial_state=initial_state_bw, dtype=dtype,</span><br><span class="line">          parallel_iterations=parallel_iterations, swap_memory=swap_memory,</span><br><span class="line">          time_major=time_major, scope=bw_scope)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>再把这个输出tmp反转一下得到Output_bw向量：</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">output_bw = _reverse(</span><br><span class="line">      tmp, seq_lengths=sequence_length,</span><br><span class="line">      seq_dim=time_dim, batch_dim=batch_dim)</span><br></pre></td></tr></table></figure>
<h5 id="前向和反向的LSTM输出堆叠"><a href="#前向和反向的LSTM输出堆叠" class="headerlink" title="前向和反向的LSTM输出堆叠"></a>前向和反向的LSTM输出堆叠</h5><blockquote>
<p>output_fw和output_bw堆叠在一起得到bi-rnn的输出，隐藏层状态output_state_fw和output_state_bw堆叠在一起得到bi-rnn的隐藏层状态，最终输出：</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">outputs = (output_fw, output_bw)</span><br><span class="line">output_states = (output_state_fw, output_state_bw)</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> (outputs, output_states)</span><br></pre></td></tr></table></figure>
<h3 id="代码-1"><a href="#代码-1" class="headerlink" title="代码"></a>代码</h3><blockquote>
<p>基于Keras框架，采用双向LSTM实现文本分类，代码如下：</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">###################### load packages ####################</span></span><br><span class="line"><span class="keyword">from</span> keras.datasets <span class="keyword">import</span> imdb</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> preprocessing</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense, Embedding, Dropout, LSTM, Bidirectional, SpatialDropout1D</span><br><span class="line"><span class="keyword">from</span> keras.utils.np_utils <span class="keyword">import</span> to_categorical</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">###################### load data ####################</span></span><br><span class="line"><span class="comment">######### 只考虑最常见的1000个词 ########</span></span><br><span class="line">num_words = <span class="number">1000</span></span><br><span class="line"></span><br><span class="line"><span class="comment">######### 导入数据 #########</span></span><br><span class="line">(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=num_words)</span><br><span class="line"></span><br><span class="line">print(x_train.shape)</span><br><span class="line">print(x_train[<span class="number">0</span>][:<span class="number">5</span>])</span><br><span class="line"></span><br><span class="line">print(y_train.shape)</span><br><span class="line">print(y_train[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">###################### preprocess data ####################</span></span><br><span class="line"><span class="comment">######## 句子长度最长设置为20 ########</span></span><br><span class="line">max_len = <span class="number">20</span></span><br><span class="line"></span><br><span class="line"><span class="comment">######## 对文本进行填充，将文本转成相同长度 ########</span></span><br><span class="line">x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=max_len)</span><br><span class="line">x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=max_len)</span><br><span class="line"></span><br><span class="line">print(x_train.shape)</span><br><span class="line">print(x_train[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">######## 对label做one-hot处理 ########</span></span><br><span class="line">num_class = <span class="number">2</span></span><br><span class="line">y_train = to_categorical(y_train, num_class)</span><br><span class="line">y_test = to_categorical(y_test, num_class)</span><br><span class="line"></span><br><span class="line">print(y_train.shape)</span><br><span class="line">print(y_train[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">###################### build network ####################</span></span><br><span class="line"><span class="comment">######## word dim 词向量维度 ########</span></span><br><span class="line">word_dim = <span class="number">8</span></span><br><span class="line"></span><br><span class="line"><span class="comment">######## network structure ########</span></span><br><span class="line">model = Sequential()</span><br><span class="line"></span><br><span class="line"><span class="comment">#### Embedding层 ####</span></span><br><span class="line">model.add(Embedding(input_dim=<span class="number">1000</span>, output_dim=word_dim, input_length=max_len))</span><br><span class="line"></span><br><span class="line"><span class="comment">#### dropout ####</span></span><br><span class="line">model.add(SpatialDropout1D(<span class="number">0.3</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">#### bi-RNN ####</span></span><br><span class="line">model.add(Bidirectional(LSTM(<span class="number">100</span>, dropout=<span class="number">0.3</span>, recurrent_dropout=<span class="number">0.3</span>)))</span><br><span class="line"></span><br><span class="line"><span class="comment">#### dense ####</span></span><br><span class="line">model.add(Dense(<span class="number">1024</span>, activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">#### dropout ####</span></span><br><span class="line">model.add(Dropout(<span class="number">0.8</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">#### dense ####</span></span><br><span class="line">model.add(Dense(<span class="number">1024</span>, activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">#### dropout ####</span></span><br><span class="line">model.add(Dropout(<span class="number">0.8</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">#### 输出层 ####</span></span><br><span class="line">model.add(Dense(num_class, activation=<span class="string">&#x27;softmax&#x27;</span>))</span><br><span class="line"></span><br><span class="line">print(model.summary())</span><br><span class="line"></span><br><span class="line"><span class="comment">######## optimization and train ########</span></span><br><span class="line">model.<span class="built_in">compile</span>(loss=<span class="string">&#x27;categorical_crossentropy&#x27;</span>, optimizer=<span class="string">&#x27;adam&#x27;</span>, metrics=[<span class="string">&#x27;acc&#x27;</span>])</span><br><span class="line">model.fit(x_train, y_train, batch_size=<span class="number">512</span>, epochs=<span class="number">20</span>, verbose=<span class="number">1</span>, validation_data=(x_test, y_test))</span><br></pre></td></tr></table></figure>
<blockquote>
<p>结果如下：</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line">(25000,)</span><br><span class="line">[1, 14, 22, 16, 43]</span><br><span class="line">(25000,)</span><br><span class="line">1</span><br><span class="line">(25000, 20)</span><br><span class="line">[ 65  16  38   2  88  12  16 283   5  16   2 113 103  32  15  16   2  19</span><br><span class="line"> 178  32]</span><br><span class="line">(25000, 2)</span><br><span class="line">[0. 1.]</span><br><span class="line">_________________________________________________________________</span><br><span class="line">Layer (type)                 Output Shape              Param #   </span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">embedding_1 (Embedding)      (None, 20, 8)             8000      </span><br><span class="line">_________________________________________________________________</span><br><span class="line">spatial_dropout1d_1 (Spatial (None, 20, 8)             0         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">bidirectional_1 (Bidirection (None, 200)               87200     </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dense_1 (Dense)              (None, 1024)              205824    </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dropout_1 (Dropout)          (None, 1024)              0         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dense_2 (Dense)              (None, 1024)              1049600   </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dropout_2 (Dropout)          (None, 1024)              0         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dense_3 (Dense)              (None, 2)                 2050      </span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">Total params: 1,352,674</span><br><span class="line">Trainable params: 1,352,674</span><br><span class="line">Non-trainable params: 0</span><br><span class="line">_________________________________________________________________</span><br><span class="line">None</span><br><span class="line">Train on 25000 samples, validate on 25000 samples</span><br><span class="line">Epoch 1&#x2F;20</span><br><span class="line">25000&#x2F;25000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 22s 884us&#x2F;step - loss: 0.6930 - acc: 0.5094 - val_loss: 0.6887 - val_acc: 0.5946</span><br><span class="line">Epoch 2&#x2F;20</span><br><span class="line">25000&#x2F;25000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 18s 700us&#x2F;step - loss: 0.6251 - acc: 0.6475 - val_loss: 0.5463 - val_acc: 0.7220</span><br><span class="line">Epoch 3&#x2F;20</span><br><span class="line">25000&#x2F;25000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 18s 703us&#x2F;step - loss: 0.5466 - acc: 0.7254 - val_loss: 0.5231 - val_acc: 0.7366</span><br><span class="line">Epoch 4&#x2F;20</span><br><span class="line">25000&#x2F;25000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 18s 704us&#x2F;step - loss: 0.5296 - acc: 0.7377 - val_loss: 0.5179 - val_acc: 0.7367</span><br><span class="line">Epoch 5&#x2F;20</span><br><span class="line">25000&#x2F;25000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 18s 700us&#x2F;step - loss: 0.5209 - acc: 0.7432 - val_loss: 0.5207 - val_acc: 0.7310</span><br><span class="line">Epoch 6&#x2F;20</span><br><span class="line">25000&#x2F;25000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 18s 702us&#x2F;step - loss: 0.5151 - acc: 0.7452 - val_loss: 0.5144 - val_acc: 0.7380</span><br><span class="line">Epoch 7&#x2F;20</span><br><span class="line">25000&#x2F;25000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 17s 694us&#x2F;step - loss: 0.5118 - acc: 0.7488 - val_loss: 0.5123 - val_acc: 0.7390</span><br><span class="line">Epoch 8&#x2F;20</span><br><span class="line">25000&#x2F;25000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 18s 727us&#x2F;step - loss: 0.5064 - acc: 0.7542 - val_loss: 0.5153 - val_acc: 0.7361</span><br><span class="line">Epoch 9&#x2F;20</span><br><span class="line">25000&#x2F;25000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 18s 708us&#x2F;step - loss: 0.5060 - acc: 0.7540 - val_loss: 0.5119 - val_acc: 0.7400</span><br><span class="line">Epoch 10&#x2F;20</span><br><span class="line">25000&#x2F;25000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 18s 720us&#x2F;step - loss: 0.5042 - acc: 0.7518 - val_loss: 0.5110 - val_acc: 0.7401</span><br><span class="line">Epoch 11&#x2F;20</span><br><span class="line">25000&#x2F;25000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 18s 731us&#x2F;step - loss: 0.5052 - acc: 0.7508 - val_loss: 0.5126 - val_acc: 0.7415</span><br><span class="line">Epoch 12&#x2F;20</span><br><span class="line">25000&#x2F;25000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 18s 736us&#x2F;step - loss: 0.5003 - acc: 0.7578 - val_loss: 0.5114 - val_acc: 0.7400</span><br><span class="line">Epoch 13&#x2F;20</span><br><span class="line">25000&#x2F;25000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 19s 741us&#x2F;step - loss: 0.4983 - acc: 0.7554 - val_loss: 0.5164 - val_acc: 0.7362</span><br><span class="line">Epoch 14&#x2F;20</span><br><span class="line">25000&#x2F;25000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 23s 925us&#x2F;step - loss: 0.4976 - acc: 0.7616 - val_loss: 0.5115 - val_acc: 0.7403</span><br><span class="line">Epoch 15&#x2F;20</span><br><span class="line">25000&#x2F;25000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 23s 926us&#x2F;step - loss: 0.4949 - acc: 0.7599 - val_loss: 0.5118 - val_acc: 0.7401</span><br><span class="line">Epoch 16&#x2F;20</span><br><span class="line">25000&#x2F;25000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 23s 903us&#x2F;step - loss: 0.4957 - acc: 0.7608 - val_loss: 0.5110 - val_acc: 0.7403</span><br><span class="line">Epoch 17&#x2F;20</span><br><span class="line">25000&#x2F;25000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 23s 926us&#x2F;step - loss: 0.4919 - acc: 0.7610 - val_loss: 0.5109 - val_acc: 0.7405</span><br><span class="line">Epoch 18&#x2F;20</span><br><span class="line">25000&#x2F;25000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 23s 927us&#x2F;step - loss: 0.4909 - acc: 0.7622 - val_loss: 0.5107 - val_acc: 0.7408</span><br><span class="line">Epoch 19&#x2F;20</span><br><span class="line">25000&#x2F;25000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 23s 921us&#x2F;step - loss: 0.4886 - acc: 0.7608 - val_loss: 0.5108 - val_acc: 0.7397</span><br><span class="line">Epoch 20&#x2F;20</span><br><span class="line">25000&#x2F;25000 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 23s 917us&#x2F;step - loss: 0.4895 - acc: 0.7624 - val_loss: 0.5132 - val_acc: 0.7369</span><br></pre></td></tr></table></figure>

<p>参考：</p>
<p><a target="_blank" rel="noopener" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">http://colah.github.io/posts/2015-08-Understanding-LSTMs/</a></p>
<p><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/9dc9f41f0b29/">https://www.jianshu.com/p/9dc9f41f0b29/</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/jmh1996/article/details/83476061">https://blog.csdn.net/jmh1996/article/details/83476061</a></p>
<p><a target="_blank" rel="noopener" href="https://www.jiqizhixin.com/articles/2018-10-24-13">https://www.jiqizhixin.com/articles/2018-10-24-13</a></p>
</div><div class="article-licensing box"><div class="licensing-title"><p>循环神经网络详解</p><p><a href="https://huzhiliang.com/2019/10/31/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/">https://huzhiliang.com/2019/10/31/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>作者</h6><p>ฅ´ω`ฅ</p></div></div><div class="level-item is-narrow"><div><h6>发布于</h6><p>2019-10-31</p></div></div><div class="level-item is-narrow"><div><h6>更新于</h6><p>2019-11-02</p></div></div><div class="level-item is-narrow"><div><h6>许可协议</h6><p><a class="icon" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a><a class="icon" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a><a class="icon" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/RNN/">RNN</a><a class="link-muted mr-2" rel="tag" href="/tags/GRN/">GRN</a><a class="link-muted mr-2" rel="tag" href="/tags/LSTM/">LSTM</a></div><div class="notification is-danger">You need to set <code>install_url</code> to use ShareThis. Please set it in <code>_config.yml</code>.</div></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">喜欢这篇文章？打赏一下作者吧</h3><div class="buttons is-centered"><a class="button donate" href="/" target="_blank" rel="noopener" data-type="afdian"><span class="icon is-small"><i class="fas fa-charging-station"></i></span><span>爱发电</span></a><a class="button donate" data-type="alipay"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>支付宝</span><span class="qrcode"><img src="/" alt="支付宝"></span></a><a class="button donate" href="/" target="_blank" rel="noopener" data-type="buymeacoffee"><span class="icon is-small"><i class="fas fa-coffee"></i></span><span>送我杯咖啡</span></a><a class="button donate" href="/" target="_blank" rel="noopener" data-type="patreon"><span class="icon is-small"><i class="fab fa-patreon"></i></span><span>Patreon</span></a><div class="notification is-danger">You forgot to set the <code>business</code> or <code>currency_code</code> for Paypal. Please set it in <code>_config.yml</code>.</div><a class="button donate" data-type="wechat"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>微信</span><span class="qrcode"><img src="/" alt="微信"></span></a></div></div></div><div class="card"><nav class="post-navigation mt-4 level is-mobile card-content"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2019/11/01/Binary-Search-Tree-Iterator/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">Binary Search Tree Iterator</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2019/10/31/Knapsack-%E8%83%8C%E5%8C%85%E9%97%AE%E9%A2%98/"><span class="level-item">Knapsack - 背包问题</span><i class="level-item fas fa-chevron-right"></i></a></div></nav></div><div class="card"><div class="card-content"><h3 class="title is-5">评论</h3><div class="notification is-danger">You forgot to set the <code>shortname</code> for Disqus. Please set it in <code>_config.yml</code>.</div></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/avatar.png" alt="Your name"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Your name</p><p class="is-size-6 is-block">Your title</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Your location</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives"><p class="title">161</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories"><p class="title">14</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags"><p class="title">97</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/ppoffice" target="_blank" rel="noopener" id="widget-follow">微博 Weibo</a></div><div class="level is-mobile"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/ppoffice"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">链接</h3><ul class="menu-list"><li><a class="level is-mobile is-mobile" href="https://hexo.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Hexo</span></span><span class="level-right"><span class="level-item tag">hexo.io</span></span></a></li><li><a class="level is-mobile is-mobile" href="https://bulma.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Bulma</span></span><span class="level-right"><span class="level-item tag">bulma.io</span></span></a></li></ul></div><a class="link-more button is-light is-small size-small" href="/friends/">查看更多</a></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/C/"><span class="level-start"><span class="level-item">C++</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Golang/"><span class="level-start"><span class="level-item">Golang</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Kaggle/"><span class="level-start"><span class="level-item">Kaggle</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/LeetCode/"><span class="level-start"><span class="level-item">LeetCode</span></span><span class="level-end"><span class="level-item tag">85</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/"><span class="level-start"><span class="level-item">Python</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"><span class="level-start"><span class="level-item">数据结构</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"><span class="level-start"><span class="level-item">机器学习</span></span><span class="level-end"><span class="level-item tag">26</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"><span class="level-start"><span class="level-item">深度学习</span></span><span class="level-end"><span class="level-item tag">29</span></span></a><ul><li><a class="level is-mobile" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/CNN/"><span class="level-start"><span class="level-item">CNN</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">16</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/"><span class="level-start"><span class="level-item">迁移学习</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%E7%A0%B4%E8%A7%A3/"><span class="level-start"><span class="level-item">破解</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E7%AE%97%E6%B3%95/"><span class="level-start"><span class="level-item">算法</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%8B%B1%E8%AF%AD%E5%AD%A6%E4%B9%A0/"><span class="level-start"><span class="level-item">英语学习</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">最新文章</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2020-03-05T05:41:28.000Z">2020-03-05</time></p><p class="title"><a href="/2020/03/05/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BACRF-%E4%B8%89-%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E4%B8%8E%E7%BB%B4%E7%89%B9%E6%AF%94%E7%AE%97%E6%B3%95%E8%A7%A3%E7%A0%81/">条件随机场CRF(三) 模型学习与维特比算法解码</a></p><p class="categories"><a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a> / <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP/">NLP</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2020-03-04T08:24:55.000Z">2020-03-04</time></p><p class="title"><a href="/2020/03/04/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BACRF-%E4%BA%8C-%E5%89%8D%E5%90%91%E5%90%8E%E5%90%91%E7%AE%97%E6%B3%95%E8%AF%84%E4%BC%B0%E6%A0%87%E8%AE%B0%E5%BA%8F%E5%88%97%E6%A6%82%E7%8E%87/">条件随机场CRF(二) 前向后向算法评估标记序列概率</a></p><p class="categories"><a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a> / <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP/">NLP</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2020-03-03T02:59:49.000Z">2020-03-03</time></p><p class="title"><a href="/2020/03/03/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BACRF-%E4%B8%80-%E4%BB%8E%E9%9A%8F%E6%9C%BA%E5%9C%BA%E5%88%B0%E7%BA%BF%E6%80%A7%E9%93%BE%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/">条件随机场CRF(一)从随机场到线性链条件随机场</a></p><p class="categories"><a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a> / <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP/">NLP</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2020-03-02T02:46:01.000Z">2020-03-02</time></p><p class="title"><a href="/2020/03/02/%E6%96%87%E6%9C%AC%E6%8C%96%E6%8E%98%E7%9A%84%E5%88%86%E8%AF%8D%E5%8E%9F%E7%90%86/">文本挖掘的分词原理</a></p><p class="categories"><a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a> / <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP/">NLP</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2020-02-20T01:01:00.000Z">2020-02-20</time></p><p class="title"><a href="/2020/02/20/optimize-water-distribution-in-a-village/">optimize water distribution in a village</a></p><p class="categories"><a href="/categories/LeetCode/">LeetCode</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">归档</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2020/03/"><span class="level-start"><span class="level-item">三月 2020</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/02/"><span class="level-start"><span class="level-item">二月 2020</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/01/"><span class="level-start"><span class="level-item">一月 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/12/"><span class="level-start"><span class="level-item">十二月 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/11/"><span class="level-start"><span class="level-item">十一月 2019</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/10/"><span class="level-start"><span class="level-item">十月 2019</span></span><span class="level-end"><span class="level-item tag">33</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/09/"><span class="level-start"><span class="level-item">九月 2019</span></span><span class="level-end"><span class="level-item tag">16</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/08/"><span class="level-start"><span class="level-item">八月 2019</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/07/"><span class="level-start"><span class="level-item">七月 2019</span></span><span class="level-end"><span class="level-item tag">17</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/05/"><span class="level-start"><span class="level-item">五月 2019</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/04/"><span class="level-start"><span class="level-item">四月 2019</span></span><span class="level-end"><span class="level-item tag">12</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/03/"><span class="level-start"><span class="level-item">三月 2019</span></span><span class="level-end"><span class="level-item tag">17</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/02/"><span class="level-start"><span class="level-item">二月 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/01/"><span class="level-start"><span class="level-item">一月 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/08/"><span class="level-start"><span class="level-item">八月 2018</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/07/"><span class="level-start"><span class="level-item">七月 2018</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/05/"><span class="level-start"><span class="level-item">五月 2018</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/03/"><span class="level-start"><span class="level-item">三月 2018</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/02/"><span class="level-start"><span class="level-item">二月 2018</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/01/"><span class="level-start"><span class="level-item">一月 2018</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2017/12/"><span class="level-start"><span class="level-item">十二月 2017</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2017/11/"><span class="level-start"><span class="level-item">十一月 2017</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2017/10/"><span class="level-start"><span class="level-item">十月 2017</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2017/09/"><span class="level-start"><span class="level-item">九月 2017</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2017/07/"><span class="level-start"><span class="level-item">七月 2017</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2017/06/"><span class="level-start"><span class="level-item">六月 2017</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2017/05/"><span class="level-start"><span class="level-item">五月 2017</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2017/03/"><span class="level-start"><span class="level-item">三月 2017</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2017/02/"><span class="level-start"><span class="level-item">二月 2017</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/archives/2015/04/"><span class="level-start"><span class="level-item">四月 2015</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2015/02/"><span class="level-start"><span class="level-item">二月 2015</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">标签</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/Array/"><span class="tag">Array</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Attention/"><span class="tag">Attention</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Binary-Tree/"><span class="tag">Binary Tree</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/C/"><span class="tag">C++</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CFG/"><span class="tag">CFG</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CNN/"><span class="tag">CNN</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CPU/"><span class="tag">CPU</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CYK/"><span class="tag">CYK</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Caffee/"><span class="tag">Caffee</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/English/"><span class="tag">English</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Facebook/"><span class="tag">Facebook</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/GAN/"><span class="tag">GAN</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/GBDT/"><span class="tag">GBDT</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/GPU/"><span class="tag">GPU</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/GRN/"><span class="tag">GRN</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/GloVe/"><span class="tag">GloVe</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Go/"><span class="tag">Go</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Google-apac/"><span class="tag">Google apac</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Go%E8%AF%AD%E8%A8%80/"><span class="tag">Go语言</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Greedy/"><span class="tag">Greedy</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/HMM/"><span class="tag">HMM</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/IBM-Modes/"><span class="tag">IBM Modes</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/KL%E6%95%A3%E5%BA%A6/"><span class="tag">KL散度</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/LSTM/"><span class="tag">LSTM</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/LeetCode/"><span class="tag">LeetCode</span><span class="tag">31</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Lexicalized-PCFG/"><span class="tag">Lexicalized PCFG</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Lexized-PCFG/"><span class="tag">Lexized PCFG</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Log-Linear-Models/"><span class="tag">Log-Linear Models</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Machine-Learning/"><span class="tag">Machine Learning</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/PCFG/"><span class="tag">PCFG</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Paper/"><span class="tag">Paper</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Programmer/"><span class="tag">Programmer</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Python/"><span class="tag">Python</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Pytorch/"><span class="tag">Pytorch</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Queue/"><span class="tag">Queue</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/RNN/"><span class="tag">RNN</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Seq2seq/"><span class="tag">Seq2seq</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Tagging/"><span class="tag">Tagging</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Tencent/"><span class="tag">Tencent</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Tensorflow/"><span class="tag">Tensorflow</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/VIP/"><span class="tag">VIP</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Viterbi/"><span class="tag">Viterbi</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Word-representation/"><span class="tag">Word representation</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/XGBoost/"><span class="tag">XGBoost</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/exhaustive-search/"><span class="tag">exhaustive search</span><span class="tag">7</span></a></div><div class="control"><a class="tags has-addons" href="/tags/git/"><span class="tag">git</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/github/"><span class="tag">github</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/numpy/"><span class="tag">numpy</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/python/"><span class="tag">python</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/spark/"><span class="tag">spark</span><span class="tag">23</span></a></div><div class="control"><a class="tags has-addons" href="/tags/turtle/"><span class="tag">turtle</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/word2vec/"><span class="tag">word2vec</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E4%B8%93%E9%A2%98/"><span class="tag">专题</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E4%BA%8C%E5%88%86%E6%90%9C%E7%B4%A2/"><span class="tag">二分搜索</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E4%BA%8C%E5%88%86%E6%90%9C%E7%B4%A2%E6%A0%91/"><span class="tag">二分搜索树</span><span class="tag">9</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E4%BA%8C%E5%88%86%E6%A0%91/"><span class="tag">二分树</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E4%BA%8C%E8%BF%9B%E5%88%B6/"><span class="tag">二进制</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/"><span class="tag">优化算法</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%88%86%E7%B1%BB/"><span class="tag">分类</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%88%86%E8%AF%8D/"><span class="tag">分词</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%89%91%E6%8C%87offer/"><span class="tag">剑指offer</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%8A%A8%E6%80%81%E5%9B%9E%E5%BD%92/"><span class="tag">动态回归</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"><span class="tag">动态规划</span><span class="tag">22</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%9B%9E%E5%BD%92/"><span class="tag">回归</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%9B%BE%E8%AE%BA/"><span class="tag">图论</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%8E%92%E5%BA%8F/"><span class="tag">排序</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%95%B0%E5%AD%A6/"><span class="tag">数学</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86/"><span class="tag">数学原理</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"><span class="tag">数据分析</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/"><span class="tag">条件随机场</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6/"><span class="tag">极大似然</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9/"><span class="tag">模型压缩</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/"><span class="tag">模型部署</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"><span class="tag">深度学习</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%89%88%E6%9D%83/"><span class="tag">版权</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%89%B9%E5%BE%81%E6%8A%BD%E5%8F%96/"><span class="tag">特征抽取</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%AC%94%E8%AE%B0/"><span class="tag">笔记</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%AE%97%E6%B3%95/"><span class="tag">算法</span><span class="tag">9</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86/"><span class="tag">算法原理</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%AE%97%E6%B3%95%E5%A4%8D%E6%9D%82%E5%BA%A6/"><span class="tag">算法复杂度</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/"><span class="tag">线性模型</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%BB%84%E5%90%88%E6%A0%91/"><span class="tag">组合树</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C/"><span class="tag">经典网络</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%BB%9F%E8%AE%A1%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91/"><span class="tag">统计机器翻译</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%BD%91%E6%98%93%E4%BA%91%E9%9F%B3%E4%B9%90/"><span class="tag">网易云音乐</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%81%9A%E7%B1%BB/"><span class="tag">聚类</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%83%8C%E5%8C%85%E9%97%AE%E9%A2%98/"><span class="tag">背包问题</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%84%9A%E6%9C%AC/"><span class="tag">脚本</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%AF%AD%E6%B3%95/"><span class="tag">语法</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/"><span class="tag">语言模型</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E6%95%B4/"><span class="tag">超参数调整</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/"><span class="tag">迁移学习</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E9%80%92%E5%BD%92/"><span class="tag">递归</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E9%82%AE%E4%BB%B6/"><span class="tag">邮件</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E9%93%BE%E8%A1%A8/"><span class="tag">链表</span><span class="tag">19</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E9%99%8D%E7%BB%B4/"><span class="tag">降维</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E9%9D%A2%E8%AF%95%E4%B8%93%E5%9C%BA/"><span class="tag">面试专场</span><span class="tag">2</span></a></div></div></div></div></div><div class="card widget" id="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">订阅更新</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button is-primary" type="submit" value="订阅"></div></div></form></div></div></div><div class="card widget"><div class="card-content"><div class="notification is-danger">You need to set <code>client_id</code> and <code>slot_id</code> to show this AD unit. Please set it in <code>_config.yml</code>.</div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img class="logo-img" src="/" alt="MCFON" height="28"><img class="logo-img-dark" src="/" alt="MCFON" height="28"></a><p class="is-size-7"><span>&copy; 2020 ฅ´ω`ฅ</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/imaegoo/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a><a href="http://www.miitbeian.gov.cn" target="_blank">豫ICP备18017229号</a> - </p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" async></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><div class="searchbox-pinyin"><label class="checkbox"><input id="search-by-pinyin" type="checkbox" checked="checked"><span> 拼音检索</span></label></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/imaegoo/pinyin.js" defer></script><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script><script type="text/javascript" src="/js/imaegoo/imaegoo.js"></script><script type="text/javascript" src="/js/imaegoo/universe.js"></script><script type="text/javascript" src="/js/live2d/autoload.js"></script></body></html>